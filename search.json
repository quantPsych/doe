[
  {
    "objectID": "worksheets/potential-outcome.html",
    "href": "worksheets/potential-outcome.html",
    "title": "Potential Outcomes, ANOVA, and ANCOVA",
    "section": "",
    "text": "Each unit has two outcomes: - Y(1): if treated - Y(0): if control\nWe only observe one of these.\n\n\n\n\n\n\nStudent\nY(1)\nY(0)\nTreated?\nY_obs\nUnobserved\n\n\n\n\nAlice\n90\n80\n1\n\n\n\n\nBen\n85\n85\n0\n\n\n\n\nCarla\n95\n87\n1\n\n\n\n\n\nQuestions: 1. Why can’t we observe both Y(1) and Y(0)? 2. How can we estimate ATE?\n\n\n\n\n\n\n\nGroup\nStudents\nY_obs\nMean\n\n\n\n\nTreated\nAlice, Carla\n[90, 95]\n\n\n\nControl\nBen\n[85]\n\n\n\n\nATE = Mean_Treated - Mean_Control = ?\n\n\n\n\n\n\nStudent\nPre-Test\nTreated\nPost-Test\n\n\n\n\nMaya\n88\n1\n88\n\n\nZoe\n92\n0\n92\n\n\nLiam\n85\n1\n85\n\n\nRay\n83\n0\n83\n\n\n\nCalculate: - Group post-test means - ATE - What’s wrong?\n\n\n\nAssume slope = 1\nOverall Pre-Test Mean = ?\n\n\n\nGroup\nPre-Test Mean\nPost-Test Mean\nAdjust\nAdjusted Mean\n\n\n\n\nTreated\n\n\n\n\n\n\nControl\n\n\n\n\n\n\n\nAdjusted ATE = ?"
  },
  {
    "objectID": "worksheets/potential-outcome.html#part-1-understanding-potential-outcomes",
    "href": "worksheets/potential-outcome.html#part-1-understanding-potential-outcomes",
    "title": "Potential Outcomes, ANOVA, and ANCOVA",
    "section": "",
    "text": "Each unit has two outcomes: - Y(1): if treated - Y(0): if control\nWe only observe one of these.\n\n\n\n\n\n\nStudent\nY(1)\nY(0)\nTreated?\nY_obs\nUnobserved\n\n\n\n\nAlice\n90\n80\n1\n\n\n\n\nBen\n85\n85\n0\n\n\n\n\nCarla\n95\n87\n1\n\n\n\n\n\nQuestions: 1. Why can’t we observe both Y(1) and Y(0)? 2. How can we estimate ATE?"
  },
  {
    "objectID": "worksheets/potential-outcome.html#part-2-estimate-ate",
    "href": "worksheets/potential-outcome.html#part-2-estimate-ate",
    "title": "Potential Outcomes, ANOVA, and ANCOVA",
    "section": "",
    "text": "Group\nStudents\nY_obs\nMean\n\n\n\n\nTreated\nAlice, Carla\n[90, 95]\n\n\n\nControl\nBen\n[85]\n\n\n\n\nATE = Mean_Treated - Mean_Control = ?"
  },
  {
    "objectID": "worksheets/potential-outcome.html#part-3-confounding",
    "href": "worksheets/potential-outcome.html#part-3-confounding",
    "title": "Potential Outcomes, ANOVA, and ANCOVA",
    "section": "",
    "text": "Student\nPre-Test\nTreated\nPost-Test\n\n\n\n\nMaya\n88\n1\n88\n\n\nZoe\n92\n0\n92\n\n\nLiam\n85\n1\n85\n\n\nRay\n83\n0\n83\n\n\n\nCalculate: - Group post-test means - ATE - What’s wrong?"
  },
  {
    "objectID": "worksheets/potential-outcome.html#part-4-ancova",
    "href": "worksheets/potential-outcome.html#part-4-ancova",
    "title": "Potential Outcomes, ANOVA, and ANCOVA",
    "section": "",
    "text": "Assume slope = 1\nOverall Pre-Test Mean = ?\n\n\n\nGroup\nPre-Test Mean\nPost-Test Mean\nAdjust\nAdjusted Mean\n\n\n\n\nTreated\n\n\n\n\n\n\nControl\n\n\n\n\n\n\n\nAdjusted ATE = ?"
  },
  {
    "objectID": "syllabus/schedule.html",
    "href": "syllabus/schedule.html",
    "title": "📅 Course Schedule",
    "section": "",
    "text": "The following is a tentative schedule for the course, outlining the topics covered each week, the corresponding reading assignments, and the due dates for assignments and assessments. This schedule is subject to change, and students should refer to the Canvas Assignments and announcements for the most up-to-date information.\nHowever, the final exam date is not subject to change, as it is determined and scheduled by the university. Please plan accordingly based on the final exam schedule provided by UNM:\n\nFinal Exam Date: Tuesday, May 13, 2025, 10:00 a.m.–12:00 p.m.\nLocation: Classroom (as specified in the syllabus)\n\nStudents are strongly encouraged to stay proactive, monitor changes, and ensure they meet all revised deadlines and expectations as announced.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\nReadings & Homework Assignments\n\n\n\n\n1\nJan. 21 & 23, 2025\nIntroduction to Experimental Design\nCh. 1 (Sections 1.1–1.1.1), Ch. 2 (Sections 2.1–2.2).\n\n\n2\nJan. 28 & 30, 2025\nCompletely Randomized Designs (CRD)\nCh. 3 (Sections 3.1–3.5); Homework 1 (due Feb. 6).\n\n\n3\nFeb. 4 & 6, 2025\nMultiple Comparisons\nCh. 4 (Sections 4.1–4.4); Homework 2 (due Feb. 13).\n\n\n4\nFeb. 11 & 13, 2025\nChecking Model Assumptions\nCh. 5 (Sections 5.1–5.3); Homework 3 (due Feb. 20).\n\n\n5\nFeb. 18 & 20, 2025\nTwo-Way Factorial Designs\nCh. 6 (Sections 6.1–6.5); Homework 4 (due Feb. 27).\n\n\n6\nFeb. 25 & 27, 2025\nHigher-Order Factorial Designs\nCh. 7 (Sections 7.1–7.6); Homework 5 (due Mar. 6).\n\n\n7\nMar. 4 & 6, 2025\nMidterm Exam\nReview all chapters and notes.\n\n\n8\nMar. 11 & 13, 2025\nBlocking & RCBD\nCh. 10 (Sections 10.1–10.4); Homework 6 (due Mar. 25).\n\n\n9\nMar. 16–20, 2025\n==Spring Break==\nNo classes.\n\n\n10\nMar. 25 & 27, 2025\nComplete Block Designs\nCh. 10 (Sections 10.6–10.8); Homework 7 (due Apr. 3).\n\n\n11\nApr. 1 & 3, 2025\nAnalysis of Covariance (ANCOVA)\nCh. 9 (Sections 9.1–9.4); Homework 8 (due Apr. 10).\n\n\n12\nApr. 8 & 10, 2025\nRow–Column (Latin square) Designs\nCh. 12 (Sections 12.1–12.4); Homework 9 (due Apr. 17).\n\n\n13\nApr. 15 & 17, 2025\nRandom & Mixed Models\nCh. 17 (Sections 17.1–17.8); Homework 10 (due Apr. 24).\n\n\n14\nApr. 22 & 24, 2025\nNested Models\nCh. 18 (Sections 18.1–18.4); Homework 11 (due May 1).\n\n\n15\nApr. 29 & May 1, 2025\nSplit-Plot Designs\nCh. 19 (Sections 19.1–19.5); Homework 12 (due May 6).\n\n\n16\nMay 6 & 8, 2025\nCatching Up and Review\nComprehensive review and Q&A.\n\n\n17\nMay 13\nFinal Exam\nComprehensive (Weeks 1–15).",
    "crumbs": [
      "Home",
      "Course Overview",
      "Schedule"
    ]
  },
  {
    "objectID": "syllabus/schedule.html#tentative-course-schedule",
    "href": "syllabus/schedule.html#tentative-course-schedule",
    "title": "📅 Course Schedule",
    "section": "",
    "text": "Week\nDate\nTopics\nReadings & Homework Assignments\n\n\n\n\n1\nJan. 21 & 23, 2025\nIntroduction to Experimental Design\nCh. 1 (Sections 1.1–1.1.1), Ch. 2 (Sections 2.1–2.2).\n\n\n2\nJan. 28 & 30, 2025\nCompletely Randomized Designs (CRD)\nCh. 3 (Sections 3.1–3.5); Homework 1 (due Feb. 6).\n\n\n3\nFeb. 4 & 6, 2025\nMultiple Comparisons\nCh. 4 (Sections 4.1–4.4); Homework 2 (due Feb. 13).\n\n\n4\nFeb. 11 & 13, 2025\nChecking Model Assumptions\nCh. 5 (Sections 5.1–5.3); Homework 3 (due Feb. 20).\n\n\n5\nFeb. 18 & 20, 2025\nTwo-Way Factorial Designs\nCh. 6 (Sections 6.1–6.5); Homework 4 (due Feb. 27).\n\n\n6\nFeb. 25 & 27, 2025\nHigher-Order Factorial Designs\nCh. 7 (Sections 7.1–7.6); Homework 5 (due Mar. 6).\n\n\n7\nMar. 4 & 6, 2025\nMidterm Exam\nReview all chapters and notes.\n\n\n8\nMar. 11 & 13, 2025\nBlocking & RCBD\nCh. 10 (Sections 10.1–10.4); Homework 6 (due Mar. 25).\n\n\n9\nMar. 16–20, 2025\n==Spring Break==\nNo classes.\n\n\n10\nMar. 25 & 27, 2025\nComplete Block Designs\nCh. 10 (Sections 10.6–10.8); Homework 7 (due Apr. 3).\n\n\n11\nApr. 1 & 3, 2025\nAnalysis of Covariance (ANCOVA)\nCh. 9 (Sections 9.1–9.4); Homework 8 (due Apr. 10).\n\n\n12\nApr. 8 & 10, 2025\nRow–Column (Latin square) Designs\nCh. 12 (Sections 12.1–12.4); Homework 9 (due Apr. 17).\n\n\n13\nApr. 15 & 17, 2025\nRandom & Mixed Models\nCh. 17 (Sections 17.1–17.8); Homework 10 (due Apr. 24).\n\n\n14\nApr. 22 & 24, 2025\nNested Models\nCh. 18 (Sections 18.1–18.4); Homework 11 (due May 1).\n\n\n15\nApr. 29 & May 1, 2025\nSplit-Plot Designs\nCh. 19 (Sections 19.1–19.5); Homework 12 (due May 6).\n\n\n16\nMay 6 & 8, 2025\nCatching Up and Review\nComprehensive review and Q&A.\n\n\n17\nMay 13\nFinal Exam\nComprehensive (Weeks 1–15).",
    "crumbs": [
      "Home",
      "Course Overview",
      "Schedule"
    ]
  },
  {
    "objectID": "r_help.html",
    "href": "r_help.html",
    "title": "R Help and Tutorial",
    "section": "",
    "text": "Welcome to the R Help and Tutorial Section. This resource organizes R help files into structured categories based on Subjects, Topics, and implementation through R Base and Packages.\n\n\nBelow are the available subjects and their corresponding topics. Select a topic to view detailed help pages.\n\n\n\nData Manipulation\n\nCreating Data Frames\nWorking with Factors\nSubsetting Data\nData Transformation and Wrangling\nInporting Data into R\n\nData Visualization\n\nBase R Graphics\nggplot2 Basics\nsjPlot Tutorial\n\nStatistical Analysis\n\nHypothesis Testing\nGames-Howell Post-Hoc Comparison\nStandadized Rannge Distribution and ANOVA\n\nQuarto Tutorial\n\nQuarto Tutorial\nFigures, Tables, Captions, and Referencing in Quarto\nFigures and Tables in Quarto: Advanced Features",
    "crumbs": [
      "Home",
      "R Code and Datasets",
      "R Help"
    ]
  },
  {
    "objectID": "r_help.html#r-subjects-topics",
    "href": "r_help.html#r-subjects-topics",
    "title": "R Help and Tutorial",
    "section": "",
    "text": "Below are the available subjects and their corresponding topics. Select a topic to view detailed help pages.\n\n\n\nData Manipulation\n\nCreating Data Frames\nWorking with Factors\nSubsetting Data\nData Transformation and Wrangling\nInporting Data into R\n\nData Visualization\n\nBase R Graphics\nggplot2 Basics\nsjPlot Tutorial\n\nStatistical Analysis\n\nHypothesis Testing\nGames-Howell Post-Hoc Comparison\nStandadized Rannge Distribution and ANOVA\n\nQuarto Tutorial\n\nQuarto Tutorial\nFigures, Tables, Captions, and Referencing in Quarto\nFigures and Tables in Quarto: Advanced Features",
    "crumbs": [
      "Home",
      "R Code and Datasets",
      "R Help"
    ]
  },
  {
    "objectID": "r_help.html#r-packages",
    "href": "r_help.html#r-packages",
    "title": "R Help and Tutorial",
    "section": "Help by R Packages",
    "text": "Help by R Packages\n\nData Manipulation\n\ndplyr\ntidyr\npurrr\nstringr\n\n\n\nData Visualization\n\nggplot2\nsjPlot Tutorial\n\n\n\nStatistical Analysis\n\nemmeans\nmultcomp\npwr\nGames-Howell Post-Hoc Comparison\nStandadized Rannge Distribution and ANOVA\n\n\n\nQuarto Tutorial\n\nQuarto Tutorial\nFigures, Tables, Captions, and Referencing in Quarto\nFigures and Tables in Quarto: Advanced Features",
    "crumbs": [
      "Home",
      "R Code and Datasets",
      "R Help"
    ]
  },
  {
    "objectID": "r_help.html#by-subjects",
    "href": "r_help.html#by-subjects",
    "title": "R Help and Tutorial",
    "section": "By Subjects:",
    "text": "By Subjects:\n\nData Manipulation\n\nHow to Import Data Into R from DataCamp.\n\nData Visualization\n\nggplot2 Basics\n\nStatistical Analysis\n\nRegression from DataCamp.\nANOVA in R from DataNovia.\n\nProgramming with R\n\nControl Structures from Roger D. Peng.\nWriting Functions from Hadley Wickham.\nDebugging from Hadley Wickham.\nDebugging with the RStudio IDE from Positron RStudio Support.",
    "crumbs": [
      "Home",
      "R Code and Datasets",
      "R Help"
    ]
  },
  {
    "objectID": "r_help.html#general-r-help-and-resources",
    "href": "r_help.html#general-r-help-and-resources",
    "title": "R Help and Tutorial",
    "section": "General R Help and Resources:",
    "text": "General R Help and Resources:\n\nR Documentation: User-friendly interface to R documentation.\nRStudio Cheatsheets: Quick reference guides for R and RStudio.\nR for Data Science: Online book on data science with R by Hadley Wickham and Garrett Grolemund.\nR Programming for Data Science Online book by Roger D. Peng.\nAdvanced R: Online book on advanced R programming by Hadley Wickham.\nPosit Community: Posit Community forum for R users.\nR Bloggers: Blog aggregator for R programming.\nStack Overflow R Tag: Stack Overflow questions tagged with R.",
    "crumbs": [
      "Home",
      "R Code and Datasets",
      "R Help"
    ]
  },
  {
    "objectID": "r_help.html#software-tools-and-resources",
    "href": "r_help.html#software-tools-and-resources",
    "title": "R Help and Tutorial",
    "section": "Software Tools and Resources",
    "text": "Software Tools and Resources\nTo complete assignments and analyses, you will need access to R and related tools.\n\nR Installation: Download R\n\nRStudio IDE: Download RStudio\n\nVS Code IDE: Download VS Code\nPositron IDE: Download Positron\nRStudio IDE Tutorial\nVS Code Editor Tutorial\nQuarto Authoring with RStudio\nQuarto Authoring with VS Code",
    "crumbs": [
      "Home",
      "R Code and Datasets",
      "R Help"
    ]
  },
  {
    "objectID": "r_help.html#additional-online-resources",
    "href": "r_help.html#additional-online-resources",
    "title": "R Help and Tutorial",
    "section": "Additional Online Resources",
    "text": "Additional Online Resources\n\nQuarto Documentation: Learn Quarto\nOnline R Help:\n\nCRAN R Documentation\nRStudio Cheat Sheets\n\nVideo Tutorials:\n\nR Programming for Beginners (YouTube)",
    "crumbs": [
      "Home",
      "R Code and Datasets",
      "R Help"
    ]
  },
  {
    "objectID": "lectures/week-13_nested-models.html",
    "href": "lectures/week-13_nested-models.html",
    "title": "Nested Models in Statistical Design",
    "section": "",
    "text": "In the realm of experimental design and statistical analysis, Nested Models are indispensable for handling data with hierarchical or grouped structures. These models are particularly useful when the levels of one factor are only meaningful within the levels of another factor. Understanding nested models allows researchers to account for variability at different levels, ensuring more accurate and generalizable inferences.\nIntuitive Analogy:\nConsider a university conducting a study on student performance across different courses. Here, courses can be nested within departments, meaning each course belongs to only one department. This hierarchical structure necessitates a nested model to appropriately account for the variability both within and between departments.\n\n\nBy the end of this lecture, you will be able to:\n\nRecognize and model nested factors.\nDifferentiate between nested and crossed factors.\nFormulate fixed and random nested models.\nApply variance components and hypothesis tests to nested designs.\nImplement nested models using R.\nEngage with advanced mathematical concepts through proofs and derivations.\nEnhance your understanding through practical R examples and exercises."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#introduction",
    "href": "lectures/week-13_nested-models.html#introduction",
    "title": "Nested Models in Statistical Design",
    "section": "",
    "text": "In the realm of experimental design and statistical analysis, Nested Models are indispensable for handling data with hierarchical or grouped structures. These models are particularly useful when the levels of one factor are only meaningful within the levels of another factor. Understanding nested models allows researchers to account for variability at different levels, ensuring more accurate and generalizable inferences.\nIntuitive Analogy:\nConsider a university conducting a study on student performance across different courses. Here, courses can be nested within departments, meaning each course belongs to only one department. This hierarchical structure necessitates a nested model to appropriately account for the variability both within and between departments.\n\n\nBy the end of this lecture, you will be able to:\n\nRecognize and model nested factors.\nDifferentiate between nested and crossed factors.\nFormulate fixed and random nested models.\nApply variance components and hypothesis tests to nested designs.\nImplement nested models using R.\nEngage with advanced mathematical concepts through proofs and derivations.\nEnhance your understanding through practical R examples and exercises."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#nested-vs.-crossed-factors",
    "href": "lectures/week-13_nested-models.html#nested-vs.-crossed-factors",
    "title": "Nested Models in Statistical Design",
    "section": "Nested vs. Crossed Factors",
    "text": "Nested vs. Crossed Factors\n\nNested Factors\nDefinition:\nNested factors occur when the levels of one factor are exclusively contained within the levels of another factor. Each level of the nested factor appears only within one level of the parent factor.\nExamples:\n\nCores within Bales: In wool experiments, multiple cores are sampled from each bale. Here, cores are nested within bales.\nHeads within Machines: When evaluating machine performance, different heads are used within each machine, and each head is specific to a machine.\n\nVisual Representation:\nMachine 1\n├── Head 1\n├── Head 2\nMachine 2\n├── Head 1\n├── Head 2\n├── Head 3\n…\n\n\nCrossed Factors\nDefinition:\nCrossed factors are those where every level of one factor appears with every level of another factor. There is no hierarchical containment, and levels are fully crossed.\nExamples:\n\nTreatments across Blocks: Applying multiple treatments across all blocks in an experiment.\nStudents and Teachers: If every teacher instructs every student, factors are crossed.\n\nVisual Representation:\nTeacher 1\n├── Student 1\n├── Student 2\nTeacher 2\n├── Student 1\n├── Student 2\n…\n\n\nApplications of Nested Models\n\nManufacturing: Assessing variability of machine components within different machines.\nBiological Studies: Measuring responses of plants within different plots.\nEducational Research: Evaluating student performance within classrooms and schools.\n\nReference:\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#fixed-effects-nested-model",
    "href": "lectures/week-13_nested-models.html#fixed-effects-nested-model",
    "title": "Nested Models in Statistical Design",
    "section": "Fixed-Effects Nested Model",
    "text": "Fixed-Effects Nested Model\nA Fixed-Effects Nested Model is used when both the parent and nested factors are of primary interest and their effects are fixed.\n\nModel Specification\n\\[\nY_{ijt} = \\mu + \\alpha_i + \\beta_j(i) + \\epsilon_{ijt}, \\quad \\epsilon_{ijt} \\sim N(0, \\sigma^2)\n\\]\nWhere:\n\n\\(Y_{ijt}\\): Response for the \\(t\\)-th observation in the \\(j\\)-th nested factor within the \\(i\\)-th parent factor.\n\\(\\mu\\): Overall mean.\n\\(\\alpha_i\\): Fixed effect of the \\(i\\)-th level of Factor \\(A\\) (parent factor).\n\\(\\beta_j(i)\\): Fixed effect of the \\(j\\)-th level of Factor \\(B\\) nested within Factor \\(A\\).\n\\(\\epsilon_{ijt}\\): Random error.\n\nIntuitive Example:\nEvaluating the performance of different machine heads within specific machines, where both machines and heads are of fixed interest."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#random-effects-nested-model",
    "href": "lectures/week-13_nested-models.html#random-effects-nested-model",
    "title": "Nested Models in Statistical Design",
    "section": "Random-Effects Nested Model",
    "text": "Random-Effects Nested Model\nA Random-Effects Nested Model treats the nested factors as random effects, assuming they are randomly sampled from a larger population.\n\nModel Specification\n\\[\nY_{ijt} = \\mu + \\alpha_i + B_j(i) + \\epsilon_{ijt}, \\quad B_j(i) \\sim N(0, \\sigma^2_B), \\quad \\epsilon_{ijt} \\sim N(0, \\sigma^2)\n\\]\nWhere:\n\n\\(B_j(i)\\): Random effect for the \\(j\\)-th level of Factor \\(B\\) nested within Factor \\(A\\).\n\\(\\sigma^2_B\\): Variance component for the nested random effect.\n\\(\\sigma^2\\): Residual error variance.\n\nIntuitive Example:\nAssessing the variability of machine heads across randomly selected machines from a factory population.\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-mean-squares",
    "href": "lectures/week-13_nested-models.html#expected-mean-squares",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Mean Squares",
    "text": "Expected Mean Squares\nIn random-effects nested models, the Expected Mean Squares (EMS) are pivotal for estimating variance components. EMS are derived based on the model’s structure and assumptions.\n\nEMS for Fixed-Effects Nested Model\nGiven the model:\n\\[\nY_{ijt} = \\mu + \\alpha_i + \\beta_j(i) + \\epsilon_{ijt}\n\\]\nExpected Mean Squares:\n\\[\nEMS_A = \\sigma^2 + r \\sigma^2_B + r b Q(\\alpha_i)\n\\]\n\\[\nEMS_B(A) = \\sigma^2 + r \\sigma^2_B\n\\]\n\\[\nEMS_E = \\sigma^2\n\\]\nWhere:\n\n\\(r\\): Number of replicates per nested factor.\n\\(b\\): Number of levels of Factor \\(B\\) within each level of Factor \\(A\\).\n\\(Q(\\alpha_i)\\): Sum of squares of fixed effects.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\n\n\nEstimation of Variance Components\nUsing the EMS, variance components are estimated as follows:\n\\[\n\\hat{\\sigma}^2_B = \\frac{MS_B(A) - MS_E}{r}\n\\]\n\\[\n\\hat{\\sigma}^2 = MS_E\n\\]\nWhere:\n\n\\(MS_B(A)\\): Mean Square for the nested factor.\n\\(MS_E\\): Mean Square for the error term.\n\nStep-by-Step Example:\nSuppose we have:\n\n\\(a = 5\\) levels of Factor \\(A\\).\n\\(b = 4\\) levels of Factor \\(B\\) nested within each \\(A\\).\n\\(r = 3\\) replicates per \\(B\\).\n\nAn ANOVA table would provide \\(MS_A\\), \\(MS_B(A)\\), and \\(MS_E\\), from which variance components are estimated."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#example-1-machine-heads-nested-in-machines",
    "href": "lectures/week-13_nested-models.html#example-1-machine-heads-nested-in-machines",
    "title": "Nested Models in Statistical Design",
    "section": "Example 1: Machine Heads Nested in Machines",
    "text": "Example 1: Machine Heads Nested in Machines\nScenario:\nAssessing the performance of machine heads nested within machines to understand variability attributable to different machines.\n\nStep-by-Step R Implementation\n\n# Load necessary library\nlibrary(lme4)\n# Simulated Data\nset.seed(123)\nmachines &lt;- factor(rep(1:5, each = 20))\nheads &lt;- factor(rep(rep(1:4, each = 5), 5))\nresponse &lt;- rnorm(100, mean = as.numeric(machines) + as.numeric(heads) / 2, sd = 1)\ndata &lt;- data.frame(response, machines, heads) # Fit Random-Effects Nested Model\nmodel &lt;- lmer(response ~ (1 | machines / heads), data = data)\nsummary(model)\n\n# Extract Variance Components\nVarCorr(model)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interpretation",
    "href": "lectures/week-13_nested-models.html#interpretation",
    "title": "Nested Models in Statistical Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nModel Fitting:\n\nThe lmer function fits a random-intercept model where heads are nested within machines.\n\nSummary Output:\n\nProvides estimates for the fixed effect (overall mean) and random effects (variance components for machines and heads).\n\nVariance Components:\n\n\nmachines: Variability attributable to different machines.\nheads within machines: Variability attributable to different heads within the same machine.\nResidual: Unexplained variability.\n\nReference:\nKempthorne, O. (1977). Design and Analysis of Experiments. Robert E. Krieger Publishing Company."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#example-2-nested-design-anova",
    "href": "lectures/week-13_nested-models.html#example-2-nested-design-anova",
    "title": "Nested Models in Statistical Design",
    "section": "Example 2: Nested Design ANOVA",
    "text": "Example 2: Nested Design ANOVA\n\nScenario\nAnalyzing a nested design using traditional ANOVA methods.\nStep-by-Step R Implementation\n\n# Nested ANOVA with aov\nmodel_aov &lt;- aov(response ~ machines + Error(machines / heads), data = data)\nsummary(model_aov)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interpretation-1",
    "href": "lectures/week-13_nested-models.html#interpretation-1",
    "title": "Nested Models in Statistical Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nModel Fitting:\n\nThe aov function specifies heads nested within machines using the Error term.\n\nANOVA Summary:\n\nDisplays sources of variation, degrees of freedom, sum of squares, mean squares, and F-values for both machines and heads within machines.\n\nHypothesis Testing:\n\n\nMachines: Tests if there is significant variability between different machines.\nHeads within Machines: Tests if there is significant variability between different heads within the same machine.\n\nReference:\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#degrees-of-freedom",
    "href": "lectures/week-13_nested-models.html#degrees-of-freedom",
    "title": "Nested Models in Statistical Design",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nIn nested designs, allocating degrees of freedom correctly is crucial for accurate hypothesis testing.\nCalculation for Nested Effects\nFor a nested effect \\(B(A)\\):\n\\[\n\\nu_{B(A)} = (b - 1) \\times a\n\\]\nWhere:\n\n\\(b\\): Number of levels of Factor (B) within each level of Factor (A).\n\\(a\\): Number of levels of Factor (A).\n\nExample:\nIf there are 5 machines (\\(a = 5\\)) and 4 heads within each machine (\\(b = 4\\)), then:\n\\[\n\\nu_{B(A)} = (4 - 1) \\times 5 = 15\n\\]\nReference:\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#estimable-contrasts",
    "href": "lectures/week-13_nested-models.html#estimable-contrasts",
    "title": "Nested Models in Statistical Design",
    "section": "Estimable Contrasts",
    "text": "Estimable Contrasts\nIn nested models, contrasts within nested factors are restricted to comparisons within each level of the parent factor.\n\nConstructing Contrasts\n\nWithin-Machine Contrasts: Compare different heads within the same machine.\nBetween-Machine Contrasts: Compare different machines, irrespective of heads.\n\n\n\nExample\nComparing the performance of Head 1 vs. Head 2 within Machine 3.\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#summary",
    "href": "lectures/week-13_nested-models.html#summary",
    "title": "Nested Models in Statistical Design",
    "section": "Summary",
    "text": "Summary\n\nNested Models are essential for analyzing data with hierarchical or grouped structures.\nFixed-Effects Nested Models focus on specific levels of factors, providing detailed insights into those particular levels.\nRandom-Effects Nested Models generalize findings to larger populations by treating nested factors as random samples.\nVariance Components quantify the variability at different hierarchical levels, facilitating a deeper understanding of data structure.\nR Implementation using packages like lme4 and functions like aov allows for flexible modeling of nested designs.\nAdvanced Topics such as degrees of freedom allocation and estimable contrasts ensure rigorous statistical analysis.\nMathematical Derivations in the appendix provide a theoretical foundation, enhancing comprehension of underlying principles."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#key-takeaways",
    "href": "lectures/week-13_nested-models.html#key-takeaways",
    "title": "Nested Models in Statistical Design",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nProperly modeling nested structures is crucial for accurate inference.\nDifferentiating between fixed and random effects guides appropriate model selection.\nUnderstanding variance components aids in dissecting sources of variability.\nPractical R examples bridge the gap between theory and application, fostering hands-on learning.\n\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#variance-component-estimation",
    "href": "lectures/week-13_nested-models.html#variance-component-estimation",
    "title": "Nested Models in Statistical Design",
    "section": "Variance Component Estimation",
    "text": "Variance Component Estimation\n\nDeriving Expected Mean Squares for Nested Models\nConsider the Fixed-Effects Nested Model:\n\\[\nY_{ijt} = \\mu + \\alpha_i + \\beta_j(i) + \\epsilon_{ijt}, \\quad \\epsilon_{ijt} \\sim N(0, \\sigma^2)\n\\]\n\n\nExpected Mean Squares\n\nFor Factor (A):\n\n\\[\nEMS_A = \\sigma^2 + r \\sigma^2_B + r b Q(\\alpha_i)\n\\]\nWhere \\(Q(\\alpha_i)\\) is the sum of squares due to Factor (A).\n\nFor Factor (B(A)):\n\n\\[\nEMS_B(A) = \\sigma^2 + r \\sigma^2_B\n\\]\n\nFor Error:\n\n\\[\nEMS_E = \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#derivation-steps",
    "href": "lectures/week-13_nested-models.html#derivation-steps",
    "title": "Nested Models in Statistical Design",
    "section": "Derivation Steps",
    "text": "Derivation Steps\n\nAssumptions:\n\n\n\\(\\alpha_i\\) and \\(\\beta_j(i)\\) are fixed effects.\nErrors are independent and normally distributed.\n\n\nVariance Components:\n\n\nBetween (A) Groups: Variance due to Factor (A) is zero since (A) is fixed.\nBetween (B(A)) Groups: Variance due to Factor (B) nested within (A) is \\(\\sigma^2_B\\).\nWithin (B(A)) Groups: Residual variance is \\(\\sigma^2\\).\n\n\nEstimation:\n\n\nUsing the ANOVA table, mean squares are equated to expected mean squares to solve for variance components.\n\n\\[\nMS_A = \\sigma^2 + r \\sigma^2_B\n\\]\n\\[\nMS_B(A) = \\sigma^2 + r \\sigma^2_B\n\\]\n\\[\nMS_E = \\sigma^2\n\\]\nSolving for \\(\\sigma^2_B\\):\n\\[\n\\hat{\\sigma}^2_B = \\frac{MS_A - MS_E}{r}\n\\]\n\\[\n\\hat{\\sigma}^2 = MS_E\n\\]\n\nConclusion\nThe derivation showcases how ANOVA decomposes total variability into components attributable to nested factors and residual error, facilitating the estimation of variance components.\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#satterthwaite-approximation",
    "href": "lectures/week-13_nested-models.html#satterthwaite-approximation",
    "title": "Nested Models in Statistical Design",
    "section": "Satterthwaite Approximation",
    "text": "Satterthwaite Approximation\nThe Satterthwaite Approximation is employed to estimate the degrees of freedom for variance components, enabling the construction of confidence intervals.\n\nSteps\n\nEstimate Variance Components:\n\nUsing REML or other estimation methods, obtain estimates for \\(\\sigma^2_B\\) and \\(\\sigma^2\\).\n\nCalculate Degrees of Freedom:\n\nApproximated based on the variance component estimates.\n\nConstruct Confidence Intervals:\n\n\\[\nCI = \\hat{\\sigma}^2 \\pm t_{\\alpha/2, df} \\times SE(\\hat{\\sigma}^2)\n\\]\nWhere:\n\n\\(\\hat{\\sigma}^2\\): Estimated variance component.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(SE(\\hat{\\sigma}^2)\\): Standard error of the variance component estimate."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#example-in-r",
    "href": "lectures/week-13_nested-models.html#example-in-r",
    "title": "Nested Models in Statistical Design",
    "section": "Example in R",
    "text": "Example in R\n\n# Fit the Model\n\nmodel &lt;- lmer(response ~ (1 | machines / heads), data = data)\n# Extract variance Components\nvar_components &lt;- VarCorr(model)\nsigma_B_sq &lt;- as.numeric(var_components$machines:heads)\nsigma_sq &lt;- attr(var_components, \"sc\")^2\n# Confidence Intervals Using Wald Method\nconfint(model, parm = \"sigma_B_sq\", method = \"Wald\")\nconfint(model, parm = \"sigma_sq\", method = \"Wald\")\n\n\nInterpretation\nThe confint function provides confidence intervals for the variance components, allowing for inference about population variability.\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#medium-difficulty",
    "href": "lectures/week-13_nested-models.html#medium-difficulty",
    "title": "Nested Models in Statistical Design",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#two-way-ancova-with-interaction",
    "href": "lectures/week-13_nested-models.html#two-way-ancova-with-interaction",
    "title": "Nested Models in Statistical Design",
    "section": "1. Two-Way ANCOVA with Interaction",
    "text": "1. Two-Way ANCOVA with Interaction\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate (X).\nFit an ANCOVA model adjusting for (X).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\n\nset.seed(123)\n# Define Factors and Covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each = 30))\nX &lt;- rnorm(90, mean = 50, sd = 10)\n# Simulate Responses with Treatment Effects and Covariate EffectY &lt;- 5 + ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + 0.5 * (X - mean(X)) + rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n# Fit ANCOVA Model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\nsummary(model_sim)\n\n# Diagnostic Plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated Marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#questions",
    "href": "lectures/week-13_nested-models.html#questions",
    "title": "Nested Models in Statistical Design",
    "section": "Questions",
    "text": "Questions\n\nMain Effects:\n\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate (X): How does (X) influence the response variable (Y)?\n\n\nSignificance Testing:\n\n\nAre the treatment effects statistically significant after adjusting for (X)?\nIs the covariate (X) a significant predictor of (Y)?\n\n\nDiagnostic Plots:\n\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\n\nInterpretation of Pairwise Comparisons:\n\n\nWhich treatments significantly differ from each other after adjusting for (X)?\nHow do the adjusted means compare across treatments?"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-answers",
    "href": "lectures/week-13_nested-models.html#expected-answers",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Answers",
    "text": "Expected Answers\n\nMain Effects:\n\n\nTreatment: Treatments B and C have positive effects relative to Treatment A, indicating higher response values.\nCovariate (X): The covariate (X) has a positive influence on (Y), suggesting that as (X) increases, (Y) tends to increase.\n\n\nSignificance Testing:\n\n\nTreatment Effects: If the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nCovariate (X): If the p-value for (X) is below the significance level, (X) is a significant predictor.\n\n\nDiagnostic Plots:\n\n\nResiduals vs. Fitted: Random scatter suggests homoscedasticity and linearity.\nQ-Q Plot: Points close to the diagonal indicate normality of residuals.\nInfluential Observations: Absence of points with high leverage or large residuals indicates no influential observations.\n\n\nInterpretation of Pairwise Comparisons:\n\n\nSignificant Differences: Significant pairwise differences indicate which specific treatments differ after adjusting for (X).\nAdjusted Means: Provide a clearer comparison by accounting for the covariate’s effect, highlighting the true treatment effects.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#unbalanced-two-factor-ancova-analysis",
    "href": "lectures/week-13_nested-models.html#unbalanced-two-factor-ancova-analysis",
    "title": "Nested Models in Statistical Design",
    "section": "1. Unbalanced Two-Factor ANCOVA Analysis",
    "text": "1. Unbalanced Two-Factor ANCOVA Analysis\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example",
    "href": "lectures/week-13_nested-models.html#r-code-example",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(car)\nset.seed(456)\n\n# Simulate Unbalanced Data\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))FactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n# Simulate Responses with Treatment and Covariate EffectsY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + 0.5 * (X - mean(X)) + rnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA Model\n\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n# Type II Sum of Squares\n\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\n\nprint(typeII)\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#discussion-points",
    "href": "lectures/week-13_nested-models.html#discussion-points",
    "title": "Nested Models in Statistical Design",
    "section": "Discussion Points",
    "text": "Discussion Points\n\nType II vs. Type III SS:\n\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\n\nImpact of Imbalance:\n\n\nType II SS: Provides unbiased estimates of main effects under the assumption of no interaction.\nType III SS: Necessary when interactions are present or when the design is unbalanced, as it accounts for all factors simultaneously.\n\n\nSignificance of Effects:\n\n\nType II SS: May show significant main effects if there’s true variability not explained by interactions.\nType III SS: May adjust significance levels based on interactions, leading to different conclusions.\n\n\nImplications for Experimental Conclusions:\n\n\nChoosing SS Type: Critical for accurate inference; Type I SS is inappropriate due to dependency on factor order.\nMisinterpretation Risks: Incorrect SS type can lead to misleading conclusions about factor significance and interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-insights",
    "href": "lectures/week-13_nested-models.html#expected-insights",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Insights",
    "text": "Expected Insights\n\nType II SS: Suitable when interactions are not of primary interest, providing clear main effect interpretations.\nType III SS: Essential in unbalanced designs or when interactions are present, ensuring main effects are evaluated conditionally.\nImbalance Effects: Can distort Type I SS results, making Type II and III SS more robust alternatives in such scenarios.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#three-factor-interaction-interpretation",
    "href": "lectures/week-13_nested-models.html#three-factor-interaction-interpretation",
    "title": "Nested Models in Statistical Design",
    "section": "2. Three-Factor Interaction Interpretation",
    "text": "2. Three-Factor Interaction Interpretation\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example-1",
    "href": "lectures/week-13_nested-models.html#r-code-example-1",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(emmeans)\n\nlibrary(ggplot2)\n\nset.seed(789)# Simulate Data\n\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\n\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\n\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\n\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate Responses with Three-way Interaction\n\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + 0.5 * (X - mean(X)) + ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0,1) +\n\nifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + rnorm(72, mean=0, sd=3)data_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit Three-way ANCOVA Model\n\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\n\nsummary(fit_three)\n\n# Plot Three-way Interaction\n\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) + geom_point() + geom_line() + facet_wrap(~ FactorC) + labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") + theme_minimal()# Estimated Marginal means\n\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\n\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\n\nsummary(contrast_results)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interpretation-3",
    "href": "lectures/week-13_nested-models.html#interpretation-3",
    "title": "Nested Models in Statistical Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nThree-Way Interaction:\n\n\nVisualization: Non-parallel lines across different levels of Factor C indicate a significant three-way interaction.\nStatistical Significance: Significant three-way interaction in the model summary confirms that the combined effect of Factors A, B, and C deviates from additivity.\n\n\nImpact on Main and Two-Way Interactions:\n\n\nMain Effects: Cannot be interpreted independently as their effects depend on the levels of other factors.\nTwo-Way Interactions: The presence of a significant three-way interaction implies that two-way interactions also vary across the third factor.\n\n\nPractical Implications:\n\n\nComplex Relationships: Understanding how multiple factors interact provides deeper insights into the data.\nDecision Making: Facilitates more informed decisions by recognizing nuanced effect patterns.\n\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#mathematical-proof-of-interaction-contrasts",
    "href": "lectures/week-13_nested-models.html#mathematical-proof-of-interaction-contrasts",
    "title": "Nested Models in Statistical Design",
    "section": "3. Mathematical Proof of Interaction Contrasts",
    "text": "3. Mathematical Proof of Interaction Contrasts\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#proof-outline",
    "href": "lectures/week-13_nested-models.html#proof-outline",
    "title": "Nested Models in Statistical Design",
    "section": "Proof Outline",
    "text": "Proof Outline\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta){ij} + (\\alpha\\gamma){ik} + (\\beta\\gamma){jk} + (\\alpha\\beta\\gamma){ijk} + \\epsilon_{ijk}\n\\]\nWhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\\((\\alpha\\beta)*{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma){ik}\\) and \\((\\beta\\gamma){jk}\\).\nThree-Way Interaction:\n\\((\\alpha\\beta\\gamma)*{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\n\nAdditive Model:\n\nIf all interaction terms are zero (\\((\\alpha\\beta){ij} = (\\alpha\\gamma){ik} = (\\beta\\gamma){jk} = (\\alpha\\beta\\gamma){ijk} = 0\\)), the model simplifies to:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nThis represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model:\n\nIf any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\nConclusion:\n\nInteraction contrasts (\\((\\alpha\\beta){ij}\\), \\((\\alpha\\gamma){ik}\\), \\((\\beta\\gamma){jk}\\), and \\((\\alpha\\beta\\gamma){ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#adjusted-treatment-means",
    "href": "lectures/week-13_nested-models.html#adjusted-treatment-means",
    "title": "Nested Models in Statistical Design",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe Adjusted Mean for treatment (i) accounts for the influence of blocking factors, providing a more accurate estimate of the treatment effect.\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment (i).\n\\(\\bar{Y}_i\\): Mean response for treatment (i).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment (i).\n\\(\\bar{X}\\): Overall mean of the covariate."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#derivation",
    "href": "lectures/week-13_nested-models.html#derivation",
    "title": "Nested Models in Statistical Design",
    "section": "Derivation",
    "text": "Derivation\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment (i):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#f-test-for-homogeneity-of-slopes",
    "href": "lectures/week-13_nested-models.html#f-test-for-homogeneity-of-slopes",
    "title": "Nested Models in Statistical Design",
    "section": "F-Test for Homogeneity of Slopes",
    "text": "F-Test for Homogeneity of Slopes\nTesting the assumption of homogeneity of regression slopes ensures that the relationship between the covariate and the response is consistent across all treatment groups.\nModel Comparison\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + \\epsilon_{hi}\n\\]\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + (\\theta \\tau){hi} + \\epsilon{hi}\n\\]\nWhere \\((\\theta \\tau)_{hi}\\) allows the slope of the covariate to vary with treatments."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#hypothesis-testing",
    "href": "lectures/week-13_nested-models.html#hypothesis-testing",
    "title": "Nested Models in Statistical Design",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nNull Hypothesis ((H_0)): Homogeneity of slopes (\\(\\beta\\) is the same across treatments).\nAlternative Hypothesis ((H_A)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta_j\\) for some (i, j)).\n\nTest Procedure\n\nFit Both Models:\n\n\nReduced Model: Without interaction.\nFull Model: With interaction.\n\n\nCompare Models Using ANOVA:\n\n\n# Fit Reduced Model\nmodel_reduced &lt;- lm(Time ~ fColor + x, data = balloon.data)\n\n# Fit Full Model with Interaction\nmodel_full &lt;- lm(Time ~ fColor * x, data = balloon.data)\n\n# Compare Models\n\nanova(model_reduced, model_full)\n\n\nDecision Rule:\n\n\nIf the interaction term is significant (\\(p &lt; \\alpha\\)), reject (H_0) and conclude that slopes are not homogeneous.\nIf not significant, accept (H_0) and proceed with the reduced model.\n\nConclusion:\nA significant interaction term indicates that the effect of the covariate varies across treatments, violating the homogeneity of slopes assumption and necessitating a more complex model.\nReference:\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-13_nested-models.html#confidence-intervals-for-higher-order-contrasts",
    "title": "Nested Models in Statistical Design",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts provide insights into the magnitude and significance of these interactions.\nFormula:\n\\[\nCI = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#steps-in-r",
    "href": "lectures/week-13_nested-models.html#steps-in-r",
    "title": "Nested Models in Statistical Design",
    "section": "Steps in R",
    "text": "Steps in R\n\nEstimate the Contrast:\n\nUsing the emmeans package to define and estimate contrasts.\n\nCalculate the Confidence Interval:\n\nThe emmeans package automatically provides confidence intervals when performing contrasts."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#example-1",
    "href": "lectures/week-13_nested-models.html#example-1",
    "title": "Nested Models in Statistical Design",
    "section": "Example",
    "text": "Example\n\nlibrary(emmeans)\n\n# Fit the ANCOVA Model\n\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define Estimated Marginal means\n\nemm &lt;- emmeans(model, ~ Treatment)\n\n# Define a Contrast (e.g., Treatment B vs. Treatment A)\n\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\n\nsummary(contrast_AB, infer = TRUE)\n\nInterpretation:\nThe output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#medium-difficulty-1",
    "href": "lectures/week-13_nested-models.html#medium-difficulty-1",
    "title": "Nested Models in Statistical Design",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANCOVA with Interaction\n\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate (X).\nFit an ANCOVA model adjusting for (X).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example-2",
    "href": "lectures/week-13_nested-models.html#r-code-example-2",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nset.seed(123)\n\n# Define Factors and Covariate\n\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\n\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate Responses with Treatment Effects and Covariate Effect\n\nY &lt;- 5 +\n\nifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) +\n\n0.5 * (X - mean(X)) +\n\nrnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA Model\n\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\nsummary(model_sim)\n\n# Diagnostic Plots\n\npar(mfrow = c(2, 2))\n\nplot(model_sim)\n\n# Estimated Marginal means\n\nlibrary(emmeans)\n\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\n\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\n\nsummary(pairwise_sim)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#questions-1",
    "href": "lectures/week-13_nested-models.html#questions-1",
    "title": "Nested Models in Statistical Design",
    "section": "Questions",
    "text": "Questions\n\nMain Effects:\n\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate (X): How does (X) influence the response variable (Y)?\n\n\nSignificance Testing:\n\n\nAre the treatment effects statistically significant after adjusting for (X)?\nIs the covariate (X) a significant predictor of (Y)?\n\n\nDiagnostic Plots:\n\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\n\nInterpretation of Pairwise Comparisons:\n\n\nWhich treatments significantly differ from each other after adjusting for (X)?\nHow do the adjusted means compare across treatments?"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-answers-1",
    "href": "lectures/week-13_nested-models.html#expected-answers-1",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Answers",
    "text": "Expected Answers\n\nMain Effects:\n\n\nTreatment: Treatments B and C have positive effects relative to Treatment A, indicating higher response values.\nCovariate (X): The covariate (X) has a positive influence on (Y), suggesting that as (X) increases, (Y) tends to increase.\n\n\nSignificance Testing:\n\n\nTreatment Effects: If the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nCovariate (X): If the p-value for (X) is below the significance level, (X) is a significant predictor.\n\n\nDiagnostic Plots:\n\n\nResiduals vs. Fitted: Random scatter suggests homoscedasticity and linearity.\nQ-Q Plot: Points close to the diagonal indicate normality of residuals.\nInfluential Observations: Absence of points with high leverage or large residuals indicates no influential observations.\n\n\nInterpretation of Pairwise Comparisons:\n\n\nSignificant Differences: Significant pairwise differences indicate which specific treatments differ after adjusting for (X).\nAdjusted Means: Provide a clearer comparison by accounting for the covariate’s effect, highlighting the true treatment effects.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#unbalanced-two-factor-ancova-analysis-1",
    "href": "lectures/week-13_nested-models.html#unbalanced-two-factor-ancova-analysis-1",
    "title": "Nested Models in Statistical Design",
    "section": "1. Unbalanced Two-Factor ANCOVA Analysis",
    "text": "1. Unbalanced Two-Factor ANCOVA Analysis\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example-3",
    "href": "lectures/week-13_nested-models.html#r-code-example-3",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(car)\n\nset.seed(456)\n\n# Simulate Unbalanced Data\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\n\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\n\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate Responses with Treatment and Covariate Effects\n\nY &lt;- 5 +\n\nifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) +\n\nifelse(FactorB == \"B1\", 0, 3) +\n\n0.5 * (X - mean(X)) +\n\nrnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA Model\n\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\n\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\n\nprint(typeII)\n\n# Type III Sum of Squares\n\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\n\nprint(typeIII)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#discussion-points-1",
    "href": "lectures/week-13_nested-models.html#discussion-points-1",
    "title": "Nested Models in Statistical Design",
    "section": "Discussion Points",
    "text": "Discussion Points\n\nType II vs. Type III SS:\n\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\n\nImpact of Imbalance:\n\n\nType II SS: Provides unbiased estimates of main effects under the assumption of no interaction.\nType III SS: Necessary when interactions are present or when the design is unbalanced, as it accounts for all factors simultaneously.\n\n\nSignificance of Effects:\n\n\nType II SS: May show significant main effects if there’s true variability not explained by interactions.\nType III SS: May adjust significance levels based on interactions, leading to different conclusions.\n\n\nImplications for Experimental Conclusions:\n\n\nChoosing SS Type: Critical for accurate inference; Type I SS is inappropriate due to dependency on factor order.\nMisinterpretation Risks: Incorrect SS type can lead to misleading conclusions about factor significance and interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-insights-1",
    "href": "lectures/week-13_nested-models.html#expected-insights-1",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Insights",
    "text": "Expected Insights\n\nType II SS: Suitable when interactions are not of primary interest, providing clear main effect interpretations.\nType III SS: Essential in unbalanced designs or when interactions are present, ensuring main effects are evaluated conditionally.\nImbalance Effects: Can distort Type I SS results, making Type II and III SS more robust alternatives in such scenarios.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#three-factor-interaction-interpretation-1",
    "href": "lectures/week-13_nested-models.html#three-factor-interaction-interpretation-1",
    "title": "Nested Models in Statistical Design",
    "section": "2. Three-Factor Interaction Interpretation",
    "text": "2. Three-Factor Interaction Interpretation\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example-4",
    "href": "lectures/week-13_nested-models.html#r-code-example-4",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(emmeans)\n\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate Data\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\n\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\n\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\n\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate Responses with Three-way Interaction\n\nY &lt;- 5 +\n\nifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) +\n\nifelse(FactorB == \"B1\", 0, 3) +\n\nifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) +\n\n0.5 * (X - mean(X)) +\n\nifelse(FactorA == \"A1\" & FactorB == \"B1\", 0,\n\nifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) +\n\nrnorm(72, mean=0, sd=3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit Three-way ANCOVA Model\n\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\n\nsummary(fit_three)\n\n# Plot Three-way Interaction\n\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) +\n\ngeom_point() +\n\ngeom_line() +\n\nfacet_wrap(~ FactorC) +\n\nlabs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") +\n\ntheme_minimal()\n\n# Estimated Marginal means\n\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\n\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\n\nsummary(contrast_results)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interpretation-4",
    "href": "lectures/week-13_nested-models.html#interpretation-4",
    "title": "Nested Models in Statistical Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nThree-Way Interaction:\n\n\nVisualization: The interaction plot shows non-parallel lines across different levels of Factor C, indicating a significant three-way interaction.\nStatistical Significance: The model summary reveals a significant three-way interaction, confirming that the combined effect of Factors A, B, and C is not purely additive.\n\n\nImpact on Main and Two-Way Interactions:\n\n\nMain Effects: Cannot be interpreted independently as their effects depend on the levels of other factors.\nTwo-Way Interactions: The presence of a significant three-way interaction implies that two-way interactions also vary across the third factor.\n\n\nPractical Implications:\n\n\nComplex Relationships: Understanding how multiple factors interact provides deeper insights into the data.\nDecision Making: Facilitates more informed decisions by recognizing nuanced effect patterns.\n\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#mathematical-proof-of-interaction-contrasts-1",
    "href": "lectures/week-13_nested-models.html#mathematical-proof-of-interaction-contrasts-1",
    "title": "Nested Models in Statistical Design",
    "section": "3. Mathematical Proof of Interaction Contrasts",
    "text": "3. Mathematical Proof of Interaction Contrasts\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#proof-outline-1",
    "href": "lectures/week-13_nested-models.html#proof-outline-1",
    "title": "Nested Models in Statistical Design",
    "section": "Proof Outline",
    "text": "Proof Outline\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta){ij} + (\\alpha\\gamma){ik} + (\\beta\\gamma){jk} + (\\alpha\\beta\\gamma){ijk} + \\epsilon_{ijk}\n\\]\nWhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\\((\\alpha\\beta)*{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma){ik}\\) and \\((\\beta\\gamma){jk}\\).\nThree-Way Interaction:\n\\((\\alpha\\beta\\gamma)*{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\n\nAdditive Model:\n\nIf all interaction terms are zero (\\((\\alpha\\beta){ij} = (\\alpha\\gamma){ik} = (\\beta\\gamma){jk} = (\\alpha\\beta\\gamma){ijk} = 0\\)), the model simplifies to:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nThis represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model:\n\nIf any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\nConclusion:\n\nInteraction contrasts (\\((\\alpha\\beta){ij}\\), \\((\\alpha\\gamma){ik}\\), \\((\\beta\\gamma){jk}\\), and \\((\\alpha\\beta\\gamma){ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-mean-squares-for-nested-models",
    "href": "lectures/week-13_nested-models.html#expected-mean-squares-for-nested-models",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Mean Squares for Nested Models",
    "text": "Expected Mean Squares for Nested Models\nFor the model \\(Y_{ijt} = \\mu + \\alpha_i + \\beta_j(i) + \\epsilon_{ijt}\\), the Expected Mean Squares (EMS) are:\n\\[\nEMS_A = \\sigma^2 + r \\sigma^2_B + r b Q(\\alpha_i)\n\\]\n\\[\nEMS_B(A) = \\sigma^2 + r \\sigma^2_B\n\\]\n\\[\nEMS_E = \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#derivation-steps-1",
    "href": "lectures/week-13_nested-models.html#derivation-steps-1",
    "title": "Nested Models in Statistical Design",
    "section": "Derivation Steps",
    "text": "Derivation Steps\n\nAssumptions:\n\n\nFixed Effects: \\(\\alpha_i\\) and \\(\\beta_j(i)\\) are fixed.\nRandom Error: \\(\\epsilon_{ijt} \\sim N(0, \\sigma^2)\\).\nReplicates: \\(r\\) replicates per nested factor.\n\n\nVariance Components:\n\n\nBetween (A) Groups: Variance due to Factor (A) is captured by \\(\\alpha_i\\).\nBetween (B(A)) Groups: Variance due to Factor (B) nested within (A) is \\(\\sigma^2_B\\).\nWithin (B(A)) Groups: Residual variance is \\(\\sigma^2\\).\n\n\nCalculation of EMS:\n\n\nEMS for Factor (A):\n\n\\[\nEMS_A = Var(\\alpha_i) + Var(\\beta_j(i)) + Var(\\epsilon_{ijt}) = \\sigma^2 + r \\sigma^2_B + r b Q(\\alpha_i)\n\\]\n\nEMS for Factor (B(A)):\n\n\\[\nEMS_B(A) = Var(\\beta_j(i)) + Var(\\epsilon_{ijt}) = \\sigma^2 + r \\sigma^2_B\n\\]\n\nEMS for Error:\n\n\\[\nEMS_E = Var(\\epsilon_{ijt}) = \\sigma^2\n\\]\n\nEstimation of Variance Components:\n\nUsing the ANOVA table, mean squares are equated to EMS to solve for \\(\\sigma^2_B\\) and \\(\\sigma^2\\):\n\\[\n\\hat{\\sigma}^2_B = \\frac{MS_A - MS_E}{r}\n\\]\n\\[\n\\hat{\\sigma}^2 = MS_E\n\\]\nWhere:\n\n\\(MS_A\\): Mean Square for Factor (A).\n\\(MS_E\\): Mean Square for Error.\n\nConclusion:\nThe derivation demonstrates how ANOVA decomposes total variability into components attributable to nested factors and residual error, enabling the estimation of variance components.\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interaction-contrasts-measure-deviations-from-additivity",
    "href": "lectures/week-13_nested-models.html#interaction-contrasts-measure-deviations-from-additivity",
    "title": "Nested Models in Statistical Design",
    "section": "Interaction Contrasts Measure Deviations from Additivity",
    "text": "Interaction Contrasts Measure Deviations from Additivity\nDefinition:\nIn a three-factor design, interaction contrasts quantify how the combined effect of three factors deviates from the additive effects of each factor individually and their two-way interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#proof-steps",
    "href": "lectures/week-13_nested-models.html#proof-steps",
    "title": "Nested Models in Statistical Design",
    "section": "Proof Steps",
    "text": "Proof Steps\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta){ij} + (\\alpha\\gamma){ik} + (\\beta\\gamma){jk} + (\\alpha\\beta\\gamma){ijk} + \\epsilon_{ijk}\n\\]\nWhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\\((\\alpha\\beta)*{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma){ik}\\) and \\((\\beta\\gamma){jk}\\).\nThree-Way Interaction:\n\\((\\alpha\\beta\\gamma)*{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\n\nAdditive Model:\n\nIf all interaction terms are zero (\\((\\alpha\\beta){ij} = (\\alpha\\gamma){ik} = (\\beta\\gamma){jk} = (\\alpha\\beta\\gamma){ijk} = 0\\)), the model simplifies to:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nThis represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model:\n\nIf any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\nConclusion:\n\nInteraction contrasts (\\((\\alpha\\beta){ij}\\), \\((\\alpha\\gamma){ik}\\), \\((\\beta\\gamma){jk}\\), and \\((\\alpha\\beta\\gamma){ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#references",
    "href": "lectures/week-13_nested-models.html#references",
    "title": "Nested Models in Statistical Design",
    "section": "References",
    "text": "References\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley.\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\nKempthorne, O. (1977). Design and Analysis of Experiments. Robert E. Krieger Publishing Company.\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html",
    "href": "lectures/week-11_row-column.html",
    "title": "Row and Column Designs",
    "section": "",
    "text": "In the realm of experimental design, Row–Column Designs serve as an extension of traditional blocking techniques by incorporating two independent blocking factors: rows and columns. These designs are instrumental in controlling multiple sources of variability, thereby enhancing the precision and efficiency of treatment comparisons. By organizing experimental units into a matrix of rows and columns, Row–Column Designs mitigate the influence of two distinct nuisance factors, enabling clearer insights into the effects of the primary treatments under investigation.\nIntuitive Example: Imagine conducting an agricultural experiment to test different crop varieties. The field is naturally divided into rows and columns due to varying sunlight and soil moisture gradients. By applying each variety once per row and column, you control for these environmental gradients, ensuring that differences in crop performance are attributable to the varieties themselves rather than external factors.\nThis lecture delves into the structure, analysis, and practical applications of Row–Column Designs, with a focus on specific examples like Latin Square Designs and Youden Designs. We will explore theoretical foundations, implement analyses using R, and engage with advanced topics to provide a comprehensive understanding suitable for graduate-level studies.\n\n\n\nUnderstand the principles and utility of Row–Column Designs.\nExplore specific examples such as Latin Square and Youden Designs.\nAnalyze data using ANOVA tailored for Row–Column Designs.\nExtend designs to accommodate factorial experiments.\nImplement Row–Column Designs in R with step-by-step guidance.\nReinforce concepts through exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-11_row-column.html#objectives",
    "href": "lectures/week-11_row-column.html#objectives",
    "title": "Row and Column Designs",
    "section": "",
    "text": "Understand the principles and utility of Row–Column Designs.\nExplore specific examples such as Latin Square and Youden Designs.\nAnalyze data using ANOVA tailored for Row–Column Designs.\nExtend designs to accommodate factorial experiments.\nImplement Row–Column Designs in R with step-by-step guidance.\nReinforce concepts through exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-11_row-column.html#rowcolumn-design-structure",
    "href": "lectures/week-11_row-column.html#rowcolumn-design-structure",
    "title": "Row and Column Designs",
    "section": "Row–Column Design Structure",
    "text": "Row–Column Design Structure\nA Row–Column Design arranges experimental units in a two-dimensional grid comprising rows and columns. Each cell in the grid represents an experimental unit to which a treatment is applied. This design incorporates two independent blocking factors:\n\nRows: Control for variability along the horizontal dimension.\nColumns: Control for variability along the vertical dimension.\n\nDiagram of a 3x3 Latin Square:\n\n\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\n\nRow 1\nA\nB\nC\n\n\nRow 2\nB\nC\nA\n\n\nRow 3\nC\nA\nB"
  },
  {
    "objectID": "lectures/week-11_row-column.html#blocking-factors",
    "href": "lectures/week-11_row-column.html#blocking-factors",
    "title": "Row and Column Designs",
    "section": "Blocking Factors",
    "text": "Blocking Factors\n\nRow Blocks: Address systematic variability across rows, such as gradients in environmental conditions.\nColumn Blocks: Address systematic variability across columns, such as variations in soil composition.\n\nIntuitive Explanation: Consider a greenhouse experiment where temperature gradients run horizontally and light intensity gradients run vertically. By arranging treatments in a row–column grid, you control for these two sources of environmental variability, ensuring that treatment effects are not confounded with spatial gradients."
  },
  {
    "objectID": "lectures/week-11_row-column.html#advantages",
    "href": "lectures/week-11_row-column.html#advantages",
    "title": "Row and Column Designs",
    "section": "Advantages",
    "text": "Advantages\n\nReduced Variability: By controlling two sources of variability, Row–Column Designs offer greater precision in estimating treatment effects.\nEfficiency: These designs make efficient use of experimental units, especially in small-scale experiments with inherent multidimensional variability.\nFlexibility: They can be extended to accommodate factorial experiments and multiple interactions.\nEnhanced Analytical Power: ANOVA tailored for Row–Column Designs can effectively isolate treatment effects from block effects.\n\nReference: Montgomery, D. C., Peck, E. A., & Vining, G. G. (2020). Introduction to Linear Regression Analysis. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html#definition",
    "href": "lectures/week-11_row-column.html#definition",
    "title": "Row and Column Designs",
    "section": "Definition",
    "text": "Definition\nA Latin Square Design is a specialized type of Row–Column Design where:\n\nEach treatment appears exactly once in each row.\nEach treatment appears exactly once in each column.\n\nThis ensures that the design is balanced and controls for both row and column effects effectively."
  },
  {
    "objectID": "lectures/week-11_row-column.html#construction",
    "href": "lectures/week-11_row-column.html#construction",
    "title": "Row and Column Designs",
    "section": "Construction",
    "text": "Construction\n\nStandard Latin Squares:\nFor \\(v = 3\\) treatments, a Latin Square can be constructed as follows:\n\n\\[\n\\begin{array}{|c|c|c|}\n   \\hline\n   A & B & C \\\\\n   \\hline\n   B & C & A \\\\\n   \\hline\n   C & A & B \\\\\n   \\hline\n   \\end{array}\n\\]\n\nRows and Columns: Each treatment \\(A\\), \\(B\\), and \\(C\\) appears once per row and column.\n\n\nRandomization:\nTo prevent systematic bias, randomize the order of treatments within rows and columns. This can be achieved by randomly permuting the treatment labels for rows and columns."
  },
  {
    "objectID": "lectures/week-11_row-column.html#model",
    "href": "lectures/week-11_row-column.html#model",
    "title": "Row and Column Designs",
    "section": "Model",
    "text": "Model\nFor a Row–Column Design with \\(b\\) rows, \\(c\\) columns, and \\(v\\) treatments, the ANOVA model is defined as:\n\\[\nY_{hqi} = \\mu + \\theta_h + \\phi_q + \\tau_i + \\epsilon_{hqi},\n\\]\nwhere:\n\n\\(Y_{hqi}\\): Response for treatment \\(i\\) in row \\(h\\) and column \\(q\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of row \\(h\\).\n\\(\\phi_q\\): Effect of column \\(q\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hqi} \\sim N(0, \\sigma^2)\\): Random error term.\n\n\nKey Assumptions\n\nNormality: The residuals (\\(\\epsilon_{hqi}\\)) are normally distributed.\nHomogeneity of Variance (Homoscedasticity): The variance of residuals is constant across all treatments, rows, and columns.\nIndependence: Observations are independent of each other.\nNo Interaction: There is no interaction between rows and treatments or columns and treatments, i.e., the effects of treatments are consistent across rows and columns.\n\n\n\nANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegrees of Freedom (DF)\nSum of Squares (SS)\nMean Square (MS)\nF-ratio\n\n\n\n\nRows\n\\(b - 1\\)\n\\(SS_{\\text{Rows}}\\)\n\\(MS_{\\text{Rows}} = \\frac{SS_{\\text{Rows}}}{b - 1}\\)\n\\(F = \\frac{MS_{\\text{Rows}}}{MS_{\\text{Error}}}\\)\n\n\nColumns\n\\(c - 1\\)\n\\(SS_{\\text{Columns}}\\)\n\\(MS_{\\text{Columns}} = \\frac{SS_{\\text{Columns}}}{c - 1}\\)\n\\(F = \\frac{MS_{\\text{Columns}}}{MS_{\\text{Error}}}\\)\n\n\nTreatments adj\n\\(v - 1\\)\n\\(SS_{\\text{Trt adj}}\\)\n\\(MS_{\\text{Trt adj}} = \\frac{SS_{\\text{Trt adj}}}{v - 1}\\)\n\\(F = \\frac{MS_{\\text{Treatments}}}{MS_{\\text{Error}}}\\)\n\n\nError\n\\(bc - b - c - v + 2\\)\n\\(SS_{\\text{Error}}\\)\n\\(MS_{\\text{Error}} = \\frac{SS_{\\text{Error}}}{bc - b - c - v + 2}\\)\n-\n\n\nTotal\n\\(b c - 1\\)\n\\(SS_{\\text{Total}}\\)\n-\n-"
  },
  {
    "objectID": "lectures/week-11_row-column.html#hypothesis-testing",
    "href": "lectures/week-11_row-column.html#hypothesis-testing",
    "title": "Row and Column Designs",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): All treatment means are equal (\\(\\tau_i = 0\\) for all \\(i\\)).\nAlternative Hypothesis (\\(H_A\\)): At least one treatment mean differs.\n\nDecision Rule: Reject \\(H_0\\) if the F-ratio for treatments exceeds the critical value from the F-distribution with degrees of freedom \\((v - 1, bc - b - c - v + 2)\\) at the chosen significance level (\\(\\alpha\\))."
  },
  {
    "objectID": "lectures/week-11_row-column.html#example-interpretation",
    "href": "lectures/week-11_row-column.html#example-interpretation",
    "title": "Row and Column Designs",
    "section": "Example Interpretation",
    "text": "Example Interpretation\n\nRows and Columns: Assess the significance of row and column effects. Significant row or column effects indicate that these blocking factors explain a substantial portion of the variability in the response.\nTreatments: Determine if treatments have a significant impact on the response variable after accounting for row and column effects.\nError: Represents unexplained variability, serving as the denominator for F-ratios."
  },
  {
    "objectID": "lectures/week-11_row-column.html#computational-formula",
    "href": "lectures/week-11_row-column.html#computational-formula",
    "title": "Row and Column Designs",
    "section": "Computational Formula",
    "text": "Computational Formula\nThese formulas typically apply to the Analysis of Variance (ANOVA) for experimental designs involving treatments laid out in a structure with rows and columns, aiming to control for variability from these two sources (e.g., Latin Squares, Youden Squares, Row-Column Designs).\n\n\\(b\\): Number of rows\n\\(c\\): Number of columns\n\\(v\\): Number of treatments\n\\(y_{hq}\\): Observation in row \\(h\\) and column \\(q\\)\n\\(t(h,q)\\): Treatment applied in cell \\((h,q)\\)\n\\(N = bc\\): Total number of observations (assuming a complete layout)\n\\(B_h = \\sum_{q=1}^{c} y_{hq}\\): Total for row \\(h\\) (sum of \\(c\\) observations)\n\\(C_q = \\sum_{h=1}^{b} y_{hq}\\): Total for column \\(q\\) (sum of \\(b\\) observations)\n\\(T_i = \\sum_{(h,q): t(h,q)=i} y_{hq}\\): Total for treatment \\(i\\)\n\\(G = \\sum_{h=1}^{b} \\sum_{q=1}^{c} y_{hq}\\): Grand total (sum of \\(N=bc\\) observations)\n\\(r\\): Number of replications per treatment (assumed constant in some provided formulas, but could be \\(r_i\\))\n\\(n_{h.i}\\): Number of times treatment \\(i\\) appears in row \\(h\\)\n\\(n_{.qi}\\): Number of times treatment \\(i\\) appears in column \\(q\\)\n\\(CF = \\frac{G^2}{N} = \\frac{G^2}{bc}\\): Correction Factor\n\nNote. We have \\(r\\) replications per treatment in a complete design, but in some cases, \\(r_i\\) may vary. The formulas provided assume a complete layout with equal replication. ## Unadjusted Row Effect Sum of Squares (\\(SS_\\theta\\))\n\\[\nSS_{Rows} = \\frac{1}{c} \\sum_{h=1}^{b} B_h^2 - \\frac{2 G^2}{bc} + \\frac{b G^2}{bc} = \\frac{1}{c} \\sum_{h=1}^{b} B_h^2 - \\frac{G^2}{bc}\n\\]"
  },
  {
    "objectID": "lectures/week-11_row-column.html#derivation-of-row-effect-sum-of-squares",
    "href": "lectures/week-11_row-column.html#derivation-of-row-effect-sum-of-squares",
    "title": "Row and Column Designs",
    "section": "Derivation of Row Effect Sum of Squares",
    "text": "Derivation of Row Effect Sum of Squares\nThe unadjusted sum of squares for rows measures the variability between row means.\n\nRow mean \\(h\\): \\(\\bar{y}_{h.} = B_h / c\\)\nGrand mean: \\(\\bar{y}_{..} = G / N = G / (bc)\\)\nThe sum of squares is the sum of squared deviations from the grand mean, weighted by the number of observations per row:\n\n\\[\nSS_{Rows} = \\sum_{h=1}^{b} \\sum_{q=1}^{c} (\\bar{y}_{h.} - \\bar{y}_{..})^2 = \\sum_{h=1}^{b} c (\\bar{y}_{h.} - \\bar{y}_{..})^2\n\\]\n\nSubstitute the means:\n\n\\[\nSS_{Rows} = \\sum_{h=1}^{b} c \\left(\\frac{B_h}{c} - \\frac{G}{bc}\\right)^2 = \\sum_{h=1}^{b} c \\left[\\frac{B_h^2}{c^2} - \\frac{2 B_h G}{bc^2} + \\frac{G^2}{(bc)^2}\\right]\n\\]\n\nDistribute the sum and simplify:\n\n\\[\nSS_{Rows} = \\sum_{h=1}^{b} \\left[\\frac{B_h^2}{c} - \\frac{2 B_h G}{bc} + \\frac{c G^2}{b^2 c^2}\\right]\n\\]\n\\[\nSS_{Rows} = \\left(\\frac{1}{c} \\sum_{h=1}^{b} B_h^2\\right) - \\frac{2 G}{bc} \\left(\\sum_{h=1}^{b} B_h\\right) + \\sum_{h=1}^{b} \\frac{G^2}{bc}\n\\]\n\nSince \\(\\sum B_h = G\\) and the last term sums \\(b\\) times:\n\n\\[\nSS_{Rows} = \\frac{1}{c} \\sum_{h=1}^{b} B_h^2 - \\frac{2 G^2}{bc} + \\frac{b G^2}{bc} = \\frac{1}{c} \\sum_{h=1}^{b} B_h^2 - \\frac{G^2}{bc}\n\\]\n\nThe formula’s \\(1/c\\) factor arises naturally because each \\(B_h\\) is the sum of \\(c\\) observations. Dividing the squared total \\(B_h^2\\) by \\(c\\) scales it appropriately for the sum of squares calculation based on means."
  },
  {
    "objectID": "lectures/week-11_row-column.html#unadjusted-column-effect-sum-of-squares-ss_phi",
    "href": "lectures/week-11_row-column.html#unadjusted-column-effect-sum-of-squares-ss_phi",
    "title": "Row and Column Designs",
    "section": "Unadjusted Column Effect Sum of Squares (\\(ss_\\phi\\))",
    "text": "Unadjusted Column Effect Sum of Squares (\\(ss_\\phi\\))\n\\[\nSS_{Cols} = \\frac{1}{b} \\sum_{q=1}^{c} C_q^2 - \\frac{G^2}{bc}\n\\]\n\nDerivation of Standard Formula:\n\nThe derivation is analogous to the Row SS derivation, swapping roles of rows and columns.\n\nColumn mean \\(q\\): \\(\\bar{y}_{.q} = C_q / b\\)\nGrand mean: \\(\\bar{y}_{..} = G / (bc)\\)\nSum of squares:\n\n\\[\nSS_{Cols} = \\sum_{q=1}^{c} \\sum_{h=1}^{b} (\\bar{y}_{.q} - \\bar{y}_{..})^2 = \\sum_{q=1}^{c} b (\\bar{y}_{.q} - \\bar{y}_{..})^2\n\\]\n\nFollowing similar algebraic steps as for \\(SS_{Rows}\\) leads to:\n\n\\[\nSS_{Cols} = \\frac{1}{b} \\sum_{q=1}^{c} C_q^2 - \\frac{G^2}{bc}\n\\]\n\nTreatment Effect (Adjusted for Rows and Columns)\nAdjustment is needed because treatments are generally not orthogonal to rows and columns. We first find adjusted treatment totals (\\(Q_i\\)) and then use them to calculate the adjusted treatment sum of squares (\\(SS_\\text{Trt adju}\\)).\n\nAdjusted Treatment Totals (\\(Q_i\\))\n\\[\nQ_i = T_i - \\frac{1}{c} \\sum_{h=1}^{b} n_{h.i} B_h - \\frac{1}{b} \\sum_{q=1}^{c} n_{.qi} C_q + \\frac{b}{c} r_i G\n\\]\nwhere\n\n\\(T_i\\): Total for treatment \\(i\\)\n\\(B_h\\): Total for row \\(h\\)\n\\(C_q\\): Total for column \\(q\\)\n\\(G\\): Grand total\n\\(h\\): Row index\n\\(q\\): Column index\n\\(n_{h.i}\\): Number of times treatment \\(i\\) appears in row \\(h\\)\n\\(n_{.qi}\\): Number of times treatment \\(i\\) appears in column \\(q\\)\n\nNote. This uses potentially variable replication \\(r_i\\). If \\(r\\) is constant, \\(r_i\\) becomes \\(r\\). This formula assumes \\(r_i\\) replications per treatment."
  },
  {
    "objectID": "lectures/week-11_row-column.html#explanation-of-standard-formula",
    "href": "lectures/week-11_row-column.html#explanation-of-standard-formula",
    "title": "Row and Column Designs",
    "section": "Explanation of Standard Formula:",
    "text": "Explanation of Standard Formula:\n\nThe adjusted total \\(Q_i\\) is the raw treatment total$ T_i$ corrected for the effects of the specific rows and columns where treatment \\(i\\) appeared.\nThe adjustment subtracts the estimated contribution of row means (\\(B_h/c\\)) and column means (\\(C_q/b\\)) for the units receiving treatment i and adds back the overall mean effect (\\(G/bc\\)) scaled by the number of replications (\\(r_i\\)).\nThis quantity arises naturally from the normal equations for estimating treatment effects (\\(\\tau_i\\)) in the linear model \\(y_{hq(i)} = \\mu + \\theta_h + \\phi_q + \\tau_i + \\epsilon_{hq(i)}\\) after eliminating row and column parameters. The \\(Q_i\\) form the right-hand side of the reduced normal equations for the treatment effects."
  },
  {
    "objectID": "lectures/week-11_row-column.html#adjusted-treatment-sum-of-squares-ss_texttrt-adj",
    "href": "lectures/week-11_row-column.html#adjusted-treatment-sum-of-squares-ss_texttrt-adj",
    "title": "Row and Column Designs",
    "section": "Adjusted Treatment Sum of Squares (\\(SS_\\text{Trt adj}\\))",
    "text": "Adjusted Treatment Sum of Squares (\\(SS_\\text{Trt adj}\\))\n\\[\nSS_\\text{Trt adj} = \\sum_{i=1}^{v} Q_i \\hat{\\tau}_i\n\\]\nwhere:\n\n\\(ss_{Tadj}\\): Adjusted treatment sum of squares\n\\(Q_i\\): Adjusted treatment total for treatment \\(i\\)\n\\(\\hat{\\tau}_i\\): Estimated treatment effect for treatment \\(i\\)\n\\(v\\): Number of treatments\n\\(\\hat{\\tau}_i\\): Estimated treatment effect (solution to the reduced normal equations)"
  },
  {
    "objectID": "lectures/week-11_row-column.html#explanation-of-standard-formula-1",
    "href": "lectures/week-11_row-column.html#explanation-of-standard-formula-1",
    "title": "Row and Column Designs",
    "section": "Explanation of Standard Formula:",
    "text": "Explanation of Standard Formula:\n\nThis formula arises directly from the theory of linear models.\nThe adjusted sum of squares for a factor (here, treatments) can be expressed as the quadratic form \\(\\hat{\\tau}' Q\\), where \\(\\hat{\\tau}\\) is the vector of estimated treatment effects (solutions to the reduced normal equations \\(C \\hat{\\tau} = Q\\)) and \\(Q\\) is the vector of adjusted totals.\nExpanding \\(\\hat{\\tau}' Q\\) gives the summation \\(\\sum Q_i \\hat{\\tau}_i\\)."
  },
  {
    "objectID": "lectures/week-11_row-column.html#analysis-of-variance-anova",
    "href": "lectures/week-11_row-column.html#analysis-of-variance-anova",
    "title": "Row and Column Designs",
    "section": "Analysis of Variance (ANOVA)",
    "text": "Analysis of Variance (ANOVA)\nFor a Latin Square Design with \\(v\\) treatments, the ANOVA model includes:\n\\[\nY_{hqi} = \\mu + \\theta_h + \\phi_q + \\tau_i + \\epsilon_{hqi},\n\\]\nwhere:\n\n\\(Y_{hqi}\\): Response for treatment \\(i\\) in row \\(h\\) and column \\(q\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of row \\(h\\).\n\\(\\phi_q\\): Effect of column \\(q\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hqi} \\sim N(0, \\sigma^2)\\): Random error term.\n\nIn a classical (fully balanced) Latin Square, the treatment sum of squares is the same whether you view it as “adjusted” (after removing row and column effects) or “unadjusted” (looking at treatments alone). In other words, they do not differ."
  },
  {
    "objectID": "lectures/week-11_row-column.html#why-they-are-the-same",
    "href": "lectures/week-11_row-column.html#why-they-are-the-same",
    "title": "Row and Column Designs",
    "section": "Why They Are the Same",
    "text": "Why They Are the Same\n\nOrthogonality\nIn a Latin Square, the design is constructed so that treatments are orthogonal (statistically independent) to the row and column factors. Orthogonality means that when you estimate the effect of treatments, you are not “borrowing” or “confounding” variation that belongs to the row or column blocks.\nBalanced Cells\nEach treatment occurs exactly once in every row and once in every column, so there is no overlap in the sum of squares partition:\n\\[\n\\text{SSTotal} \\;=\\; \\text{SSRows} \\;+\\; \\text{SSColumns} \\;+\\; \\text{SSTreatments} \\;+\\; \\text{SSError}.\n\\] Because of this strict balance, the decomposition of sums of squares is clean and unique.\nAdjusted = Unadjusted for Orthogonal Designs\nWhen factors are orthogonal, “partial” or “adjusted” sum of squares for a given factor (e.g. treatments) equals the “unadjusted” sum of squares you would get if you calculated it before removing the other factors. No matter the sequence in which you enter factors into the model, the SS for each orthogonal factor is unchanged."
  },
  {
    "objectID": "lectures/week-11_row-column.html#when-they-might-differ",
    "href": "lectures/week-11_row-column.html#when-they-might-differ",
    "title": "Row and Column Designs",
    "section": "When They Might Differ",
    "text": "When They Might Differ\n\nMissing Data or Unbalanced Layout\nIf some runs are missing or the design is not balanced, the orthogonality breaks. Then the “adjusted” (Type III or partial) sum of squares for treatments can differ from the “unadjusted” (Type I or sequential) sum of squares.\nNon-orthogonal Blocks/Treatments\nIf blocking factors and treatments are not arranged in an orthogonal way (as in some incomplete-block or more complex designs), you can again see discrepancies.\n\nIn the standard (complete, balanced) Latin Square, however, there is perfect orthogonality among row blocks, column blocks, and treatments—so the treatment SS is invariant to whether you consider it “adjusted” or “unadjusted.”"
  },
  {
    "objectID": "lectures/week-11_row-column.html#example-anova-table-for-latin-square",
    "href": "lectures/week-11_row-column.html#example-anova-table-for-latin-square",
    "title": "Row and Column Designs",
    "section": "Example ANOVA Table for Latin Square",
    "text": "Example ANOVA Table for Latin Square\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegrees of Freedom\nSum of Squares\nMean Square\nF-ratio\n\n\n\n\nRows\n\\(v - 1\\)\n\\(SS_\\text{Rows}\\)\n\\(MS_\\text{Rows}\\)\n\\(F = \\frac{MS_\\text{Rows}}{MS_\\text{Error}}\\)\n\n\nColumns\n\\(v - 1\\)\n\\(SS_\\text{Columns}\\)\n\\(MS_\\text{Columns}\\)\n\\(F = \\frac{MS_\\text{Columns}}{MS_\\text{Error}}\\)\n\n\nTreatments\n\\(v - 1\\)\n\\(SS_\\text{Trt}\\)\n\\(MS_\\text{Trt}\\)\n\\(F = \\frac{MS_\\text{Trt}}{MS_\\text{Error}}\\)\n\n\nError\n\\((v-2)(v-1)\\)\n\\(SS_\\text{Error}\\)\n\\(MS_\\text{Error}\\)\n\n\n\nTotal\n\\(v^2 - 1\\)\n\\(SS_\\text{Total}\\)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOrthogonality: In a Latin Square, the design is constructed so that treatments are orthogonal (statistically independent) to the row and column factors. As a result, when you estimate the effect of treatments, you are not “borrowing” or “confounding” variation that belongs to the row or column blocks.\n\n\nHypothesis Testing:\n\nNull Hypothesis (\\(H_0\\)): All treatment means are equal (\\(\\tau_k = 0\\) for all \\(k\\)).\nAlternative Hypothesis (\\(H_A\\)): At least one treatment mean differs.\n\nDecision Rule: Reject \\(H_0\\) if the F-ratio for treatments exceeds the critical value from the F-distribution with degrees of freedom \\((v - 1, (v-2)(v-1))\\) at the chosen significance level (\\(\\alpha\\))."
  },
  {
    "objectID": "lectures/week-11_row-column.html#example-in-r",
    "href": "lectures/week-11_row-column.html#example-in-r",
    "title": "Row and Column Designs",
    "section": "Example in R",
    "text": "Example in R\n\n# Simulated Data for Latin Square Design\nset.seed(123)\ndata &lt;- data.frame(\n    row = factor(rep(1:3, each = 3)),\n    column = factor(rep(1:3, times = 3)),\n    treatment = factor(c(\"A\", \"B\", \"C\", \"B\", \"C\", \"A\", \"C\", \"A\", \"B\")),\n    response = c(10.2, 15.3, 20.1, 14.8, 19.5, 9.7, 19.0, 9.9, 14.2)\n)\n\n# Fit ANOVA Model\nfit &lt;- lm(response ~ row + column + treatment, data = data)\nanova(fit)\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nrow\n2\n1.0688889\n0.5344444\n6.5890411\n0.1317690\n\n\ncolumn\n2\n0.1088889\n0.0544444\n0.6712329\n0.5983607\n\n\ntreatment\n2\n138.2422222\n69.1211111\n852.1780822\n0.0011721\n\n\nResiduals\n2\n0.1622222\n0.0811111\nNA\nNA\n\n\n\n\n\n\n\nInterpretation:\n\nRows and Columns: Control for row and column effects, ensuring that treatment comparisons are not confounded by these blocking factors.\nTreatments: Assess the main effect of treatments on the response variable after accounting for row and column effects.\nBecause the Latin Square is fully balanced, the treatment SS is invariant to whether you consider it “adjusted” or “unadjusted.”\nError: Represents variability not explained by rows, columns, or treatments."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html",
    "href": "lectures/week-10_ancova-part2.html",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "",
    "text": "In statistical analysis, Analysis of Covariance (ANCOVA) serves as a powerful tool that combines the principles of ANOVA (Analysis of Variance) and regression. ANCOVA allows researchers to adjust the response variable for one or more covariates (also known as nuisance variables) that are related to the response but are not influenced by the treatments. This adjustment enhances the precision of treatment effect estimates and controls for potential confounding factors.\nIntuition: Imagine you are a teacher assessing the effectiveness of different teaching methods (treatments) on students’ final exam scores (response). However, students come with varying levels of prior knowledge (covariate). By using ANCOVA, you can adjust the final scores based on their initial knowledge, providing a clearer picture of how each teaching method truly affects performance.\nThis lecture delves into the theoretical foundations, assumptions, and applications of ANCOVA. We will explore practical implementations using R, ensuring a balance between theoretical depth and applied understanding. References to classical texts will support and deepen the content, providing a rigorous academic framework.\n\n\n\nUnderstand the purpose and application of ANCOVA.\nExplore the mathematical model and assumptions underlying ANCOVA.\nLearn methods to test and interpret treatment effects adjusted for covariates.\nUse R to perform ANCOVA and evaluate its assumptions.\nExtend the model to unbalanced data and multiple covariates.\nReinforce concepts with exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix.\n\n\n\n\n\npacman::p_load(\n    car, # Companion to Applied Regression\n    emmeans, # Estimated Marginal Means\n    lmtest, # Tests for Linear Models\n    ggplot2, # Data visualization\n    dplyr, # Data manipulation\n    ggpubr, # Publication-ready plots\n    easystats, # Easy statistical analysis\n    here # File path management\n)\n# Set theme for ggplot2\ntheme_set(see::theme_modern())\n\n\n# Load the balloon data\nballoon_df &lt;- read.table(\n    here(\"data\", \"dean2017\", \"balloon.txt\"),\n    header = TRUE\n)\n# Prepare the data: center the covariate and convert treatment to factor\n\nballoon_df &lt;- balloon_df |&gt;\n    mutate(\n        Order_c = Order - mean(Order), # Center the covariate\n        fColor = factor(Color, labels = c(\"pink\", \"yellow\", \"orange\", \"blue\")) # Convert treatment to factor\n    )\n\n# Display the first few rows of the dataset\nhead(balloon_df)\n\n\n\n\n\n\n\n\nOrder\nColor\nTime\nOrder_c\nfColor\n\n\n\n\n1\n1\n22.0\n-15.5\npink\n\n\n2\n3\n24.6\n-14.5\norange\n\n\n3\n1\n20.3\n-13.5\npink\n\n\n4\n4\n19.8\n-12.5\nblue\n\n\n5\n3\n24.3\n-11.5\norange\n\n\n6\n2\n22.2\n-10.5\nyellow\n\n\n\n\n\n\n\nTable 1: Balloon Inflation Times"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#objectives",
    "href": "lectures/week-10_ancova-part2.html#objectives",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "",
    "text": "Understand the purpose and application of ANCOVA.\nExplore the mathematical model and assumptions underlying ANCOVA.\nLearn methods to test and interpret treatment effects adjusted for covariates.\nUse R to perform ANCOVA and evaluate its assumptions.\nExtend the model to unbalanced data and multiple covariates.\nReinforce concepts with exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#packages-used",
    "href": "lectures/week-10_ancova-part2.html#packages-used",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "",
    "text": "pacman::p_load(\n    car, # Companion to Applied Regression\n    emmeans, # Estimated Marginal Means\n    lmtest, # Tests for Linear Models\n    ggplot2, # Data visualization\n    dplyr, # Data manipulation\n    ggpubr, # Publication-ready plots\n    easystats, # Easy statistical analysis\n    here # File path management\n)\n# Set theme for ggplot2\ntheme_set(see::theme_modern())\n\n\n# Load the balloon data\nballoon_df &lt;- read.table(\n    here(\"data\", \"dean2017\", \"balloon.txt\"),\n    header = TRUE\n)\n# Prepare the data: center the covariate and convert treatment to factor\n\nballoon_df &lt;- balloon_df |&gt;\n    mutate(\n        Order_c = Order - mean(Order), # Center the covariate\n        fColor = factor(Color, labels = c(\"pink\", \"yellow\", \"orange\", \"blue\")) # Convert treatment to factor\n    )\n\n# Display the first few rows of the dataset\nhead(balloon_df)\n\n\n\n\n\n\n\n\nOrder\nColor\nTime\nOrder_c\nfColor\n\n\n\n\n1\n1\n22.0\n-15.5\npink\n\n\n2\n3\n24.6\n-14.5\norange\n\n\n3\n1\n20.3\n-13.5\npink\n\n\n4\n4\n19.8\n-12.5\nblue\n\n\n5\n3\n24.3\n-11.5\norange\n\n\n6\n2\n22.2\n-10.5\nyellow\n\n\n\n\n\n\n\nTable 1: Balloon Inflation Times"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#why-ancova",
    "href": "lectures/week-10_ancova-part2.html#why-ancova",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Why ANCOVA?",
    "text": "Why ANCOVA?\nANCOVA addresses several key scenarios in experimental and observational studies:\n\nAdjusting for Covariates: It accounts for variability due to covariates, improving the precision of estimating treatment effects.\nControlling for Confounding Variables: By adjusting for variables related to the response but not influenced by treatments, ANCOVA enhances causal interpretations.\nIncreasing Statistical Power: Reducing error variance by accounting for covariates can lead to more powerful tests of treatment effects.\n\nExample: Consider comparing the effects of three different fertilizers (A, B, C) on crop yield. Soil quality before treatment is a covariate that affects yield but is not influenced by the fertilizer used. ANCOVA can adjust yield differences based on initial soil quality, providing a clearer assessment of fertilizer effectiveness.\nReference: Dean, Voss & Draguljić (2017) discuss the integration of covariates in experimental designs, highlighting the benefits of ANCOVA in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#the-ancova-model-centered-form-of-the-model",
    "href": "lectures/week-10_ancova-part2.html#the-ancova-model-centered-form-of-the-model",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "The ANCOVA Model: Centered Form of the Model",
    "text": "The ANCOVA Model: Centered Form of the Model\nFor \\(v\\) treatments and \\(n\\) total observations, the ANCOVA model is defined as:\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}_{\\cdot\\cdot}) + \\epsilon_{ij},\n\\qquad(1)\\]\nwhere:\n\n\\(Y_{ij}\\): Response variable for the \\(j\\)-th unit under treatment \\(i\\).\n\\(X_{ij}\\): Covariate value for the \\(j\\)-th unit under treatment \\(i\\).\n\\(\\bar{X}_{\\cdot\\cdot}\\): Overall mean of the covariate.\n\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\): Error term.\n\\(\\mu\\) (overall mean): This is the grand mean of the response variable when the covariate \\(X_{ij}\\) is at its overall mean \\(\\bar{X}_{\\cdot\\cdot}\\).\n\\(\\tau_i\\) (treatment effect): This represents the deviation of the \\(i\\)-th treatment from the overall mean adjusted for the covariate. That is, it estimates the effect of treatment \\(i\\) at the covariate value \\(\\bar{X}_{\\cdot\\cdot}\\). These are not raw group means; they are adjusted treatment means.\n\\(\\beta\\) (regression coefficient): This is the common slope that quantifies the effect of the covariate \\(X_{ij}\\) on the response \\(Y_{ij}\\), assumed constant across treatments.\n\nIntuition: Centering the covariate reduces multicollinearity between the intercept and the covariate, enhancing the interpretability of the model parameters."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#adjusted-treatment-means",
    "href": "lectures/week-10_ancova-part2.html#adjusted-treatment-means",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe adjusted mean for treatment \\(i\\) is given by:\n\\[\n\\mu_{i}^\\text{adj} = \\mu + \\tau_i = \\bar{Y}_{i \\cdot} - \\beta (\\bar{X}_{i \\cdot} - \\bar{X}_{\\cdot\\cdot}),\n\\qquad(2)\\]\n\\[\n\\tau_i = \\bar{Y}_{i \\cdot} - \\mu - \\beta (\\bar{X}_{i \\cdot} - \\bar{X}_{\\cdot\\cdot}),\n\\]\nDerivation:\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}_{\\cdot\\cdot}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_{i\\cdot} = \\mu + \\tau_i + \\beta (\\bar{X}_{i \\cdot} - \\bar{X}_{\\cdot\\cdot}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_{i\\cdot} = \\mu + \\tau_i + \\beta (\\bar{X}_{i \\cdot}  - \\bar{X}_{\\cdot\\cdot})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_{i \\cdot} - \\beta (\\bar{X}_{i \\cdot}  - \\bar{X}_{\\cdot\\cdot})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#why-adjustment-is-necessary",
    "href": "lectures/week-10_ancova-part2.html#why-adjustment-is-necessary",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Why Adjustment is Necessary",
    "text": "Why Adjustment is Necessary\nIf the covariate differs across treatment groups (i.e., \\(\\bar{X}_{i\\cdot} \\ne \\bar{X}_{\\cdot\\cdot}\\)), then the unadjusted means \\(\\bar{Y}_{i\\cdot}\\) reflect not only the treatment effects but also differences in the covariate distributions. To isolate the treatment effect, we use:\n\\[\n\\hat{\\mu} + \\hat{\\tau}_i = \\bar{Y}_{i\\cdot} - \\hat{\\beta}(\\bar{X}_{i\\cdot} - \\bar{X}_{\\cdot\\cdot}),\n\\]\nwhich adjusts each treatment mean to the same covariate level, thus removing confounding due to the covariate."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#parameter-estimation-via-least-squares",
    "href": "lectures/week-10_ancova-part2.html#parameter-estimation-via-least-squares",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Parameter Estimation via Least Squares",
    "text": "Parameter Estimation via Least Squares\nThe estimation of model parameters (\\(\\tau_i\\) and \\(\\beta\\)) typically employs the Least Squares estimation approach:\n\nThe estimation of the covariate slope coefficient is expressed as:\n\n\\[\n\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}\n\\qquad(3)\\]\nwhere:\n\\[\nSP_{xy} = \\sum_{i=1}^{a}\\sum_{j=1}^{n_i}(x_{ij} - \\bar{x}_{i.})(y_{ij} - \\bar{y}_{i.})\n\\qquad(4)\\]\nand\n\\[\nSS_{xx} = \\sum_{i=1}^{a}\\sum_{j=1}^{n_i}(x_{ij} - \\bar{x}_{i.})^2\n\\qquad(5)\\]"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#partitioning-of-variation",
    "href": "lectures/week-10_ancova-part2.html#partitioning-of-variation",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Partitioning of Variation",
    "text": "Partitioning of Variation\nANCOVA decomposes the total observed variability into components associated with treatments, covariates, and residual errors:\n\nTotal Sum of Squares (SS Total):\n\n\\[\nSS_\\text{Total} = \\sum_{i=1}^{a}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{..})^2\n\\]\n\nTreatment Sum of Squares adjusted for covariates (SS Treatment):\n\n\\[\nSS_{\\text{Treatment}|x} = \\sum_{i=1}^{a}n_i(\\hat{\\mu}_i^{adj} - \\bar{y}_{..})^2\n\\]\n\nCovariate Sum of Squares (SS Covariate):\n\n\\[\nSS_\\text{Covariate} = \\hat{\\beta}^2 SS_{xx}\n\\]\n\nResidual Sum of Squares (SS Error):\n\n\\[\nSS_\\text{Error} = SS_\\text{Total} - SS_{\\text{Treatment}|x} - SS_\\text{Covariate}\n\\]"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#ancova-table",
    "href": "lectures/week-10_ancova-part2.html#ancova-table",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "ANCOVA Table",
    "text": "ANCOVA Table\n\n\n\n\n\n\n\n\n\n\nSource of Variation\ndf\nSum of Squares\nMean Square\nF-Statistic\n\n\n\n\nTreatment (\\(\\tau\\))\n\\(a - 1\\)\n\\(SS_{\\text{Treatment}\\mid x}\\)\n\\(\\frac{SS_{\\text{Treatment}\\mid x}}{a-1}\\)\n\\(\\frac{MS_{\\text{Treatment}\\mid x}}{MS_\\text{Error}}\\)\n\n\nCovariate (\\(x\\))\n\\(1\\)\n\\(SS_\\text{Covariate}\\)\n\n\n\n\nError\n\\(N - a - 1\\)\n\\(SS_\\text{Error}\\)\n\\(\\frac{SS_E}{N-a-1}\\)\n\n\n\nTotal\n\\(N - 1\\)\n\\(SS_\\text{Total}\\)\n\n\n\n\n\nThe ANCOVA F-test evaluates the statistical significance of treatment effects after adjusting for the covariate."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#hypothesis-testing",
    "href": "lectures/week-10_ancova-part2.html#hypothesis-testing",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nObjective: Test the null hypothesis using the F-statistic.\n\\[\nH_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_a = 0\n\\]\n\\[\nH_a: \\text{At least one } \\tau_i \\text{ is different from the others}\n\\]\nTest Statistic: \\[\nF = \\frac{MS_{\\text{Treatment}\\mid x}}{MS_\\text{Error}} \\sim F_{a-1, N-a-1}\n\\]\nDecision Rule: Reject \\(H_0\\) if \\(F &gt; F_{\\alpha; a-1, N-a-1}\\), where \\(\\alpha\\) is the significance level. Interpretation: A significant F-statistic indicates that at least one treatment mean is different after adjusting for the covariate."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#example-balloon-experiment",
    "href": "lectures/week-10_ancova-part2.html#example-balloon-experiment",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Example: Balloon Experiment",
    "text": "Example: Balloon Experiment\n\nBalloon Experiment Example (32 observations, 4 treatments):\n\n\nballoon_fit &lt;- lm(\n    Time ~ fColor + Order_c,\n    data = balloon_df,\n    contrasts = list(fColor = \"contr.sum\")\n)\ncar::Anova(balloon_fit, type = \"III\") # Type III SS\n\n\n\n\n\n\n\n\nSum Sq\nDf\nF value\nPr(&gt;F)\n\n\n\n\n(Intercept)\n13113.9013\n1\n1948.228767\n0.0000000\n\n\nfColor\n127.6788\n3\n6.322745\n0.0021746\n\n\nOrder_c\n120.8353\n1\n17.951550\n0.0002358\n\n\nResiduals\n181.7422\n27\nNA\nNA\n\n\n\n\n\n\nFigure 1: Balloon Experiment: Inflation Times by Color\n\n\n\n\n\n\\(F = 6.32 &gt; F_{0.01; 3,27} \\approx 4.57\\), reject \\(H_0\\) at \\(\\alpha = 0.01\\).\nConclusion: Balloon colors significantly affect inflation time after adjusting for time order.\n\nRationale: The table partitions variability into treatment and error components, with the F-ratio indicating significance. Adjusted means (from Figure 9.2) visualize these effects."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#explanation-adjusted-vs.-unadjusted-means-in-ancova",
    "href": "lectures/week-10_ancova-part2.html#explanation-adjusted-vs.-unadjusted-means-in-ancova",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Explanation: Adjusted vs. Unadjusted Means in ANCOVA",
    "text": "Explanation: Adjusted vs. Unadjusted Means in ANCOVA\nIn ANCOVA, unadjusted means are simple group averages of the response variable \\(Y\\) , ignoring the covariate \\(X\\) . These can be biased if \\(X\\) is associated with both the treatment and the outcome. Adjusted means account for the covariate by estimating the group means at a common value of \\(X\\) (e.g., the grand mean \\(\\bar{X}_{\\cdot\\cdot}\\) ), removing confounding effects."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#bias-in-unadjusted-means-fig.-9.2",
    "href": "lectures/week-10_ancova-part2.html#bias-in-unadjusted-means-fig.-9.2",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Bias in Unadjusted Means (Fig. 9.2)",
    "text": "Bias in Unadjusted Means (Fig. 9.2)\n\nScenario: Suppose two groups ( \\(i=1, 2\\) ) have different covariate distributions (e.g., Group 1 has lower \\(X\\) , Group 2 has higher \\(X\\) ).\nIssue: If \\(Y\\) increases with \\(X\\) , comparing unadjusted means \\(\\bar{Y}_1\\) and \\(\\bar{Y}_2\\) (horizontal lines) ignores the covariate effect, leading to biased treatment comparisons.\nSolution: Adjust means by modeling the relationship between \\(Y\\) and \\(X\\) within each group."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#r-code-to-replicate-fig.-9.2-unadjusted-means",
    "href": "lectures/week-10_ancova-part2.html#r-code-to-replicate-fig.-9.2-unadjusted-means",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "R Code to Replicate Fig. 9.2 (Unadjusted Means)",
    "text": "R Code to Replicate Fig. 9.2 (Unadjusted Means)\n\n# (unadj_means &lt;- aggregate(Time ~ fColor, data = balloon_df, mean))\n\nmeans_df &lt;- balloon_df |&gt;\n    group_by(fColor) |&gt;\n    summarise(\n        Time = mean(Time),\n        Order_c = mean(Order_c)\n    )\nmeans_df\n\n\n\n\n\n\n\n\nfColor\nTime\nOrder_c\n\n\n\n\npink\n18.3375\n-0.250\n\n\nyellow\n22.5750\n-0.875\n\n\norange\n21.8750\n1.000\n\n\nblue\n18.1875\n0.125\n\n\n\n\n\n\n\nTable 2: Unadjusted Means (Biased Comparison)\n\n\n\n\n\nggplot(balloon_df, aes(x = Order_c, y = Time, color = fColor)) +\n    geom_point(alpha = 0.6, aes(shape = fColor)) +\n    geom_smooth(\n        method = \"lm\",\n        se = FALSE,\n        linetype = \"dashed\",\n        formula = \"y~x\"\n    ) + # True relationship\n    geom_hline(\n        data = means_df,\n        aes(yintercept = Time, color = fColor),\n        linetype = \"solid\",\n        linewidth = 1\n    ) +\n    geom_vline(\n        xintercept = mean(balloon_df$Order_c),\n        linetype = \"dotted\"\n    ) + # Reference line for adjustment\n    scale_y_continuous(\n        breaks = c(\n            seq(min(balloon_df$Time), max(balloon_df$Time), by = 2),\n            means_df$Time\n        ),\n        labels = function(x) format(x, nsmall = 1, digits = 2)\n    ) +\n    labs(\n        title = \"Unadjusted Means (Biased Comparison)\",\n        x = \"Time (centered)\",\n        y = \"Time\"\n    )\n\n\n\n\n\n\n\nFigure 2: Unadjusted Means (Biased Comparison)"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#adjusted-means-1",
    "href": "lectures/week-10_ancova-part2.html#adjusted-means-1",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Adjusted Means",
    "text": "Adjusted Means\n\nmeans_df &lt;- means_df |&gt;\n    mutate(Time_adj = Time - coef(balloon_fit)[\"Order_c\"] * Order_c) # Adjusted means\nmeans_df\n\n\n\n\n\n\n\n\nfColor\nTime\nOrder_c\nTime_adj\n\n\n\n\npink\n18.3375\n-0.250\n18.28474\n\n\nyellow\n22.5750\n-0.875\n22.39035\n\n\norange\n21.8750\n1.000\n22.08603\n\n\nblue\n18.1875\n0.125\n18.21388\n\n\n\n\n\n\n\nTable 3: Adjusted Means (Unbiased Comparison)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe column Time_adj in the table above represents the adjusted means for each treatment group, accounting for the covariate (Order_c). These adjusted means provide a clearer picture of the treatment effects by removing the influence of the covariate. The column Time represents the unadjusted means, which may be biased due to the covariate’s influence.\n\n\n\n\n\n# Estimated marginal means\nemm &lt;- emmeans(balloon_fit, ~fColor)\nemm\n\n fColor emmean    SE df lower.CL upper.CL\n pink     18.3 0.917 27     16.4     20.2\n yellow   22.4 0.918 27     20.5     24.3\n orange   22.1 0.919 27     20.2     24.0\n blue     18.2 0.917 27     16.3     20.1\n\nConfidence level used: 0.95 \n\n\n\nTable 4: Adjusted Means using emmeans package\n\n\n\nThe code above uses the emmeans package to compute estimated marginal means (EMMs) for the treatment groups. EMMs are adjusted means that account for the covariate, providing a more accurate representation of treatment effects."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#adjusted-means-plot",
    "href": "lectures/week-10_ancova-part2.html#adjusted-means-plot",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Adjusted Means Plot",
    "text": "Adjusted Means Plot\n\nggplot(balloon_df, aes(x = Order_c, y = Time, color = fColor)) +\n    geom_point(alpha = 0.6) +\n    geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\") + # Regression lines\n    geom_hline(\n        data = means_df,\n        aes(yintercept = Time_adj, color = fColor),\n        linetype = \"solid\",\n        linewidth = 1\n    ) +\n    geom_vline(xintercept = 0, linetype = \"dotted\") + # Reference line for adjustment\n    labs(\n        title = \"Adjusted Means\",\n        x = \"Time (centered)\",\n        y = \"Time\"\n    ) +\n    scale_y_continuous(\n        breaks = c(\n            seq(min(balloon_df$Time), max(balloon_df$Time), by = 2),\n            means_df$Time_adj\n        ),\n        labels = function(x) format(x, nsmall = 1, digits = 2)\n    )\n\n\n\n\n\n\n\nFigure 3: Adjusted Means (Unbiased Comparison)"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#comparison-of-plots",
    "href": "lectures/week-10_ancova-part2.html#comparison-of-plots",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Comparison of Plots",
    "text": "Comparison of Plots\n\n\n\n\n\n\n\n\nAspect\nUnadjusted Plot\nAdjusted Plot\n\n\n\n\nMeans\nHorizontal lines at \\(\\bar{Y}_i\\)\nHorizontal lines at \\(Y_i\\) adjusted to \\(\\bar{X}_{\\cdot\\cdot}\\)\n\n\nBias\nBiased if \\(X\\) differs between groups\nUnbiased by accounting for \\(X\\)\n\n\nInterpretation\nMisleading treatment effect due to \\(X\\)\nTrue treatment effect isolated from \\(X\\)\n\n\nReference Line\nNone\nDotted line at \\(\\bar{X}_{\\cdot\\cdot}\\)\n\n\n\n\nKey Insight\n\nIn the unadjusted plot, Group 2’s higher mean \\(Y\\) might be due to its higher \\(X\\) values, not the treatment.\nIn the adjusted plot, both groups are compared at the same \\(X\\) ( \\(\\bar{X}_{\\cdot\\cdot}\\) ), isolating the true treatment effect. The vertical dotted line emphasizes the adjustment point."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#checking-model-assumptions",
    "href": "lectures/week-10_ancova-part2.html#checking-model-assumptions",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Checking Model Assumptions",
    "text": "Checking Model Assumptions\nObjective: Verify ANCOVA assumptions using diagnostics.\nContent:\n\nTools:\n\nResidual Plot: Plot residuals (\\(y_{ij} - \\hat{y}_{ij}\\)) vs. covariate or time order (e.g., Figure 9.3 for balloon experiment, residuals -2 to 2, time order 1 to 29).\nTests: Interaction test for homogeneity of slopes.\n\nChecks:\n\nNo patterns (linearity).\nConstant spread (homoscedasticity).\nRandom scatter (independence).\n\n\nRationale: Violations like non-linear trends or unequal slopes invalidate ANCOVA results. Residual plots ensure the model fits the data appropriately."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#linearity",
    "href": "lectures/week-10_ancova-part2.html#linearity",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "1. Linearity",
    "text": "1. Linearity\nDefinition:\nThe relationship between the response variable \\(Y\\) and the covariate \\(X\\) must be linear within each treatment group.\nMathematically:\nFor treatment \\(i\\), we assume: \\[\nE[Y_{ij} \\mid X_{ij}] = \\mu + \\tau_i + \\beta X_{ij}\n\\]\nLinearity Check (Graphical):\n\nggplot(balloon_df, aes(x = Order, y = Time, color = fColor)) +\n    geom_point(aes(shape = fColor)) +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x) +\n    labs(title = \"Linearity Check: Response vs. Covariate by Treatment\") +\n    scale_color_manual(values = c(\"pink\", \"yellow\", \"orange\", \"blue\"))\n\n\n\n\n\n\n\nFigure 4: Linearity Check: Response vs. Covariate by Treatment\n\n\n\n\n\nInterpretation:\nIf the regression lines appear approximately straight within each group, the assumption is likely satisfied."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#homogeneity-of-regression-slopes",
    "href": "lectures/week-10_ancova-part2.html#homogeneity-of-regression-slopes",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "2. Homogeneity of Regression Slopes",
    "text": "2. Homogeneity of Regression Slopes\nDefinition:\nThe effect of the covariate (\\(\\beta\\)) is assumed to be the same across all treatment groups.\nMathematically:\n\\[\n\\text{Test } H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_v\n\\]\nThis is equivalent to testing for no interaction between treatment and covariate."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#homogeneity-check-statistical",
    "href": "lectures/week-10_ancova-part2.html#homogeneity-check-statistical",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Homogeneity Check (Statistical)",
    "text": "Homogeneity Check (Statistical)\n\n# Full model allowing interaction\nmodel_full &lt;- lm(Time ~ fColor * Order_c, data = balloon_df)\n\n# Reduced model assuming common slope\nmodel_reduced &lt;- lm(Time ~ fColor + Order_c, data = balloon_df)\n\n# Compare models\nanova(model_reduced, model_full)\n\n\n\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n27\n181.7422\nNA\nNA\nNA\nNA\n\n\n24\n157.0320\n3\n24.71016\n1.258859\n0.3107502\n\n\n\n\n\n\n\nTable 5: Homogeneity of Slopes Check: Interaction Term\n\n\n\n\n\nIf the interaction term is not significant, the slopes are homogeneous, and the assumption holds.\nA “significant” interaction term suggests that the slopes differ across treatments, indicating a violation of this assumption."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#alternative-check-using-car-package",
    "href": "lectures/week-10_ancova-part2.html#alternative-check-using-car-package",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Alternative Check (Using car package)",
    "text": "Alternative Check (Using car package)\nOr, using the car package to check the interaction term using Type III sumof squares:\n\n# ANOVA table for interaction\ncar::Anova(model_full, type = \"III\")\n\n\n\n\n\n\n\n\n\nSum Sq\nDf\nF value\nPr(&gt;F)\n\n\n\n\n(Intercept)\n2678.34230\n1\n409.344637\n0.0000000\n\n\nfColor\n119.54833\n3\n6.090393\n0.0031164\n\n\nOrder_c\n17.19483\n1\n2.627973\n0.1180616\n\n\nfColor:Order_c\n24.71016\n3\n1.258859\n0.3107502\n\n\nResiduals\n157.03202\n24\nNA\nNA\n\n\n\n\n\n\n\nTable 6: Homogeneity of Slopes Check: Interaction Term (car package)\n\n\n\n\n\nIf the interaction term (fColor:Order_c) is not “significant”, the slopes are homogeneous, and the assumption holds.\nA “significant” interaction implies violation of this assumption and suggests separate slopes per group."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#independence-of-covariate-and-treatment",
    "href": "lectures/week-10_ancova-part2.html#independence-of-covariate-and-treatment",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "3. Independence of Covariate and Treatment",
    "text": "3. Independence of Covariate and Treatment\nDefinition:\nThe covariate should be independent of treatment assignment, i.e., covariate values should not systematically differ across treatment groups.\nRandom assignment of treatments is crucial to ensure this independence."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#normality-of-residuals",
    "href": "lectures/week-10_ancova-part2.html#normality-of-residuals",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "4. Normality of Residuals",
    "text": "4. Normality of Residuals\nDefinition:\nResiduals from the model should follow a normal distribution.\nDiagnostic Check (Graphical & Statistical):\n\n# Residual plots\npar(mfrow = c(1, 2))\nqqnorm(resid(model_reduced))\nqqline(resid(model_reduced))\nhist(resid(model_reduced), breaks = 10, main = \"Histogram of Residuals\")\n\n\n\n\n\n\n\nFigure 5: Normality Check: Q-Q Plot and Histogram of Residuals\n\n\n\n\n\n\n\n\n# Shapiro-Wilk test\nshapiro.test(resid(model_reduced))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model_reduced)\nW = 0.96374, p-value = 0.3465\n\n\n\nTable 7: Shapiro-Wilk Test for Normality of Residuals\n\n\n\n\nQ-Q plot should lie along the reference line.\nHistogram should resemble a bell curve.\nShapiro-Wilk p-value &gt; 0.05 indicates normality."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#homoscedasticity-equal-variances",
    "href": "lectures/week-10_ancova-part2.html#homoscedasticity-equal-variances",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "5. Homoscedasticity (Equal Variances)",
    "text": "5. Homoscedasticity (Equal Variances)\nDefinition:\nThe variance of residuals should be constant across levels of the covariate and treatments.\nDiagnostic Check (Graphical & Statistical):\n\nplot(\n    model_reduced$fitted.values,\n    resid(model_reduced),\n    xlab = \"Fitted values\",\n    ylab = \"Residuals\",\n    main = \"Homoscedasticity Check\"\n)\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\nFigure 6: Homoscedasticity Check: Residuals vs. Fitted Values\n\n\n\n\n\n\n\n\n# Breusch-Pagan Test\nlmtest::bptest(model_reduced)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_reduced\nBP = 3.9212, df = 4, p-value = 0.4168\n\n\n\nTable 8: Breusch-Pagan Test for Homoscedasticity\n\n\n\n\nRandom scatter around zero with no funneling supports homoscedasticity.\nBreusch-Pagan p-value &gt; 0.05 suggests equal variances."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#summary-table",
    "href": "lectures/week-10_ancova-part2.html#summary-table",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\nAssumption\nCheck Method\nR Code / Tool\n\n\n\n\nLinearity\nScatterplot with linear fit\nggplot2::geom_smooth()\n\n\nHomogeneity of slopes\nModel comparison (F-test)\nanova() on interaction\n\n\nIndependence of covariate/treatment\nBoxplot, ANOVA on covariate\nlm() and anova()\n\n\nNormality of residuals\nQ-Q plot, histogram, Shapiro test\nqqnorm(), shapiro.test()\n\n\nHomoscedasticity\nResiduals plot, BP test\nlmtest::bptest()\n\n\n\nEach assumption plays a critical role in ensuring valid inference from ANCOVA. Always assess assumptions before interpreting model results.\nAnalogy: Think of ANCOVA as leveling the playing field. Just as a referee ensures that all players are subject to the same rules regardless of their backgrounds, ANCOVA adjusts the response variable based on covariates, ensuring that treatment comparisons are fair and unbiased."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#treatment-contrasts-and-confidence-intervals",
    "href": "lectures/week-10_ancova-part2.html#treatment-contrasts-and-confidence-intervals",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "7. Treatment Contrasts and Confidence Intervals",
    "text": "7. Treatment Contrasts and Confidence Intervals\nObjective: Make inferences about specific treatment differences.\n\nContrast: Linear combination \\(\\sum c_i \\tau_i\\), where \\(\\sum c_i = 0\\) (e.g., \\(\\tau_1 - \\tau_2\\)).\nEstimator: \\(\\sum c_i \\hat{\\tau}_i = \\sum c_i (\\bar{y}_{i.} - \\hat{\\beta}_1 \\bar{x}_{i.})\\).\nVariance:\n\n\\[\n\\text{Var}(\\sum c_i \\hat{\\tau}_i) = \\sigma^2 \\left( \\sum \\frac{c_i^2}{n_i} + \\frac{(\\sum c_i \\bar{x}_{i.})^2}{SS_{xx}} \\right)\n\\]\n\\[\n\\widehat{\\text{Var}(\\sum c_i \\hat{\\tau}_i)} = MS_\\text{Error} \\left( \\sum \\frac{c_i^2}{n_i} + \\frac{(\\sum c_i \\bar{x}_{i.})^2}{SS_{xx}} \\right)\n\\]\n\nConfidence Interval:\n\n\\[\n\\sum c_i \\tau_i \\in \\sum c_i \\hat{\\tau}_i \\pm t_{\\alpha/2; n-a-1} \\sqrt{\\widehat{\\text{Var}(\\sum c_i \\hat{\\tau}_i)}}\n\\]\n\nRationale: Confidence intervals pinpoint which treatment differences are significant, adjusted for the covariate, aiding targeted conclusions."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#pairwise-comparisons-of-adjusted-means",
    "href": "lectures/week-10_ancova-part2.html#pairwise-comparisons-of-adjusted-means",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Pairwise Comparisons of Adjusted Means",
    "text": "Pairwise Comparisons of Adjusted Means\nObjective: Adjust for multiple tests to control error rates.\n\nContrast Formula\nFor pairwise comparison \\(\\tau_i - \\tau_j\\) (e.g., \\(\\tau_1 - \\tau_3\\)):\n\\[\n\\text{Estimate: } \\hat{\\tau}_i - \\hat{\\tau}_j = (\\bar{y}_i - \\hat{\\beta}_1 \\bar{x}_i) - (\\bar{y}_j - \\hat{\\beta}_1 \\bar{x}_j)\n\\]\n\n\nVariance Calculation\nFrom ANCOVA theory:\n\\[\n\\widehat{\\text{Var}}(\\hat\\tau_i - \\hat\\tau_j) = MS_{\\text{Error}} \\left[ \\frac{1}{n_i} + \\frac{1}{n_j} + \\frac{(\\bar{x}_i - \\bar{x}_j)^2}{SS_{xx}} \\right]\n\\]\n\n\\(SS_{xx} = \\sum_{i=1}^a \\sum_{k=1}^{n_i} (x_{ik} - \\bar{x}_{\\cdot\\cdot})^2\\) (total covariate SS)\nBalloon data parameters: \\(n_i = 8\\), \\(\\hat{\\beta}_1 = -0.211\\), \\(MS_{\\text{Error}} = 6.731\\), \\(SS_{xx}\\) (unreported but embedded in SEs)\n\n\n\nConfidence Interval Calculation\n\\[\n\\text{CI: } \\hat{\\tau}_i - \\hat{\\tau}_j \\pm  (\\text{Critical Value}) \\sqrt{\\widehat{\\text{Var}}(\\hat\\tau_i - \\hat\\tau_j)}\n\\]\n\nCI Critical Values:\n\nBonferroni: Critical value \\(t_{\\alpha/(2m); sn-a-1}\\), where \\(m\\) is the number of comparisons.\nScheffé: Critical value \\(\\sqrt{(a-1) F_{\\alpha; a-1, n-a-1}}\\).\n\nWhen multiple treatments are involved, pairwise comparisons of adjusted means can be performed to identify specific differences.\nUse the emmeans package to perform pairwise comparisons of adjusted means.\n\n\n\n\n\n\n\nWarning\n\n\n\nBecause the adjusted means are not independent unless the covariate is balanced across treatments (i.e., treatment means are equal), the Scheffé method is preferred for multiple comparisons. The Bonferroni method is conservative and may not be appropriate for all situations. The Scheffé method is more powerful and flexible, allowing for a wider range of contrasts to be tested."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#pairwise-comparisons-of-adjusted-means-using-scheffés-method",
    "href": "lectures/week-10_ancova-part2.html#pairwise-comparisons-of-adjusted-means-using-scheffés-method",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Pairwise Comparisons of Adjusted Means using Scheffé’s Method",
    "text": "Pairwise Comparisons of Adjusted Means using Scheffé’s Method\n\n# Scheffé's Method\nscheffe &lt;- contrast(emm, method = \"pairwise\", adjust = \"scheffe\") |&gt;\n    summary(infer = TRUE)\nscheffe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\npink - yellow\n-4.1056039\n1.297600\n27\n-7.9725957\n-0.2386121\n-3.1639969\n0.0340731\n\n\npink - orange\n-3.8012923\n1.298720\n27\n-7.6716211\n0.0690365\n-2.9269524\n0.0557052\n\n\npink - blue\n0.0708623\n1.297362\n27\n-3.7954172\n3.9371419\n0.0546203\n0.9999555\n\n\nyellow - orange\n0.3043116\n1.300584\n27\n-3.5715725\n4.1801957\n0.2339807\n0.9965626\n\n\nyellow - blue\n4.1764662\n1.298183\n27\n0.3077388\n8.0451936\n3.2171632\n0.0304163\n\n\norange - blue\n3.8721546\n1.297959\n27\n0.0040946\n7.7402145\n2.9832644\n0.0496784\n\n\n\n\n\n\n\nTable 9: Scheffé’s Method for Multiple Comparisons"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#plotting-pairwise-comparisons",
    "href": "lectures/week-10_ancova-part2.html#plotting-pairwise-comparisons",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Plotting Pairwise Comparisons",
    "text": "Plotting Pairwise Comparisons\n\nplot(emm, comparisons = TRUE) +\n    labs(title = \"Pairwise Comparisons of Adjusted Means\")\n\n\n\n\n\n\n\nFigure 7: Pairwise Comparisons of Adjusted Means\n\n\n\n\n\nInterpretation: Identify which balloon colors have significantly different inflation times after adjusting for run order."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#residual-analysis-using-r-base-plot",
    "href": "lectures/week-10_ancova-part2.html#residual-analysis-using-r-base-plot",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Residual Analysis using R base plot",
    "text": "Residual Analysis using R base plot\n\npar(mfrow = c(2, 2))\nplot(model_reduced)\n\n\n\n\n\n\n\nFigure 8: Post-Model Diagnostics: Residuals vs. Fitted Values"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#residual-analysis-using-car-package",
    "href": "lectures/week-10_ancova-part2.html#residual-analysis-using-car-package",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Residual Analysis using car package",
    "text": "Residual Analysis using car package\n\ncar::residualPlots(model_reduced)\n\n           Test stat Pr(&gt;|Test stat|)\nfColor                               \nOrder_c       0.5213           0.6066\nTukey test    0.8772           0.3804\n\n\n\n\n\n\n\n\nFigure 9: Post-Model Diagnostics using car package"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#diagnostics-using-performance-package",
    "href": "lectures/week-10_ancova-part2.html#diagnostics-using-performance-package",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Diagnostics using performance package",
    "text": "Diagnostics using performance package\n\nperformance::check_model(model_reduced)\n\n\n\n\n\n\n\nFigure 10: Post-Model Diagnostics using performance package"
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#handling-unbalanced-data-in-r",
    "href": "lectures/week-10_ancova-part2.html#handling-unbalanced-data-in-r",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Handling Unbalanced Data in R",
    "text": "Handling Unbalanced Data in R\n# Fit ANCOVA model with unbalanced data\n# Assume 'data_unbalanced' is loaded with unequal replicates\nfit_unbalanced &lt;- lm(Response ~ FactorA * FactorB, data = data_unbalanced)\n\n# Type II Sum of Squares\nAnova(fit_unbalanced, type = \"II\")\n\n# Type III Sum of Squares\nAnova(fit_unbalanced, type = \"III\")\nConsiderations:\n\nType II SS: Suitable when interactions are not of primary interest.\nType III SS: Appropriate when interactions are present or main effects need to be tested conditionally."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#model-with-multiple-covariates",
    "href": "lectures/week-10_ancova-part2.html#model-with-multiple-covariates",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Model with Multiple Covariates",
    "text": "Model with Multiple Covariates\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_1 X_{1ij} + \\beta_2 X_{2ij} + \\ldots + \\epsilon_{ij}\n\\]\nR Implementation:\n# Fit ANCOVA model with multiple covariates\nmodel_multi_cov &lt;- lm(Response ~ FactorA + FactorB + Covariate1 + Covariate2, data = data)\nsummary(model_multi_cov)\nInterpretation: Assess the effect of each covariate and treatment while controlling for other covariates."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#nonlinear-relationships",
    "href": "lectures/week-10_ancova-part2.html#nonlinear-relationships",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Nonlinear Relationships",
    "text": "Nonlinear Relationships\nIf the relationship between the response and covariate is not linear, polynomial terms can be included in the model.\n\nModel with Polynomial Terms\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_1 X_{ij} + \\beta_2 X_{ij}^2 + \\epsilon_{ij}\n\\]\nR Implementation:\n# Fit ANCOVA model with quadratic term\nmodel_poly &lt;- lm(Response ~ FactorA + FactorB + x + I(x^2), data = data)\nsummary(model_poly)\nInterpretation: Determine if adding a quadratic term improves model fit and captures the nonlinear relationship."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#adjusted-treatment-means-1",
    "href": "lectures/week-10_ancova-part2.html#adjusted-treatment-means-1",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe adjusted mean for treatment \\(i\\) is given by:\n\\[\n\\mu^\\text{adj} = \\mu + \\tau_i = \\bar{Y}_{i \\cdot} - \\beta (\\bar{X}_{i \\cdot} - \\bar{X}_{\\cdot\\cdot}),\n\\]\nwhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment \\(i\\).\n\\(\\bar{Y}_{i\\cdot}\\): Mean response for treatment \\(i\\).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_{i \\cdot}\\): Mean covariate value for treatment \\(i\\).\n\\(\\bar{X}_{\\cdot\\cdot}\\): Overall mean of the covariate.\n\nDerivation:\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}_{\\cdot\\cdot}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_{i\\cdot} = \\mu + \\tau_i + \\beta (\\bar{X}_{i \\cdot} - \\bar{X}_{\\cdot\\cdot}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_{i\\cdot} = \\mu + \\tau_i + \\beta (\\bar{X}_{i \\cdot}  - \\bar{X}_{\\cdot\\cdot})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_{i \\cdot} - \\beta (\\bar{X}_{i \\cdot}  - \\bar{X}_{\\cdot\\cdot})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference: Montgomery (2019) and Scheffé (1959) provide detailed derivations of adjusted means in ANCOVA."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#f-test-for-homogeneity-of-slopes",
    "href": "lectures/week-10_ancova-part2.html#f-test-for-homogeneity-of-slopes",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "F-Test for Homogeneity of Slopes",
    "text": "F-Test for Homogeneity of Slopes\nTo test the assumption of homogeneity of regression slopes (i.e., that \\(\\beta\\) is consistent across treatments), an F-test can be conducted by comparing two models:\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta X_{ij} + \\epsilon_{ij}\n\\]\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_i X_{ij} + \\epsilon_{ij}\n\\]\nWhere \\(\\beta_i = \\beta + (\\alpha\\beta)_i\\) allows for different slopes across treatments.\nTest Procedure:\n\nNull Hypothesis (\\(H_0\\)): Homogeneity of slopes (\\(\\beta_i = \\beta\\) for all \\(i\\)).\nAlternative Hypothesis (\\(H_A\\)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta\\) for at least one \\(i\\)).\n\nF-Statistic Calculation:\n\\[\nF = \\frac{\\text{SSR}_{\\text{Interaction}} / \\text{DF}_{\\text{Interaction}}}{\\text{SSE} / \\text{DF}_{\\text{Error}}}\n\\]\nWhere:\n\n\\(\\text{SSR}_{\\text{Interaction}}\\): Sum of squares for the interaction term.\n\\(\\text{DF}_{\\text{Interaction}}\\): Degrees of freedom for the interaction.\n\\(\\text{SSE}\\): Sum of squares for error.\n\\(\\text{DF}_{\\text{Error}}\\): Degrees of freedom for error.\n\nDecision Rule:\n\nCompare the calculated F-statistic to the critical value from the F-distribution.\nReject \\(H_0\\) if \\(F\\) exceeds the critical value, indicating heterogeneous slopes.\n\nReference: Scheffé (1959) elaborates on hypothesis testing for interaction terms in ANCOVA."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-10_ancova-part2.html#confidence-intervals-for-higher-order-contrasts",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts can provide insights into the magnitude and significance of these interactions.\n\nFormula\n\\[\n\\text{CI} = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\n\n\nSteps in R\n\nEstimate the Contrast:\nUsing the emmeans package to define and estimate contrasts.\nCalculate the Confidence Interval:\nThe emmeans package automatically provides confidence intervals when performing contrasts.\n\nExample:\nlibrary(emmeans)\n\n# Fit the ANCOVA model\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define estimated marginal means\nemm &lt;- emmeans(model, ~ Treatment)\n\n# Define a contrast (e.g., Treatment B vs. Treatment A)\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\nsummary(contrast_AB, infer = TRUE)\nInterpretation: The output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#conclusion",
    "href": "lectures/week-10_ancova-part2.html#conclusion",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "Conclusion",
    "text": "Conclusion\nAnalysis of Covariance (ANCOVA) is a versatile statistical technique that enhances the power and precision of experiments by adjusting for covariates. By combining the strengths of ANOVA and regression, ANCOVA controls for extraneous variability, enabling more accurate assessment of treatment effects. Mastery of ANCOVA empowers graduate students to conduct robust experimental analyses, interpret complex interactions, and draw meaningful causal inferences.\nKey Takeaways:\n\nPurpose of ANCOVA: Adjusts for covariates to isolate treatment effects.\nAssumptions: Linearity, homogeneity of regression slopes, independence of covariate and treatment, normality, and homoscedasticity.\nModel Fitting: Involves testing treatment effects while accounting for covariates.\nExtensions: ANCOVA can handle unbalanced designs, multiple covariates, and nonlinear relationships.\nPractical Implementation: R provides comprehensive tools for performing ANCOVA and diagnostic assessments.\nAdvanced Topics: Understanding higher-order interactions and mathematical derivations enhances analytical rigor.\n\nFuture Directions: Further exploration into advanced experimental designs, mixed-effects models, and nonparametric ANCOVA methods will provide a deeper understanding of complex data structures and enhance analytical capabilities."
  },
  {
    "objectID": "lectures/week-10_ancova-part2.html#references",
    "href": "lectures/week-10_ancova-part2.html#references",
    "title": "ANCOVA: Statistical Analysis of Covariance",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html",
    "href": "lectures/week-09_power-analysis.html",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "One-way ANOVA tests whether the means of three or more groups differ significantly. For example, imagine comparing the average test scores of students taught using three teaching methods (Method A, Method B, Method C).\n\n\n\n\nPower (1 - β): The probability of correctly rejecting a false null hypothesis (typically targeted at 0.80 or higher).\nSignificance Level (α): The probability of a Type I error (rejecting a true null hypothesis), often set at 0.05.\nEffect Size: A measure of the magnitude of differences between group means, quantified in ANOVA using Cohen’s f.\nSample Size (r): The number of observations per group, which we often solve for in power analysis.\nNoncentrality Parameter (λ): A parameter that shifts the F-distribution under the alternative hypothesis, used to compute power.\n\n\n\n\nCohen’s f measures the standardized difference between group means relative to within-group variability. The formula is:\n\\[\nf = \\sqrt{\\frac{\\sum_{i=1}^k (\\mu_i - \\mu)^2 / k}{\\sigma^2}}\n\\]\nWhere:\n\n\\(k\\): Number of groups– In the textbook, \\(\\nu\\) is used instead of \\(k\\).\n\\(\\mu_i\\): Mean of group \\(i\\).\n\\(\\mu\\): Overall mean across all groups.\n\\(\\sigma^2\\): Within-group variance (assumed equal across groups).\n\nInterpretation (Cohen’s benchmarks):\n\nSmall: \\(f = 0.1\\)\nMedium: \\(f = 0.25\\)\nLarge: \\(f = 0.4\\)\n\n\n\n\nSuppose we have three groups (\\(k = 3\\)) with means \\(\\mu_1 = 10\\), \\(\\mu_2 = 12\\), \\(\\mu_3 = 14\\), and within-group variance \\(\\sigma^2 = 16\\). The overall mean is:\n\\[\n\\mu = \\frac{10 + 12 + 14}{3} = 12\n\\]\nNumerator of \\(f^2\\):\n\\[\n\\frac{\\sum (\\mu_i - \\mu)^2}{k} = \\frac{(10 - 12)^2 + (12 - 12)^2 + (14 - 12)^2}{3} = \\frac{4 + 0 + 4}{3} = \\frac{8}{3} \\approx 2.667\n\\]\n\\[\nf = \\sqrt{\\frac{2.667}{16}} = \\sqrt{0.1667} \\approx 0.408\n\\]\nThis is a large effect size, close to Cohen’s \\(f = 0.4\\).\n\n\n\nThe noncentrality parameter quantifies the shift in the F-distribution under the alternative hypothesis:\n\\[\n\\lambda = \\frac{r \\sum_{i=1}^k (\\mu_i - \\mu)^2}{\\sigma^2}\n\\]\nWhere \\(r\\) is the sample size per group (assuming balanced design). Using Cohen’s f:\n\\[\n\\lambda = r k f^2\n\\]\n\n\n\nUsing the above means and \\(\\sigma^2 = 16\\), with \\(r = 10\\) per group:\n\\[\n\\sum (\\mu_i - \\mu)^2 = 4 + 0 + 4 = 8\n\\]\n\\[\n\\lambda = \\frac{10 \\times 8}{16} = 5\n\\]\nOr via \\(f\\):\n\\[\n\\lambda = 10 \\times 3 \\times (0.408)^2 = 30 \\times 0.1665 \\approx 5\n\\]\n\n\n\nPower is calculated using the noncentral F-distribution:\n\nNumerator degrees of freedom: \\(df_1 = k - 1\\)\nDenominator degrees of freedom: \\(df_2 = N - k\\), where \\(N = r k\\)\nCritical F-value: \\(F_{\\text{crit}}\\) at \\(\\alpha\\), found from the F-distribution.\nPower: \\(1 - P(F &lt;  F_{\\text{crit}} | \\lambda)\\), where \\(F\\) follows a noncentral F-distribution.\n\n\n\n\nGoal: Determine the sample size per group (\\(r\\)) for \\(k = 3\\) groups, medium effect size (\\(f = 0.25\\)), \\(\\alpha = 0.05\\), and power = 0.80.\n\n\n\n\nlibrary(pwr)\n\nk &lt;- 3\nf &lt;- 0.25\nalpha &lt;- 0.05\npower &lt;- 0.80\n\nresult &lt;- pwr.anova.test(k = k, f = f, sig.level = alpha, power = power)\nprint(result)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nOutput:\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.39771\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\nWe need approximately \\(r = 53\\) subjects per group, so total \\(N = 3 \\times 53 = 159\\).\n\n\n\n\nDegrees of Freedom:\n\n\\(df_1 = k - 1 = 2\\)\n\\(df_2 = N - k = 3 \\times 53 - 3 = 156\\)\n\nNoncentrality Parameter:\n\n\\[\n\\lambda = r k f^2 = 53 \\times 3 \\times 0.25^2 = 159 \\times 0.0625 = 9.9375\n\\]\n\nCritical F-value:\n\n\\[\nF_{\\text{crit}} = qf(0.95, 2, 156) \\approx 3.05\n\\]\n\nPower:\n\n\\[\n\\text{Power} = 1 - pf(3.05, 2, 156, ncp = 9.9375)\n\\]\n\npower_manual &lt;- 1 - pf(qf(0.95, 2, 156), 2, 156, ncp = 9.9375)\nprint(power_manual)\n\n[1] 0.8048873\n\n\nOutput: Approximately 0.803, confirming the power is close to 0.80, validating the pwr result."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#what-is-one-way-anova",
    "href": "lectures/week-09_power-analysis.html#what-is-one-way-anova",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "One-way ANOVA tests whether the means of three or more groups differ significantly. For example, imagine comparing the average test scores of students taught using three teaching methods (Method A, Method B, Method C)."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#key-concepts-in-power-analysis",
    "href": "lectures/week-09_power-analysis.html#key-concepts-in-power-analysis",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "Power (1 - β): The probability of correctly rejecting a false null hypothesis (typically targeted at 0.80 or higher).\nSignificance Level (α): The probability of a Type I error (rejecting a true null hypothesis), often set at 0.05.\nEffect Size: A measure of the magnitude of differences between group means, quantified in ANOVA using Cohen’s f.\nSample Size (r): The number of observations per group, which we often solve for in power analysis.\nNoncentrality Parameter (λ): A parameter that shifts the F-distribution under the alternative hypothesis, used to compute power."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#effect-size-cohens-f",
    "href": "lectures/week-09_power-analysis.html#effect-size-cohens-f",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "Cohen’s f measures the standardized difference between group means relative to within-group variability. The formula is:\n\\[\nf = \\sqrt{\\frac{\\sum_{i=1}^k (\\mu_i - \\mu)^2 / k}{\\sigma^2}}\n\\]\nWhere:\n\n\\(k\\): Number of groups– In the textbook, \\(\\nu\\) is used instead of \\(k\\).\n\\(\\mu_i\\): Mean of group \\(i\\).\n\\(\\mu\\): Overall mean across all groups.\n\\(\\sigma^2\\): Within-group variance (assumed equal across groups).\n\nInterpretation (Cohen’s benchmarks):\n\nSmall: \\(f = 0.1\\)\nMedium: \\(f = 0.25\\)\nLarge: \\(f = 0.4\\)"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#example-calculation-of-cohens-f",
    "href": "lectures/week-09_power-analysis.html#example-calculation-of-cohens-f",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "Suppose we have three groups (\\(k = 3\\)) with means \\(\\mu_1 = 10\\), \\(\\mu_2 = 12\\), \\(\\mu_3 = 14\\), and within-group variance \\(\\sigma^2 = 16\\). The overall mean is:\n\\[\n\\mu = \\frac{10 + 12 + 14}{3} = 12\n\\]\nNumerator of \\(f^2\\):\n\\[\n\\frac{\\sum (\\mu_i - \\mu)^2}{k} = \\frac{(10 - 12)^2 + (12 - 12)^2 + (14 - 12)^2}{3} = \\frac{4 + 0 + 4}{3} = \\frac{8}{3} \\approx 2.667\n\\]\n\\[\nf = \\sqrt{\\frac{2.667}{16}} = \\sqrt{0.1667} \\approx 0.408\n\\]\nThis is a large effect size, close to Cohen’s \\(f = 0.4\\)."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#noncentrality-parameter-λ",
    "href": "lectures/week-09_power-analysis.html#noncentrality-parameter-λ",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "The noncentrality parameter quantifies the shift in the F-distribution under the alternative hypothesis:\n\\[\n\\lambda = \\frac{r \\sum_{i=1}^k (\\mu_i - \\mu)^2}{\\sigma^2}\n\\]\nWhere \\(r\\) is the sample size per group (assuming balanced design). Using Cohen’s f:\n\\[\n\\lambda = r k f^2\n\\]"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#example",
    "href": "lectures/week-09_power-analysis.html#example",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "Using the above means and \\(\\sigma^2 = 16\\), with \\(r = 10\\) per group:\n\\[\n\\sum (\\mu_i - \\mu)^2 = 4 + 0 + 4 = 8\n\\]\n\\[\n\\lambda = \\frac{10 \\times 8}{16} = 5\n\\]\nOr via \\(f\\):\n\\[\n\\lambda = 10 \\times 3 \\times (0.408)^2 = 30 \\times 0.1665 \\approx 5\n\\]"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#power-calculation",
    "href": "lectures/week-09_power-analysis.html#power-calculation",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "Power is calculated using the noncentral F-distribution:\n\nNumerator degrees of freedom: \\(df_1 = k - 1\\)\nDenominator degrees of freedom: \\(df_2 = N - k\\), where \\(N = r k\\)\nCritical F-value: \\(F_{\\text{crit}}\\) at \\(\\alpha\\), found from the F-distribution.\nPower: \\(1 - P(F &lt;  F_{\\text{crit}} | \\lambda)\\), where \\(F\\) follows a noncentral F-distribution."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#numerical-example",
    "href": "lectures/week-09_power-analysis.html#numerical-example",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "Goal: Determine the sample size per group (\\(r\\)) for \\(k = 3\\) groups, medium effect size (\\(f = 0.25\\)), \\(\\alpha = 0.05\\), and power = 0.80."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#using-rs-pwr-package",
    "href": "lectures/week-09_power-analysis.html#using-rs-pwr-package",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "library(pwr)\n\nk &lt;- 3\nf &lt;- 0.25\nalpha &lt;- 0.05\npower &lt;- 0.80\n\nresult &lt;- pwr.anova.test(k = k, f = f, sig.level = alpha, power = power)\nprint(result)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nOutput:\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.39771\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\nWe need approximately \\(r = 53\\) subjects per group, so total \\(N = 3 \\times 53 = 159\\)."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#manual-verification",
    "href": "lectures/week-09_power-analysis.html#manual-verification",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "",
    "text": "Degrees of Freedom:\n\n\\(df_1 = k - 1 = 2\\)\n\\(df_2 = N - k = 3 \\times 53 - 3 = 156\\)\n\nNoncentrality Parameter:\n\n\\[\n\\lambda = r k f^2 = 53 \\times 3 \\times 0.25^2 = 159 \\times 0.0625 = 9.9375\n\\]\n\nCritical F-value:\n\n\\[\nF_{\\text{crit}} = qf(0.95, 2, 156) \\approx 3.05\n\\]\n\nPower:\n\n\\[\n\\text{Power} = 1 - pf(3.05, 2, 156, ncp = 9.9375)\n\\]\n\npower_manual &lt;- 1 - pf(qf(0.95, 2, 156), 2, 156, ncp = 9.9375)\nprint(power_manual)\n\n[1] 0.8048873\n\n\nOutput: Approximately 0.803, confirming the power is close to 0.80, validating the pwr result."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#what-is-two-way-anova",
    "href": "lectures/week-09_power-analysis.html#what-is-two-way-anova",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "What is Two-Way ANOVA?",
    "text": "What is Two-Way ANOVA?\nTwo-way ANOVA examines the effects of two categorical factors (e.g., teaching method and study time) on a response variable, testing:\n\nMain Effect of Factor A\nMain Effect of Factor B\nInteraction Effect (A × B)"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#key-concepts",
    "href": "lectures/week-09_power-analysis.html#key-concepts",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Key Concepts",
    "text": "Key Concepts\nPower analysis in two-way ANOVA requires separate calculations for each effect, adjusting effect sizes and degrees of freedom accordingly."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#effect-size-cohens-f-for-two-way-anova",
    "href": "lectures/week-09_power-analysis.html#effect-size-cohens-f-for-two-way-anova",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Effect Size: Cohen’s F for Two-Way ANOVA",
    "text": "Effect Size: Cohen’s F for Two-Way ANOVA\nFor each effect, Cohen’s f is the ratio of the effect’s standard deviation to the within-group standard deviation (\\(\\sigma\\)).\n\nMain Effect A:\n\n\\[\nf_A = \\frac{\\sigma_A}{\\sigma}, \\quad \\sigma_A = \\sqrt{\\frac{\\sum_{i=1}^a (\\alpha_i)^2}{a}}\n\\]\nwhere \\(\\alpha_i = \\mu_{i.} - \\mu\\) (deviation of Factor A’s level means from the grand mean).\n\nMain Effect B:\n\n\\[\nf_B = \\frac{\\sigma_B}{\\sigma}, \\quad \\sigma_B = \\sqrt{\\frac{\\sum_{j=1}^b (\\beta_j)^2}{b}}\n\\]\nwhere \\(\\beta_j = \\mu_{.j} - \\mu\\).\n\nInteraction Effect:\n\n\\[\nf_{AB} = \\frac{\\sigma_{AB}}{\\sigma}, \\quad \\sigma_{AB} = \\sqrt{\\frac{\\sum_{i=1}^a \\sum_{j=1}^b (\\gamma_{ij})^2}{a b}}\n\\]\nwhere \\(\\gamma_{ij} = \\mu_{ij} - \\mu_{i.} - \\mu_{.j} + \\mu\\) (interaction terms)."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#noncentrality-parameter-λ-1",
    "href": "lectures/week-09_power-analysis.html#noncentrality-parameter-λ-1",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Noncentrality Parameter (λ)",
    "text": "Noncentrality Parameter (λ)\nAssuming \\(r\\) replicates per cell, \\(a\\) levels of Factor A, and \\(b\\) levels of Factor B:\n\nMain Effect A:\n\n\\[\n\\lambda_A = \\frac{r b \\sum_{i=1}^a (\\alpha_i)^2}{\\sigma^2} = r b a f_A^2\n\\]\n\nMain Effect B:\n\n\\[\n\\lambda_B = \\frac{r a \\sum_{j=1}^b (\\beta_j)^2}{\\sigma^2} = r a b f_B^2\n\\]\n\nInteraction:\n\n\\[\n\\lambda_{AB} = \\frac{r \\sum_{i=1}^a \\sum_{j=1}^b (\\gamma_{ij})^2}{\\sigma^2} = r a b f_{AB}^2\n\\]"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#degrees-of-freedom",
    "href": "lectures/week-09_power-analysis.html#degrees-of-freedom",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\n\nTotal sample size: \\(N = a b r\\)\nMain Effect A: \\(df_1 = a - 1\\), \\(df_2 = N - a b\\)\nMain Effect B: \\(df_1 = b - 1\\), \\(df_2 = N - a b\\)\nInteraction: \\(df_1 = (a - 1)(b - 1)\\), \\(df_2 = N - a b\\)"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#numerical-example-1",
    "href": "lectures/week-09_power-analysis.html#numerical-example-1",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Numerical Example",
    "text": "Numerical Example\nScenario: A 2×2 design (Factor A: 2 levels, Factor B: 2 levels). We want to detect a main effect of Factor A with a difference of 4 units between levels, \\(\\sigma^2 = 7.5\\), power = 0.80, \\(\\alpha = 0.05\\). Find \\(r\\) (replicates per cell)."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#step-1-define-the-effect",
    "href": "lectures/week-09_power-analysis.html#step-1-define-the-effect",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Step 1: Define the Effect",
    "text": "Step 1: Define the Effect\nSuppose Factor A’s marginal means are \\(\\mu_{1.} = 12\\), \\(\\mu_{2.} = 8\\), so \\(\\mu_{1.} - \\mu_{2.} = 4\\). Grand mean \\(\\mu = 10\\). Effects are:\n\\[\n\\alpha_1 = 12 - 10 = 2, \\quad \\alpha_2 = 8 - 10 = -2\n\\]\n\\[\n\\sum \\alpha_i^2 = 2^2 + (-2)^2 = 8\n\\]"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#step-2-noncentrality-parameter",
    "href": "lectures/week-09_power-analysis.html#step-2-noncentrality-parameter",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Step 2: Noncentrality Parameter",
    "text": "Step 2: Noncentrality Parameter\n\\[\n\\lambda_A = \\frac{r b \\sum \\alpha_i^2}{\\sigma^2} = \\frac{r \\times 2 \\times 8}{7.5} = \\frac{16 r}{7.5}\n\\]"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#step-3-degrees-of-freedom",
    "href": "lectures/week-09_power-analysis.html#step-3-degrees-of-freedom",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Step 3: Degrees of Freedom",
    "text": "Step 3: Degrees of Freedom\n\n\\(df_1 = a - 1 = 1\\)\n\\(df_2 = N - a b = 2 \\times 2 \\times r - 4 = 4r - 4\\)"
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#step-4-manual-power-calculation",
    "href": "lectures/week-09_power-analysis.html#step-4-manual-power-calculation",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Step 4: Manual Power Calculation",
    "text": "Step 4: Manual Power Calculation\nFor \\(r = 5\\):\n\n\\(N = 2 \\times 2 \\times 5 = 20\\)\n\\(df_2 = 20 - 4 = 16\\)\n\\(\\lambda_A = \\frac{16 \\times 5}{7.5} \\approx 10.67\\)\n\\(F_{\\text{crit}} = qf(0.95, 1, 16) \\approx 4.49\\)\n\n\\[\n\\text{Power} = 1 - pf(4.49, 1, 16, ncp = 10.67)\n\\]\n\npower &lt;- 1 - pf(qf(0.95, 1, 16), 1, 16, ncp = 10.67)\nprint(power)\n\n[1] 0.8654718\n\n\nOutput: ~0.85 &gt; 0.80.\nFor \\(r = 4\\):\n\n\\(df_2 = 4 \\times 4 - 4 = 12\\)\n\\(\\lambda_A = \\frac{16 \\times 4}{7.5} \\approx 8.53\\)\n\\(F_{\\text{crit}} \\approx 4.75\\)\nPower ~0.71 &lt;- 0.80.\n\nThus, \\(r = 5\\) is the smallest integer satisfying power ≥ 0.80."
  },
  {
    "objectID": "lectures/week-09_power-analysis.html#using-r-approximation-via-pwr",
    "href": "lectures/week-09_power-analysis.html#using-r-approximation-via-pwr",
    "title": "Power Analysis in One-Way and Two-Way ANOVA",
    "section": "Using R (Approximation via pwr)",
    "text": "Using R (Approximation via pwr)\nSince pwr doesn’t directly handle two-way ANOVA, we approximate the main effect as a t-test (since \\(df_1 = 1\\)):\n\\[\nd = \\frac{\\mu_{1.} - \\mu_{2.}}{\\sqrt{\\sigma^2 / (r b)}} = \\frac{4}{\\sqrt{7.5 / (r \\times 2)}}\n\\]\nBut for precision, use manual F-test or specialized tools like G*Power. The pwr2 package isn’t standard for this; we rely on pwr for one-way or custom calculations."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "This example highlights the design of an experiment to compare the effects of inpatient and outpatient protocols on the in-laboratory measurement of resting metabolic rate (RMR) in humans. The study was conducted to address discrepancies between different experimental protocols and their implications for RMR measurements.\n\n\n\nA prior study observed that RMR measurements on elderly individuals were, on average, 8% higher using an outpatient protocol compared to an inpatient protocol.\nSuch discrepancies could undermine the comparability of results across laboratories using different protocols.\nThe goal of the study was to assess whether the protocol type has a negligible effect on RMR measurements.\n\n\n\n\n\n\n\nTo determine whether different protocols for measuring RMR significantly affect the results.\n\n\n\nThree experimental treatments were defined based on the protocol type:\n\nInpatient Protocol with Controlled Meals:\n\nPatients were fed a standardized evening meal and spent the night in the laboratory.\nRMR was measured in the morning.\n\nOutpatient Protocol with Controlled Meals:\n\nPatients were fed the same standardized evening meal at the laboratory but spent the night at home.\nRMR was measured in the morning.\n\nOutpatient Protocol with Uncontrolled Meals:\n\nPatients were instructed to eat their regular evening meal at home and spent the night at home.\nRMR was measured in the morning.\n\n\n\n\n\n\nNull Hypothesis (\\(H_0\\)): The protocol type has no effect on the measured RMR.\nAlternative Hypothesis (\\(H_a\\)): The protocol type has a significant effect on the measured RMR.\n\n\n\n\n\n\n\nTo reduce variability unrelated to the protocols, blocking factors such as age, gender, or baseline metabolic rate could be included in the design.\n\n\n\n\nRandom assignment of participants to the three protocol groups ensures that systematic biases are minimized.\n\n\n\n\n\nThe primary response variable is the measured RMR (in units such as \\(\\text{kcal/day}\\)).\n\n\n\n\n\nControlling meal content and timing for the controlled protocols.\nEnsuring adherence to the protocols for the outpatient treatments.\n\n\n\n\n\nThe data collected from the experiment can be analyzed using Analysis of Variance (ANOVA), as it is well-suited for comparing means across multiple groups.\n\n\n\nANOVA Model:\n\nResponse variable: RMR.\nFactor: Protocol type (3 levels: inpatient controlled, outpatient controlled, outpatient uncontrolled).\n\nThe ANOVA model can be expressed as:\n\n\\[\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij},\n\\]\nwhere:\n\n\\(y_{ij}\\): Observed RMR for the \\(j\\)-th participant in the \\(i\\)-th protocol.\n\\(\\mu\\): Overall mean RMR.\n\\(\\tau_i\\): Effect of the \\(i\\)-th protocol.\n\\(\\epsilon_{ij}\\): Random error term.\n\n\nNull and Alternative Hypotheses:\n\n\\(H_0: \\tau_1 = \\tau_2 = \\tau_3 = 0\\) (no effect of protocol type).\n\\(H_a: \\text{At least one } \\tau_i \\neq 0\\) (protocol type has an effect).\n\nF-statistic:\n\nCompute the ratio of between-group variability to within-group variability.\n\nPost-hoc Tests:\n\nIf \\(H_0\\) is rejected, conduct pairwise comparisons (e.g., Tukey’s test) to identify significant differences between protocol types."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html#introduction",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html#introduction",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "This example highlights the design of an experiment to compare the effects of inpatient and outpatient protocols on the in-laboratory measurement of resting metabolic rate (RMR) in humans. The study was conducted to address discrepancies between different experimental protocols and their implications for RMR measurements.\n\n\n\nA prior study observed that RMR measurements on elderly individuals were, on average, 8% higher using an outpatient protocol compared to an inpatient protocol.\nSuch discrepancies could undermine the comparability of results across laboratories using different protocols.\nThe goal of the study was to assess whether the protocol type has a negligible effect on RMR measurements."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html#experimental-design",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html#experimental-design",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "To determine whether different protocols for measuring RMR significantly affect the results.\n\n\n\nThree experimental treatments were defined based on the protocol type:\n\nInpatient Protocol with Controlled Meals:\n\nPatients were fed a standardized evening meal and spent the night in the laboratory.\nRMR was measured in the morning.\n\nOutpatient Protocol with Controlled Meals:\n\nPatients were fed the same standardized evening meal at the laboratory but spent the night at home.\nRMR was measured in the morning.\n\nOutpatient Protocol with Uncontrolled Meals:\n\nPatients were instructed to eat their regular evening meal at home and spent the night at home.\nRMR was measured in the morning.\n\n\n\n\n\n\nNull Hypothesis (\\(H_0\\)): The protocol type has no effect on the measured RMR.\nAlternative Hypothesis (\\(H_a\\)): The protocol type has a significant effect on the measured RMR."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html#considerations-for-experimental-design",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html#considerations-for-experimental-design",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "To reduce variability unrelated to the protocols, blocking factors such as age, gender, or baseline metabolic rate could be included in the design.\n\n\n\n\nRandom assignment of participants to the three protocol groups ensures that systematic biases are minimized.\n\n\n\n\n\nThe primary response variable is the measured RMR (in units such as \\(\\text{kcal/day}\\)).\n\n\n\n\n\nControlling meal content and timing for the controlled protocols.\nEnsuring adherence to the protocols for the outpatient treatments."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html#statistical-analysis",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html#statistical-analysis",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "The data collected from the experiment can be analyzed using Analysis of Variance (ANOVA), as it is well-suited for comparing means across multiple groups.\n\n\n\nANOVA Model:\n\nResponse variable: RMR.\nFactor: Protocol type (3 levels: inpatient controlled, outpatient controlled, outpatient uncontrolled).\n\nThe ANOVA model can be expressed as:\n\n\\[\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij},\n\\]\nwhere:\n\n\\(y_{ij}\\): Observed RMR for the \\(j\\)-th participant in the \\(i\\)-th protocol.\n\\(\\mu\\): Overall mean RMR.\n\\(\\tau_i\\): Effect of the \\(i\\)-th protocol.\n\\(\\epsilon_{ij}\\): Random error term.\n\n\nNull and Alternative Hypotheses:\n\n\\(H_0: \\tau_1 = \\tau_2 = \\tau_3 = 0\\) (no effect of protocol type).\n\\(H_a: \\text{At least one } \\tau_i \\neq 0\\) (protocol type has an effect).\n\nF-statistic:\n\nCompute the ratio of between-group variability to within-group variability.\n\nPost-hoc Tests:\n\nIf \\(H_0\\) is rejected, conduct pairwise comparisons (e.g., Tukey’s test) to identify significant differences between protocol types."
  },
  {
    "objectID": "lectures/week-06_higher-order.html",
    "href": "lectures/week-06_higher-order.html",
    "title": "Higher-Order Designs",
    "section": "",
    "text": "In experimental research, understanding the effects of multiple factors and their interactions is crucial for optimizing outcomes and making informed decisions. Factorial designs allow researchers to evaluate not only the individual (main) effects of factors but also how factors interact with each other to influence a response variable. Higher-order factorial designs, involving three or more factors, extend these principles, enabling the exploration of complex relationships within data.\nThis lecture delves into the structure, analysis, and interpretation of higher-order factorial designs, with a focus on two-way ANOVA models. We’ll explore main effects, interaction effects, handling unequal sample sizes, and practical implementation using R.\n\n\nImagine you’re a chef experimenting with different ingredients to perfect a recipe. Factor A could be the type of flour (whole wheat, all-purpose), Factor B the type of sugar (granulated, brown), and Factor C the baking temperature (350°F, 375°F, 400°F). Each factor influences the final product’s taste and texture. However, the optimal combination might not be obvious because the effect of sugar type could depend on the type of flour and the baking temperature. Higher-order factorial designs help uncover these intricate interactions systematically.\n\n\n\n\nUnderstand the structure and applications of higher-order factorial designs.\nDifferentiate between main effects and interaction effects.\nPerform two-way ANOVA and interpret its results.\nAddress challenges posed by unequal replication in factorial experiments.\nVisualize interactions and understand their implications.\nApply advanced statistical techniques using R for complex datasets.\nEngage with exercises to reinforce understanding and application of concepts.\n\n\n\n\nTo run the R code examples in these notes, we recommend installing the following packages:\n\n# Required R Packages\npacman::p_load(dplyr, ggplot2, emmeans, here, readr, ggpubr)\n\nif (!requireNamespace(\"hecedsm\", quietly = TRUE)) {\n    pacman::p_install_gh(\"lbelzile/hecedsm\")\n}\n\ntheme_set(theme_pubr())\n\n\ndplyr: Data manipulation and summarization.\nggplot2: Data visualization.\nlsmeans and emmeans: Post hoc comparisons and multiple comparisons.\npacman: Package management.\nhere: File path management.\nreadr: Reading data files."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#example",
    "href": "lectures/week-06_higher-order.html#example",
    "title": "Higher-Order Designs",
    "section": "",
    "text": "Imagine you’re a chef experimenting with different ingredients to perfect a recipe. Factor A could be the type of flour (whole wheat, all-purpose), Factor B the type of sugar (granulated, brown), and Factor C the baking temperature (350°F, 375°F, 400°F). Each factor influences the final product’s taste and texture. However, the optimal combination might not be obvious because the effect of sugar type could depend on the type of flour and the baking temperature. Higher-order factorial designs help uncover these intricate interactions systematically."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#objectives",
    "href": "lectures/week-06_higher-order.html#objectives",
    "title": "Higher-Order Designs",
    "section": "",
    "text": "Understand the structure and applications of higher-order factorial designs.\nDifferentiate between main effects and interaction effects.\nPerform two-way ANOVA and interpret its results.\nAddress challenges posed by unequal replication in factorial experiments.\nVisualize interactions and understand their implications.\nApply advanced statistical techniques using R for complex datasets.\nEngage with exercises to reinforce understanding and application of concepts."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#r-packages",
    "href": "lectures/week-06_higher-order.html#r-packages",
    "title": "Higher-Order Designs",
    "section": "",
    "text": "To run the R code examples in these notes, we recommend installing the following packages:\n\n# Required R Packages\npacman::p_load(dplyr, ggplot2, emmeans, here, readr, ggpubr)\n\nif (!requireNamespace(\"hecedsm\", quietly = TRUE)) {\n    pacman::p_install_gh(\"lbelzile/hecedsm\")\n}\n\ntheme_set(theme_pubr())\n\n\ndplyr: Data manipulation and summarization.\nggplot2: Data visualization.\nlsmeans and emmeans: Post hoc comparisons and multiple comparisons.\npacman: Package management.\nhere: File path management.\nreadr: Reading data files."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#factorial-design-notation",
    "href": "lectures/week-06_higher-order.html#factorial-design-notation",
    "title": "Higher-Order Designs",
    "section": "Factorial Design Notation",
    "text": "Factorial Design Notation\nA factorial experiment involves multiple factors, each with different levels. The notation \\(a \\times b \\times c \\times \\dots\\) denotes the number of levels for each factor. For example:\n\n2 × 3 × 4 Design:\n\nFactor A: 2 levels\nFactor B: 3 levels\nFactor C: 4 levels\nTotal Treatment Combinations: \\(2 \\times 3 \\times 4 = 24\\)"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#experimental-objectives",
    "href": "lectures/week-06_higher-order.html#experimental-objectives",
    "title": "Higher-Order Designs",
    "section": "Experimental Objectives",
    "text": "Experimental Objectives\n\nEstimate Main Effects: Determine the independent influence of each factor on the response.\nQuantify Two-Factor Interactions: Assess how pairs of factors jointly affect the response.\nInvestigate Higher-Order Interactions: Explore interactions involving three or more factors."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#example-1",
    "href": "lectures/week-06_higher-order.html#example-1",
    "title": "Higher-Order Designs",
    "section": "Example",
    "text": "Example\nIn a manufacturing process, factors could include machine speed (fast, medium, slow), material type (A, B), and temperature (high, low). Understanding not only how each factor affects product quality but also how combinations (e.g., fast speed with high temperature) influence outcomes is essential for process optimization."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#general-model-for-three-way-complete-model",
    "href": "lectures/week-06_higher-order.html#general-model-for-three-way-complete-model",
    "title": "Higher-Order Designs",
    "section": "General Model for “Three-Way Complete” Model",
    "text": "General Model for “Three-Way Complete” Model\nFor three-way complete model with three factors, the general linear model is:\n\\[\nY_{ijkt} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijkt}\n\\qquad(1)\\]\nWhere:\n\n\\(Y_{ijkt}\\): Observed response.\n\\(\\mu\\): Overall mean.\n\\(\\alpha_i, \\beta_j, \\gamma_k\\): Main effects of factors \\(A, B, C\\).\n\\((\\alpha\\beta)_{ij}\\): Two-factor interaction between \\(A\\) and \\(B\\).\n\\((\\alpha\\beta\\gamma)_{ijk}\\): Three-factor interaction.\n\\(\\epsilon_{ijkt} \\sim N(0, \\sigma^2)\\): Random error term."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#cell-means-model-formulation",
    "href": "lectures/week-06_higher-order.html#cell-means-model-formulation",
    "title": "Higher-Order Designs",
    "section": "Cell Means Model Formulation",
    "text": "Cell Means Model Formulation\nThe cell means model expresses the response as a sum of cell means and deviations from these means:\n\\[\nY_{ijkt} = \\mu + \\tau_{ijk} + \\epsilon_{ijkt}\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_{ijk}\\): Cell mean for the \\(ijk\\) treatment combination.\n\\(\\epsilon_{ijkt}\\): Random error term.\nNote: The cell means model is a simplified version of the general linear model.\nObjective: Estimate cell means and deviations to understand factor effects and interactions."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#key-concepts",
    "href": "lectures/week-06_higher-order.html#key-concepts",
    "title": "Higher-Order Designs",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nModel Simplification: If interaction effects are negligible, they can be removed from the model to create a submodel. This simplification aids in interpretation and model fitting. -For instance, if the factors A and B are known to be non-interacting in a three-factor experiment, then the AB and ABC interaction effects are negligible, necessitating the exclusion of the terms \\((αβ)_{ij}\\) and \\((αβγ)_{i jk}\\) from model in Equation 1. In the most extreme scenario, if no factors are anticipated to interact, a main-effects model (which incorporates no interaction terms) can be employed.\nInteraction Model Inclusion: When a model includes an interaction between a specific set of factors, all interaction terms involving subsets of those factors should be included.\nSubmodel Advantages: Simpler submodels generally yield tighter confidence intervals and more powerful hypothesis tests."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#assumptions",
    "href": "lectures/week-06_higher-order.html#assumptions",
    "title": "Higher-Order Designs",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependence: Observations are independent.\nHomoscedasticity: Constant variance across all treatment combinations.\nNormality: Errors are normally distributed.\n\n\nImportance\nThese assumptions underpin the validity of ANOVA results. Violations can lead to incorrect inferences, necessitating diagnostic checks and potential corrective measures."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#introduction-1",
    "href": "lectures/week-06_higher-order.html#introduction-1",
    "title": "Higher-Order Designs",
    "section": "Introduction",
    "text": "Introduction\nIn factorial experiments, interactions occur when the effect of one factor depends on the level of another factor. These interactions provide valuable insights into complex relationships between factors. Understanding interactions is crucial for accurately interpreting experimental results and making informed decisions in practical applications.\n\nTwo-Factor Interactions\nAn interaction between two factors, say A and B, occurs when the effect of A on the response variable depends on the level of B, or vice versa. A common method for detecting interactions is through interaction plots, where the response variable is plotted against one factor at different levels of another factor."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#example-a-hypothetical-3-times-2-times-2-experiment",
    "href": "lectures/week-06_higher-order.html#example-a-hypothetical-3-times-2-times-2-experiment",
    "title": "Higher-Order Designs",
    "section": "Example: A Hypothetical \\(3 \\times 2 \\times 2\\) Experiment",
    "text": "Example: A Hypothetical \\(3 \\times 2 \\times 2\\) Experiment\nConsider a factorial experiment with three factors:\n\nA (3 levels)\nB (2 levels)\nC (2 levels)\n\nThe sample means for each treatment combination provide insight into possible interactions. A two-factor interaction plot (AB-interaction) can be examined by averaging over the levels of C. If the interaction plot reveals non-parallel lines, an interaction is likely present."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#interpretation-of-an-interaction-plot",
    "href": "lectures/week-06_higher-order.html#interpretation-of-an-interaction-plot",
    "title": "Higher-Order Designs",
    "section": "Interpretation of an Interaction Plot",
    "text": "Interpretation of an Interaction Plot\n\nParallel lines: No interaction, meaning the effect of one factor is consistent across the levels of the other.\nNon-parallel lines: Possible interaction, indicating that the effect of one factor changes depending on the level of the other.\n\nHowever, interpreting an interaction plot should be done cautiously since it does not account for experimental error."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#three-factor-interactions",
    "href": "lectures/week-06_higher-order.html#three-factor-interactions",
    "title": "Higher-Order Designs",
    "section": "Three-Factor Interactions",
    "text": "Three-Factor Interactions\nA three-factor interaction (\\(ABC\\)) is present if the interaction between A and B changes depending on the level of C. This can be evaluated using separate interaction plots at different levels of C.\n\nNo three-factor interaction: The patterns in AB-interaction plots remain consistent across levels of C.\nSignificant three-factor interaction: The AB-interaction patterns change across levels of C, meaning the combined effect of A and B depends on C."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#graphical-representation",
    "href": "lectures/week-06_higher-order.html#graphical-representation",
    "title": "Higher-Order Designs",
    "section": "Graphical Representation",
    "text": "Graphical Representation\n\nTwo-factor interaction plots at each level of C: If the interaction pattern remains the same, C does not influence the AB-interaction.\nDifferences in interaction patterns across C: Suggests an ABC-interaction, indicating that the effect of A and B together depends on C."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#higher-order-interactions",
    "href": "lectures/week-06_higher-order.html#higher-order-interactions",
    "title": "Higher-Order Designs",
    "section": "Higher-Order Interactions",
    "text": "Higher-Order Interactions\nInteractions involving four or more factors become increasingly difficult to interpret. While graphical methods are useful for two and three-factor interactions, analysis of variance (ANOVA) tables become essential for evaluating higher-order interactions. In practice:\n\nHigher-order interactions tend to be small and may be ignored unless evidence suggests otherwise.\nLower-order interactions (twoand three-factor interactions) are prioritized in interpretation."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#what-is-separability-of-factorial-effects",
    "href": "lectures/week-06_higher-order.html#what-is-separability-of-factorial-effects",
    "title": "Higher-Order Designs",
    "section": "What Is Separability of Factorial Effects?",
    "text": "What Is Separability of Factorial Effects?\nIn a factorial experiment, several factors are varied simultaneously to study their effects on a response variable. Separability refers to the idea that if some interactions among factors are known or assumed to be negligible, then the effects of these factors can be analyzed separately (i.e., “separately” or “independently”) from the others."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#example-2",
    "href": "lectures/week-06_higher-order.html#example-2",
    "title": "Higher-Order Designs",
    "section": "Example",
    "text": "Example\n\nSuppose you have three factors—A, B, and C.\nIf prior knowledge or experimental evidence suggests that factor C does not interact with factors A or B, then the interactions AC, BC, and the three-factor interaction ABC can be removed from the model.\nIn practical terms, this means you can determine the optimal levels of A and B independently of C and vice versa, greatly simplifying both the design and interpretation of the experiment."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#why-is-it-useful",
    "href": "lectures/week-06_higher-order.html#why-is-it-useful",
    "title": "Higher-Order Designs",
    "section": "Why Is It Useful?",
    "text": "Why Is It Useful?\n\nReduced Complexity: With fewer interaction terms, the model has fewer parameters to estimate, which can improve precision and make the analysis more straightforward.\nIndependent Decision-Making: When interactions are negligible, the optimal settings for one subset of factors can be determined without worrying about their influence on other factors.\nEfficient Use of Data: A single, well-designed factorial experiment usually provides more comprehensive information than several smaller experiments done separately.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe assumption of separability must be justified by prior knowledge or preliminary analysis. Otherwise, omitting important interactions might lead to biased conclusions."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#imposing-separability",
    "href": "lectures/week-06_higher-order.html#imposing-separability",
    "title": "Higher-Order Designs",
    "section": "Imposing Separability",
    "text": "Imposing Separability\nConsider a three-factor experiment with factors A, B, and C:\n\\[\nY_{ijkt} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijkt},\n\\]\nIf prior knowledge tells us that factor C does not interact with factors A or B, then the following interaction terms are assumed to be zero:\n\\[\n(\\alpha\\gamma)_{ik} = 0,\\quad (\\beta\\gamma)_{jk} = 0,\\quad (\\alpha\\beta\\gamma)_{ijk} = 0.\n\\]\nThus, the model simplifies to:\n\\[\nY_{ijkt} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + \\epsilon_{ijkt}.\n\\qquad(2)\\]"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#derivation-outline",
    "href": "lectures/week-06_higher-order.html#derivation-outline",
    "title": "Higher-Order Designs",
    "section": "Derivation Outline",
    "text": "Derivation Outline\n\nFull Model Setup: Start with the full model including all interactions.\nHypothesis of Negligible Interactions: Hypothesize that \\((\\alpha\\gamma)_{ik} = 0\\), \\((\\beta\\gamma)_{jk} = 0\\), and \\((\\alpha\\beta\\gamma)_{ijk} = 0\\). This assumption is based on prior experiments or domain knowledge (see Montgomery (2020)).\nSimplification of the Model: Substitute the zero interaction terms into the full model to yield the simplified form:\n\n\\[\nY_{ijkt} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + \\epsilon_{ijkt}.\n\\]\n\nParameter Estimation and Orthogonality: In a balanced (orthogonal) design, the estimates of the main effects and the remaining interaction (\\(\\alpha\\beta\\)) are independent of the omitted interaction terms. This follows from the Gauss-Markov theorem, which assures that the least squares estimators are the Best Linear Unbiased Estimators (BLUE) under the classical linear model assumptions (Box et al., 2005)\n\nOrthogonality Condition: The design matrix is constructed so that the columns corresponding to main effects and the \\(AB\\) interaction are orthogonal to those corresponding to the omitted interactions. This ensures that the parameter estimates remain unbiased when the negligible interactions are excluded.\n\nDegrees of Freedom: With fewer parameters to estimate, more degrees of freedom are available for estimating the error variance, leading to increased precision in hypothesis tests regarding the main effects and the \\(AB\\) interaction.\n\nSummary: The mathematical derivation shows that by assuming certain interactions are zero, the model simplifies and the estimation of the remaining effects becomes more robust due to increased orthogonality and degrees of freedom."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#what-are-interaction-graphs",
    "href": "lectures/week-06_higher-order.html#what-are-interaction-graphs",
    "title": "Higher-Order Designs",
    "section": "What Are Interaction Graphs?",
    "text": "What Are Interaction Graphs?\nInteraction graphs provide a visual tool to decide which interactions to include in your model:\n\nNodes: Represent the factors (e.g., A, B, C, etc.).\nEdges: Drawn between nodes that are suspected to interact.\nHigher-Order Interactions: Triangular or complete connections among three or more nodes indicate the possibility of higher-order interactions."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#steps-for-constructing-an-interaction-graph",
    "href": "lectures/week-06_higher-order.html#steps-for-constructing-an-interaction-graph",
    "title": "Higher-Order Designs",
    "section": "Steps for Constructing an Interaction Graph",
    "text": "Steps for Constructing an Interaction Graph\n\nPlot the Factors: Draw points (nodes) for each factor in your experiment.\nConnect Suspected Interacting Factors: Draw a line between two nodes if you suspect a two-factor interaction.\n\nIn our example, if factors A and B interact, connect them.\n\nIndicate Higher-Order Interactions: If a three-factor interaction is possible, form a triangle among the three factors.\n\nIf factor C does not interact with A or B, leave C disconnected from A and B.\n\nComplexity for More Factors: For experiments with more than three factors, complete graphs (all nodes interconnected) suggest potential four-factor or higher interactions.\n\nPractical Implication: The interaction graph visually reinforces the decision to exclude certain interactions. For instance, in our example, if C is not connected to A or B, it supports the assumption that the interactions involving C (i.e., AC, BC, and ABC) can be dropped from the model."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#degrees-of-freedom-df",
    "href": "lectures/week-06_higher-order.html#degrees-of-freedom-df",
    "title": "Higher-Order Designs",
    "section": "Degrees of Freedom (DF)",
    "text": "Degrees of Freedom (DF)\n\nFactor A: \\(a - 1\\)\nFactor B: \\(b - 1\\)\nInteraction AB: \\((a - 1)(b - 1)\\)\nError: \\(N - ab\\)\nTotal: \\(N - 1\\)\n\nWhere \\(N = abn\\) is the total number of observations."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#sum-of-squares-ss",
    "href": "lectures/week-06_higher-order.html#sum-of-squares-ss",
    "title": "Higher-Order Designs",
    "section": "Sum of Squares (SS)",
    "text": "Sum of Squares (SS)\n\nTotal SS (\\(SS_{Total}\\)): Total variability in responses.\nFactor SS (\\(SS_A, SS_B\\)): Variability due to main effects.\nInteraction SS (\\(SS_{AB}\\)): Variability due to interactions.\nError SS (\\(SS_{Error}\\)): Unexplained variability.\n\n\\[\nSS_{Total} = \\sum (Y - \\bar{Y})^2\n\\]\n\\[\nSS_A = \\sum n_i (\\bar{Y}_{i..} - \\bar{Y})^2\n\\]\n\\[\nSS_{AB} = \\sum n_{ij} (\\bar{Y}_{ij.} - \\bar{Y}_{i..} - \\bar{Y}_{.j.} + \\bar{Y})^2\n\\]\n\\[\nSS_{Error} = SS_{Total} - SS_A - SS_B - SS_{AB}\n\\]\n\nMean Squares (MS)\n\\[\nMS = \\frac{SS}{DF}\n\\]\n\n\nANOVA Table Example\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF-statistic\n\n\n\n\nFactor A\n\\(a-1\\)\n\\(SS_A\\)\n\\(MS_A\\)\n\\(MS_A / MS_E\\)\n\n\nFactor B\n\\(b-1\\)\n\\(SS_B\\)\n\\(MS_B\\)\n\\(MS_B / MS_E\\)\n\n\nInteraction AB\n\\((a-1)(b-1)\\)\n\\(SS_{AB}\\)\n\\(MS_{AB}\\)\n\\(MS_{AB} / MS_E\\)\n\n\nError\n\\(N - ab\\)\n\\(SS_E\\)\n\\(MS_E\\)\n-\n\n\nTotal\n\\(N - 1\\)\n\\(SS_T\\)\n-\n-\n\n\n\nNote: In unbalanced designs, Type I, II, and III sums of squares may be used to handle unequal sample sizes."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#main-effects-vs.-interaction-effects",
    "href": "lectures/week-06_higher-order.html#main-effects-vs.-interaction-effects",
    "title": "Higher-Order Designs",
    "section": "Main Effects vs. Interaction Effects",
    "text": "Main Effects vs. Interaction Effects\n\nMain Effect: The independent effect of a single factor on the response.\nInteraction Effect: Occurs when the effect of one factor depends on the level of another factor.\n\n\nVisualizing Interactions\nInteraction Plots: Graphical representations where lines represent one factor’s levels across another factor’s levels.\n\nParallel Lines: Indicate no interaction; main effects are additive.\nNon-Parallel Lines: Indicate interaction; effects are dependent.\n\nExample in R:\ninteraction.plot(data$FactorA, data$FactorB, data$Response, col=c(\"blue\", \"red\"))\nInterpretation: If lines cross or are not parallel, there is an interaction effect, meaning the impact of one factor varies with the level of the other factor."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#example-popcorn-microwave-experiment",
    "href": "lectures/week-06_higher-order.html#example-popcorn-microwave-experiment",
    "title": "Higher-Order Designs",
    "section": "Example: Popcorn-Microwave Experiment",
    "text": "Example: Popcorn-Microwave Experiment\nScenario: Three factors—brand (3 levels), power (2 levels), and time (3 levels)—are tested to maximize popcorn popping percentage.\nObjective: Identify the optimal combination of brand, power, and time for best popping performance.\nR Code Example:\n\n# Load data\ndata &lt;- read_table(here(\"data\", \"dean2017\", \"popcorn.microwave.txt\")) |&gt;\n    mutate(\n        brand = factor(brand, labels = c(\"A1\", \"A2\", \"A3\")),\n        power = factor(power, labels = c(\"Low\", \"High\")),\n        time = factor(time, labels = c(\"Short\", \"Medium\", \"Long\"))\n    ) |&gt;\n    rename(PoppingRate = y)\n\ndata |&gt; head()\n\n\n\n\n\nbrand\npower\ntime\nPoppingRate\n\n\n\n\nA1\nLow\nShort\n73.8\n\n\nA1\nLow\nShort\n65.5\n\n\nA1\nLow\nMedium\n70.3\n\n\nA1\nLow\nMedium\n91.0\n\n\nA1\nLow\nLong\n72.7\n\n\nA1\nLow\nLong\n81.9\n\n\n\n\n\n\n\n\n\n# Fit three-way ANOVA model\nfit1 &lt;- aov(PoppingRate ~ brand * power * time, data = data)\n\n# Summary of ANOVA\nsummary(fit1)\n\n                 Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nbrand             2  331.1   165.6   1.889 0.18007   \npower             1  455.1   455.1   5.192 0.03512 * \ntime              2 1554.6   777.3   8.867 0.00209 **\nbrand:power       2  196.0    98.0   1.118 0.34854   \nbrand:time        4 1433.9   358.5   4.089 0.01572 * \npower:time        2   47.7    23.9   0.272 0.76484   \nbrand:power:time  4   47.3    11.8   0.135 0.96732   \nResiduals        18 1577.9    87.7                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTable 1"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#interpreting-the-three-way-anova-results-with-consideration-of-interactions",
    "href": "lectures/week-06_higher-order.html#interpreting-the-three-way-anova-results-with-consideration-of-interactions",
    "title": "Higher-Order Designs",
    "section": "Interpreting the Three-Way ANOVA Results (With Consideration of Interactions)",
    "text": "Interpreting the Three-Way ANOVA Results (With Consideration of Interactions)\nThe ANOVA table summarizes the effects of brand, power, and time on the response variable, along with their interactions. Below is a detailed interpretation:"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#key-findings",
    "href": "lectures/week-06_higher-order.html#key-findings",
    "title": "Higher-Order Designs",
    "section": "Key Findings",
    "text": "Key Findings\n\nThree-Way Interaction\n\nbrand:power:time (p = 0.967) → Not significant.\n\nThe combined effect of brand, power, and time does not show significant interaction.\n\n\n\n\nTwo-Way Interactions\n\nbrand:power (p = 0.349) → Not significant.\n\nThe effect of power does not change depending on the brand.\n\nbrand:time (p = 0.016) → Significant (p &lt; 0.05).\n\nThe effect of brand varies across different time levels.\n\npower:time (p = 0.765) → Not significant.\n\nThe effect of power does not change over time.\n\n\n\n\nMain Effects\n\npower (p = 0.035) → Significant (p &lt; 0.05).\n\nThe level of power has a significant effect on the response."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#interaction-plot-1-for-three-way-interaction--popcorn-microwave-experiment",
    "href": "lectures/week-06_higher-order.html#interaction-plot-1-for-three-way-interaction--popcorn-microwave-experiment",
    "title": "Higher-Order Designs",
    "section": "Interaction Plot 1 for Three-Way Interaction- Popcorn-Microwave Experiment",
    "text": "Interaction Plot 1 for Three-Way Interaction- Popcorn-Microwave Experiment\n\n# ggplot2 interaction plot for A and B at each level of C\n\nggplot2::ggplot(data, aes(x = brand, y = PoppingRate, color = power, group = power)) +\n    stat_summary(fun = mean, geom = \"point\") +\n    stat_summary(fun = mean, geom = \"line\") +\n    facet_wrap(~time) +\n    labs(\n        title = \"Interaction Plot for Popcorn-Microwave Experiment\",\n        x = \"Brand\",\n        y = \"Popping Rate\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\nFigure 1: Interaction Plot for Popcorn-Microwave Experiment"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#interaction-plot-2-for-three-way-interaction--popcorn-microwave-experiment",
    "href": "lectures/week-06_higher-order.html#interaction-plot-2-for-three-way-interaction--popcorn-microwave-experiment",
    "title": "Higher-Order Designs",
    "section": "Interaction Plot 2 for Three-Way Interaction- Popcorn-Microwave Experiment",
    "text": "Interaction Plot 2 for Three-Way Interaction- Popcorn-Microwave Experiment\n\n# ggplot2 interaction plot for A and B at each level of C\n\nemmeans::emmip(fit1, time ~ brand | power)\n\n\n\n\n\n\n\nFigure 2: Interaction Plot for Popcorn-Microwave Experiment"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#model-without-non-significant-interactions",
    "href": "lectures/week-06_higher-order.html#model-without-non-significant-interactions",
    "title": "Higher-Order Designs",
    "section": "Model without Non-Significant Interactions",
    "text": "Model without Non-Significant Interactions\n\n\n\n# Fit three-way ANOVA model without non-significant interactions\nfit2 &lt;- aov(PoppingRate ~ brand + power + time + brand:time, data = data)\n\n# Summary of ANOVA\nsummary(fit2)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nbrand        2  331.1   165.6   2.303 0.119991    \npower        1  455.1   455.1   6.331 0.018370 *  \ntime         2 1554.6   777.3  10.813 0.000382 ***\nbrand:time   4 1433.9   358.5   4.987 0.004052 ** \nResiduals   26 1869.0    71.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTable 2"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#post-hoc-comparisons-for-the-two-way-interaction-brandtime-simple-effects",
    "href": "lectures/week-06_higher-order.html#post-hoc-comparisons-for-the-two-way-interaction-brandtime-simple-effects",
    "title": "Higher-Order Designs",
    "section": "Post-Hoc Comparisons for the Two-Way Interaction (Brand:Time) Simple Effects",
    "text": "Post-Hoc Comparisons for the Two-Way Interaction (Brand:Time) Simple Effects\n\n\n\nemm1 &lt;- emmeans(fit2, ~ brand | time)\ncontrast(emm1, method = \"pairwise\", adjust = \"tukey\")\n\ntime = Short:\n contrast estimate SE df t.ratio p.value\n A1 - A2    -4.975  6 26  -0.830  0.6882\n A1 - A3     0.325  6 26   0.054  0.9984\n A2 - A3     5.300  6 26   0.884  0.6550\n\ntime = Medium:\n contrast estimate SE df t.ratio p.value\n A1 - A2    -4.475  6 26  -0.746  0.7384\n A1 - A3    11.400  6 26   1.902  0.1584\n A2 - A3    15.875  6 26   2.648  0.0351\n\ntime = Long:\n contrast estimate SE df t.ratio p.value\n A1 - A2    23.975  6 26   3.999  0.0013\n A1 - A3    10.175  6 26   1.697  0.2253\n A2 - A3   -13.800  6 26  -2.302  0.0733\n\nResults are averaged over the levels of: power \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\nTable 3: Post Hoc Comparisons for Brand:Time Interaction"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#post-hoc-comparisons-for-brandtime-interaction",
    "href": "lectures/week-06_higher-order.html#post-hoc-comparisons-for-brandtime-interaction",
    "title": "Higher-Order Designs",
    "section": "Post Hoc Comparisons for Brand:Time Interaction",
    "text": "Post Hoc Comparisons for Brand:Time Interaction\n\nTime = Short\n\nA1 - A2: Difference not significant (p = 0.6882).\nA1 - A3: Difference not significant (p = 0.9984).\nA2 - A3: Difference not significant (p = 0.6550).\nConclusion: No significant differences in popping rate between brands at the short time level.\n\n\n\nTime = Medium\n\nA1 - A2: Difference not significant (p = 0.7384).\nA1 - A3: Difference not significant (p = 0.1584).\nA2 - A3: Difference significant (p = 0.0351).\nConclusion: Brand A2 has a significantly higher popping rate than A1 at the medium time level.\n\n\n\nTime = Long\n\nA1 - A2: Difference significant (p = 0.0013).\nA1 - A3: Difference not significant (p = 0.2253).\nA2 - A3: Difference not significant (p = 0.0733).\nConclusion: Brand A1 has a significantly higher popping rate than A2 at the long time level."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#equal-sample-sizes-the-setup",
    "href": "lectures/week-06_higher-order.html#equal-sample-sizes-the-setup",
    "title": "Higher-Order Designs",
    "section": "Equal Sample Sizes: The Setup",
    "text": "Equal Sample Sizes: The Setup\nSuppose we have an experiment with \\(p\\) crossed factors (e.g., factors \\(A, B, C, \\ldots\\)), each having levels \\(a, b, c, \\ldots\\). We focus on a complete model or any chosen submodel. Let the total number of replicates in each treatment combination be \\(r\\). Thus, each combination of factor levels has exactly \\(r\\) observations.\nKey advantages of equal sample sizes:\n\nFormulas for sums of squares and degrees of freedom are simpler.\nEstimation of main effects and interactions via least squares takes a simpler form.\nSums of squares decompose additively among factors and interactions in many factorial designs.\n\nWe will illustrate with a four-factor design \\(A, B, C, D\\) (each with \\(a, b, c, d\\) levels respectively), though the concepts generalize to any number of factors."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#model-description-and-terminology",
    "href": "lectures/week-06_higher-order.html#model-description-and-terminology",
    "title": "Higher-Order Designs",
    "section": "Model Description and Terminology",
    "text": "Model Description and Terminology\nConsider a four-way complete model with crossed factors \\(A, B, C, D\\). Denote:\n\n\\(\\mu\\): overall mean\n\\(\\alpha_i\\), \\(\\beta_j\\), \\(\\gamma_k\\), \\(\\delta_l\\): main effects for each factor, where \\(i = 1,\\ldots,a\\), \\(j = 1,\\ldots,b\\), \\(k = 1,\\ldots,c\\), and \\(l = 1,\\ldots,d\\).\n\\(\\alpha \\beta_{ij}\\), \\(\\alpha \\gamma_{ik}\\), \\(\\beta \\gamma_{jk}\\), \\(\\ldots\\): two-way interactions\n\\(\\alpha \\beta \\gamma_{ijk}\\), \\(\\ldots\\): three-way interactions\n\\(\\alpha \\beta \\gamma \\delta_{ijkl}\\): four-way interaction\n\\(\\varepsilon_{ijklt}\\): random error for observation \\(t\\) in the \\((i, j, k, l)\\)-th combination, where \\(t = 1, \\ldots, r\\)."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#model-equation",
    "href": "lectures/week-06_higher-order.html#model-equation",
    "title": "Higher-Order Designs",
    "section": "Model Equation",
    "text": "Model Equation\n\\[\ny_{ijklt}\n= \\mu\n+ \\alpha_i + \\beta_j + \\gamma_k + \\delta_l\n+ (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + \\ldots\n+ (\\alpha\\beta\\gamma\\delta)_{ijkl}\n+ \\varepsilon_{ijklt}.\n\\]\nWhen sample sizes are equal \\((= r)\\), standard formulas for the sums of squares become more direct. If a submodel omits certain higher-order interactions (assuming they are negligible), the same patterns of degrees of freedom and sums of squares still apply, but excluding those omitted terms."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#degrees-of-freedom-for-factorial-effects",
    "href": "lectures/week-06_higher-order.html#degrees-of-freedom-for-factorial-effects",
    "title": "Higher-Order Designs",
    "section": "Degrees of Freedom for Factorial Effects",
    "text": "Degrees of Freedom for Factorial Effects\nFor any factorial effect, the degrees of freedom (\\(\\nu\\)) is the product of “levels minus 1” across all factors included in that effect. For example, an \\(A\\)-by-\\(B\\)-by-\\(D\\) interaction with factors \\(A, B, D\\) having \\(a, b, d\\) levels respectively yields:\n\\[\n\\nu_{ABD} = (a - 1)(b - 1)(d - 1).\n\\]\nSimilarly, a main effect \\(A\\) alone has \\(\\nu_A = a - 1\\) degrees of freedom. This pattern continues for interactions of any order."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#sums-of-squares-in-factorial-models",
    "href": "lectures/week-06_higher-order.html#sums-of-squares-in-factorial-models",
    "title": "Higher-Order Designs",
    "section": "Sums of Squares in Factorial Models",
    "text": "Sums of Squares in Factorial Models\nIf each cell (treatment combination) has \\(r\\) observations, the sums of squares for a main effect or interaction follow a specific pattern: we take (1) the cell means averaged over subscript indices, (2) keep certain signs and expansions, and (3) sum of squares. One can write the sum of squares for an interaction, say \\(ABD\\), using bracketed averages:\n\\[\n\\text{SS}(ABD)\n=\nr \\sum_{i,j,l}\n\\Big[\n\\bar{y}_{ij\\cdot l\\cdot} \\;-\\; \\bar{y}_{ij\\cdot\\cdot\\cdot}\n\\;-\\; \\bar{y}_{i\\cdot\\cdot l\\cdot\\cdot}\n\\;-\\; \\bar{y}_{\\cdot j\\cdot l\\cdot\\cdot}\n\\;+\\; \\bar{y}_{i\\cdot\\cdot\\cdot\\cdot\\cdot}\n\\;+\\; \\bar{y}_{\\cdot j\\cdot\\cdot\\cdot\\cdot}\n\\;+\\; \\bar{y}_{\\cdot\\cdot\\cdot l\\cdot\\cdot}\n\\;-\\; \\bar{y}_{\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot}\n\\Big]^2\n\\]\nwhere \\(\\bar{y}_{ij.l\\cdot}\\) is the average over all observations with factor \\(A\\) at level \\(i\\), \\(B\\) at level \\(j\\), \\(D\\) at level \\(l\\), and “\\(\\cdot\\)” means averaging over all other subscripts (like factor \\(C\\) and replicate index \\(t\\)). Each bracketed difference is then squared and summed over all relevant combinations \\(i, j, l\\).\nFor a main effect or any other interaction, the same principle applies: we expand the parentheses with the correct pattern of plus and minus signs, average out the indices that do not appear, square the bracket, and multiply by \\(r\\) (or by some factor depending on the number of levels omitted)."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#total-sum-of-squares-and-error",
    "href": "lectures/week-06_higher-order.html#total-sum-of-squares-and-error",
    "title": "Higher-Order Designs",
    "section": "Total Sum of Squares and Error",
    "text": "Total Sum of Squares and Error\nThe total sum of squares,\n\\[\n\\text{SS}_{\\text{Total}}\n= \\sum_{i,j,k,l,t}\n\\bigl(y_{ijklt} - \\bar{y}_{\\cdot\\cdot\\cdot\\cdot\\cdot}\\bigr)^2,\n\\]\nsplits into sums of squares for factors and interactions plus the residual (or error) sum of squares,\n\\[\n\\text{SS}_E\n= \\text{SS}_{\\text{tot}}\n-\n\\bigl[\\text{SS}_A + \\text{SS}_B + \\cdots + \\text{SS}(\\text{AB}) + \\cdots + \\text{SS}(\\text{ABCD})\\bigr].\n\\]\nWith equal sample sizes, each effect sum of squares has an interpretation corresponding to “how much variability that effect and its sub-interactions explain,” and the remainder is attributed to the residual error."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#mean-squares-and-f-statistics",
    "href": "lectures/week-06_higher-order.html#mean-squares-and-f-statistics",
    "title": "Higher-Order Designs",
    "section": "Mean Squares and F-Statistics",
    "text": "Mean Squares and F-Statistics\nFor an effect \\(E\\) (e.g. a main effect or an interaction), the mean square is:\n\\[\n\\text{MS}(E)\n=\n\\frac{\\text{SS}(E)}{\\nu_E},\n\\]\nwhere \\(\\nu_E\\) is the degrees of freedom for that effect. Similarly, the error mean square is:\n\\[\n\\text{MS}(E) = \\frac{\\text{SS}(E)}{\\nu_E},\n\\]\nwhere \\(\\nu_E\\) is the error degrees of freedom.\nTo test whether a factorial effect is negligible, we form an F-ratio:\n\\[\nF = \\frac{\\text{MS}(E)}{\\text{MS}_E},\n\\]\nand compare with an \\(F\\)-distribution with \\(\\nu_E\\) (effect) and \\(\\nu_E\\) (error) degrees of freedom at a chosen significance level \\(\\alpha\\)."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#contrasts-estimable-functions-and-confidence-intervals",
    "href": "lectures/week-06_higher-order.html#contrasts-estimable-functions-and-confidence-intervals",
    "title": "Higher-Order Designs",
    "section": "Contrasts, Estimable Functions, and Confidence Intervals",
    "text": "Contrasts, Estimable Functions, and Confidence Intervals\nAn estimable contrast for a factorial effect is a linear combination of relevant parameters whose coefficients sum to zero across each subscript index. Under equal sample sizes, the least squares estimate is obtained by substituting cell means for each parameter term. The sum of squares associated to testing that a specific contrast is zero is the square of the normalized contrast estimate.\nSimultaneous confidence intervals for main effects, interactions, or contrasts can be constructed using methods such as Bonferroni, Scheffé, Tukey, or Dunnett, adjusting critical values as appropriate."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#numerical-example-in-r",
    "href": "lectures/week-06_higher-order.html#numerical-example-in-r",
    "title": "Higher-Order Designs",
    "section": "Numerical Example in R",
    "text": "Numerical Example in R\nWe will illustrate the analysis of a four-way factorial design with equal sample sizes. Suppose each factor \\(A, B, C, D\\) has 2 levels, resulting in \\(2^4 = 16\\) cells, each with \\(r = 3\\) observations. Let us simulate the data in R and perform the standard ANOVA.\n\n# Simulate data for a 2^4 factorial with equal sample size r = 3\n# True means to illustrate a possible scenario\nset.seed(1234)\nn &lt;- 16 * 3 # total observations\n# Suppose factors A, B, C, D each with 2 levels: coded as 0,1 for simplicity\n# We'll generate a design matrix with 16 unique rows\nA &lt;- rep(rep(c(0, 1), each = 8), times = 3)\nB &lt;- rep(rep(c(0, 1), each = 4, times = 2), times = 3)\nC &lt;- rep(rep(c(0, 1), each = 2, times = 4), times = 3)\nD &lt;- rep(rep(c(0, 1), times = 8), times = 3)\n\n# Let's set up a plausible model\n# y = grand mean + A + B + ... + AB + AD + ... + noise\ngrand_mean &lt;- 10\na_eff &lt;- 2\nb_eff &lt;- -1\nc_eff &lt;- 3\nd_eff &lt;- 0.5\nab_eff &lt;- 1.5\nac_eff &lt;- 0\nbc_eff &lt;- 0\nad_eff &lt;- -0.8\nbd_eff &lt;- 0\ncd_eff &lt;- 0.7\nabcd_eff &lt;- 0.2\n\n# Generate response\ny_mean &lt;- grand_mean +\n    a_eff * A +\n    b_eff * B +\n    c_eff * C +\n    d_eff * D +\n    ab_eff * A * B +\n    ac_eff * A * C +\n    bc_eff * B * C +\n    ad_eff * A * D +\n    bd_eff * B * D +\n    cd_eff * C * D +\n    abcd_eff * A * B * C * D\n\n# Add random error\ny &lt;- y_mean + rnorm(n, mean = 0, sd = 1.5)\n\nmy_data &lt;- data.frame(\n    A = factor(A),\n    B = factor(B),\n    C = factor(C),\n    D = factor(D),\n    y = y\n)\n\nmy_data |&gt; head()\n\n\n\n\n\n\n\n\nA\nB\nC\nD\ny\n\n\n\n\n0\n0\n0\n0\n8.189401\n\n\n0\n0\n0\n1\n10.916144\n\n\n0\n0\n1\n0\n14.626662\n\n\n0\n0\n1\n1\n10.681453\n\n\n0\n1\n0\n0\n9.643687\n\n\n0\n1\n0\n1\n10.259084\n\n\n\n\n\n\n\nTable 4: Simulated Data for a 2^4 Factorial Design\n\n\n\n\n\nExplanation:\n\nWe coded the factors as 0 or 1 for simulation, though in practice we can name them “A1, A2” etc.\nWe assigned main effects to A, B, C, D, plus certain interactions. We then added random noise with standard deviation 1.5."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#anova-results-for-the-24-factorial-design",
    "href": "lectures/week-06_higher-order.html#anova-results-for-the-24-factorial-design",
    "title": "Higher-Order Designs",
    "section": "ANOVA Results for the 2^4 Factorial Design",
    "text": "ANOVA Results for the 2^4 Factorial Design\n\n\n\nfit_full_aov &lt;- aov(y ~ A * B * C * D, data = my_data)\nsummary(fit_full_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nA            1  67.91   67.91  29.178 6.17e-06 ***\nB            1   0.02    0.02   0.007   0.9331    \nC            1 163.51  163.51  70.255 1.41e-09 ***\nD            1   0.23    0.23   0.101   0.7532    \nA:B          1   7.84    7.84   3.367   0.0758 .  \nA:C          1   0.06    0.06   0.026   0.8731    \nB:C          1   0.42    0.42   0.179   0.6748    \nA:D          1  12.94   12.94   5.560   0.0247 *  \nB:D          1   0.22    0.22   0.095   0.7604    \nC:D          1   1.07    1.07   0.459   0.5031    \nA:B:C        1   0.42    0.42   0.180   0.6741    \nA:B:D        1   0.49    0.49   0.210   0.6502    \nA:C:D        1   0.00    0.00   0.001   0.9710    \nB:C:D        1   0.40    0.40   0.171   0.6816    \nA:B:C:D      1   1.61    1.61   0.693   0.4114    \nResiduals   32  74.47    2.33                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTable 5: ANOVA Results for the 2^4 Factorial Design\n\n\n\n\nExplanation:\n\nWe fit the complete 4-way factorial model in R: lm(y ~ A * B * C * D).\nThe ANOVA table from anova(fit_full) partitions the total sum of squares into main effects (A, B, C, D), two-way interactions (A:B, A:C, B:C, etc.), up to the 4-way interaction (A:B:C:D), plus the residual or error.\n\nRunning this snippet in R yields sums of squares for each effect, degrees of freedom, mean squares, \\(F\\)-statistics, and p-values. We can easily see which effects are significant and draw conclusions accordingly."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#guidelines-for-analysis-and-interpretation",
    "href": "lectures/week-06_higher-order.html#guidelines-for-analysis-and-interpretation",
    "title": "Higher-Order Designs",
    "section": "Guidelines for Analysis and Interpretation",
    "text": "Guidelines for Analysis and Interpretation\n\nCheck Interactions: Always examine higher-order interactions first. If they are negligible, remove them from the model for simplicity and improved precision on lower-order terms.\nCheck Residual Diagnostics: Plot residuals versus fitted values, check normal Q–Q plots, etc., to verify that assumptions are not severely violated.\nConduct F-tests: Evaluate significance of main effects and interactions via the standard ANOVA approach.\nMultiple Comparisons: If needed, build simultaneous confidence intervals for main effects or factor level means, using Tukey, Bonferroni, Scheffé, or Dunnett methods to maintain appropriate error rates.\nFactor Collapse: If factor interactions are not present, you can “collapse” factor levels to a simpler design or analysis. Equal cell sizes make these collapses straightforward.\nPractical Significance: Consider not just p-values but effect sizes to determine whether a factor or interaction is meaningful in practice."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#advanced-section-higher-order-interactions-and-orthogonal-decompositions",
    "href": "lectures/week-06_higher-order.html#advanced-section-higher-order-interactions-and-orthogonal-decompositions",
    "title": "Higher-Order Designs",
    "section": "Advanced Section: Higher-Order Interactions and Orthogonal Decompositions",
    "text": "Advanced Section: Higher-Order Interactions and Orthogonal Decompositions\nWhen the design is balanced (equal \\(r\\)), the sums of squares due to main effects and interactions are orthogonal decompositions of the total variability. This orthogonality simplifies the structure of the F-tests. In advanced problems:\n\nOne can form a partial or incomplete model if some interactions are considered negligible.\nOne can compute sums of squares for a submodel easily by dropping negligible effects from the complete model.\nIn unbalanced settings, computations become more complex, often requiring specialized regression-based formulas or computational software to obtain Type I, II, or III sums of squares (Montgomery 2020, Christensen 2018)."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#exercises",
    "href": "lectures/week-06_higher-order.html#exercises",
    "title": "Higher-Order Designs",
    "section": "Exercises",
    "text": "Exercises\n\nConcept Check Suppose you have a 3-factor design \\(A, B, C\\) with levels \\(a=3, b=2, c=2\\) and each cell has \\(r=4\\) observations.\n\nWhat is the total number of observations?\nWrite the degrees of freedom for each main effect and each interaction.\nShow how you would write the sum of squares formula for \\(ABC\\).\n\nManual Computation Consider a 2-factor design \\(A, B\\) each with 2 levels (\\(a=2, b=2\\)) and \\(r=3\\) observations per cell. The four cell means are:\n\n\\[\n\\bar{y}_{11.} = 12,\\quad \\bar{y}_{12.} = 15,\\quad \\bar{y}_{21.} = 14,\\quad \\bar{y}_{22.} = 20.\n\\]\n\nCompute the sums of squares for \\(A\\), \\(B\\), and \\(AB\\) using the “plus-minus” expansions.\n\nVerify that their sum plus ss(error) equals the total sum of squares.\n\n\nR Analysis\nSimulate a 3-factor balanced design with factors \\(A, B, C\\) each at 3 levels, with \\(r=2\\) observations in each cell. Introduce known main effects and interactions in your data-generating code. Then\n\nFit the full factorial model.\n\nExamine the significance of each main effect and interaction.\n\nProvide conclusions and interpret in context.\n\nInteraction Plots\nSuppose in a 3-factor design \\(A, B, C\\) with equal sample sizes, you suspect a strong \\(AB\\) interaction but negligible \\(BC\\) and \\(AC\\) interactions. Write the “rules” for the degrees of freedom and the sum of squares for \\((AB)\\). Provide a short explanation of how you would confirm or refute the presence of \\(AB\\) interaction in the data."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#types-of-sums-of-squares",
    "href": "lectures/week-06_higher-order.html#types-of-sums-of-squares",
    "title": "Higher-Order Designs",
    "section": "Types of Sums of Squares",
    "text": "Types of Sums of Squares\n\nType I (Sequential): Tests factors sequentially, dependent on the order of entry.\nType II (Marginal): Tests main effects after accounting for other main effects but ignoring interactions.\nType III (Conditional): Tests main effects after accounting for all other factors, including interactions.\n\nR Implementation with car Package:\nlibrary(car)\n\n# Fit ANOVA model\nfit &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data)\n\n# Type II Sums of Squares\ncar::Anova(fit, type=\"II\")\n\n# Type III Sums of Squares\ncar::Anova(fit, type=\"III\")\nConsiderations:\n\nType III is commonly used in software like SAS and SPSS, especially when interactions are present.\nType II is suitable when interactions are not of primary interest.\n\nReferences: Christensen (2018) provides an in-depth discussion on handling unbalanced designs and choosing appropriate sums of squares."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#three-way-anova-example",
    "href": "lectures/week-06_higher-order.html#three-way-anova-example",
    "title": "Higher-Order Designs",
    "section": "Three-Way ANOVA Example",
    "text": "Three-Way ANOVA Example\nTo illustrate these concepts, let’s walk through a three-way ANOVA example and use R’s emmeans package to conduct simple effects analysis and probe interactions. We will use a dataset from a (fictional but realistic) experiment on memory retention (Three-way analysis of variance – Experimental Designs and Statistical Methods). In this study, researchers manipulated:\n\nFeedback given during learning (Factor A with levels: none, positive, negative),\nType of learning material (Factor B with levels: low freq/low emotion words, high freq/low emotion, high freq/high emotion),\nAge group of participants (Factor C with levels: fifth graders vs. high school seniors).\n\nThe outcome is the number of words recalled one week later. There are \\(3 \\times 3 \\times 2 = 18\\) experimental conditions (cells), with a balanced design of \\(n=5\\) participants per cell (90 observations total) (Three-way analysis of variance – Experimental Designs and Statistical Methods). We fit a three-way ANOVA to this data and then examine simple effects.\n\n\n\ndata(words, package = \"hecedsm\") # hypothetical dataset from Keppel & Wickens (2004)\nxtabs(words ~ feedback + material + age, data = words)\n\n, , age = fifth grade\n\n          material\nfeedback   low freq/low emotion high freq/low emotion high freq/high emotion\n  none                       42                    44                     40\n  positive                   39                    40                     22\n  negative                   40                    38                     19\n\n, , age = senior\n\n          material\nfeedback   low freq/low emotion high freq/low emotion high freq/high emotion\n  none                       42                    44                     39\n  positive                   40                    41                     37\n  negative                   42                    40                     35\n\n\n\nTable 6: Hypothetical Dataset from Keppel & Wickens (2004)"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#r-code-for-three-way-anova",
    "href": "lectures/week-06_higher-order.html#r-code-for-three-way-anova",
    "title": "Higher-Order Designs",
    "section": "R Code for Three-Way ANOVA",
    "text": "R Code for Three-Way ANOVA\n\n# Load data and fit three-way ANOVA model\nmodel &lt;- aov(words ~ feedback * material * age, data = words)\nanova(model)\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nfeedback\n2\n26.86667\n13.433333\n7.111765\n0.0015186\n\n\nmaterial\n2\n64.86667\n32.433333\n17.170588\n0.0000008\n\n\nage\n1\n14.40000\n14.400000\n7.623529\n0.0073041\n\n\nfeedback:material\n4\n14.66667\n3.666667\n1.941177\n0.1128292\n\n\nfeedback:age\n2\n8.60000\n4.300000\n2.276471\n0.1099872\n\n\nmaterial:age\n2\n16.20000\n8.100000\n4.288235\n0.0173970\n\n\nfeedback:material:age\n4\n10.00000\n2.500000\n1.323529\n0.2694608\n\n\nResiduals\n72\n136.00000\n1.888889\nNA\nNA\n\n\n\n\n\n\n\nTable 7: R Code for Three-Way ANOVA Example"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#three-way-interaction-interpretation",
    "href": "lectures/week-06_higher-order.html#three-way-interaction-interpretation",
    "title": "Higher-Order Designs",
    "section": "Three Way Interaction Interpretation",
    "text": "Three Way Interaction Interpretation\nFor padagogial purposes, we examine the three-way interaction between material type, feedback, and age group. We will use the emmeans package to probe the interaction."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#simple-effects-analysis",
    "href": "lectures/week-06_higher-order.html#simple-effects-analysis",
    "title": "Higher-Order Designs",
    "section": "Simple Effects Analysis",
    "text": "Simple Effects Analysis\nSimple Two-Way Interactions at Each Level of C: The significant material×age interaction means the difference in recall between material types is different for fifth graders versus seniors. We can explore this by testing the simple two-way effect of material × feedback within each age group. Essentially, we split the analysis by age to see how the other factors behave for each subgroup (Microsoft PowerPoint - Three-way ANOVAr.ppt).\nWe will use the emmeans package to probe the interaction."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#ls-means-for-material-and-feedback-by-age-group",
    "href": "lectures/week-06_higher-order.html#ls-means-for-material-and-feedback-by-age-group",
    "title": "Higher-Order Designs",
    "section": "LS Means for Material and Feedback by Age Group",
    "text": "LS Means for Material and Feedback by Age Group\n\n\n\nemm_material_feedback_by_age &lt;- emmeans(model, ~ material * feedback | age)\nemm_material_feedback_by_age\n\nage = fifth grade:\n material               feedback emmean    SE df lower.CL upper.CL\n low freq/low emotion   none        8.4 0.615 72     7.17     9.63\n high freq/low emotion  none        8.8 0.615 72     7.57    10.03\n high freq/high emotion none        8.0 0.615 72     6.77     9.23\n low freq/low emotion   positive    7.8 0.615 72     6.57     9.03\n high freq/low emotion  positive    8.0 0.615 72     6.77     9.23\n high freq/high emotion positive    4.4 0.615 72     3.17     5.63\n low freq/low emotion   negative    8.0 0.615 72     6.77     9.23\n high freq/low emotion  negative    7.6 0.615 72     6.37     8.83\n high freq/high emotion negative    3.8 0.615 72     2.57     5.03\n\nage = senior:\n material               feedback emmean    SE df lower.CL upper.CL\n low freq/low emotion   none        8.4 0.615 72     7.17     9.63\n high freq/low emotion  none        8.8 0.615 72     7.57    10.03\n high freq/high emotion none        7.8 0.615 72     6.57     9.03\n low freq/low emotion   positive    8.0 0.615 72     6.77     9.23\n high freq/low emotion  positive    8.2 0.615 72     6.97     9.43\n high freq/high emotion positive    7.4 0.615 72     6.17     8.63\n low freq/low emotion   negative    8.4 0.615 72     7.17     9.63\n high freq/low emotion  negative    8.0 0.615 72     6.77     9.23\n high freq/high emotion negative    7.0 0.615 72     5.77     8.23\n\nConfidence level used: 0.95 \n\n\n\nTable 8: Least-Squares Means by Age Group"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#r-code-for-simple-effects-of-material-and-feedback-by-age-group",
    "href": "lectures/week-06_higher-order.html#r-code-for-simple-effects-of-material-and-feedback-by-age-group",
    "title": "Higher-Order Designs",
    "section": "R Code for Simple Effects of Material and Feedback by Age Group",
    "text": "R Code for Simple Effects of Material and Feedback by Age Group\nIn R, the emmeans::joint_tests() function conveniently provides tests analogous to an ANOVA for specified slices of the data. We use the by argument to get separate tests for each age level. This produces two separate ANOVA summaries, one for each age group.\n\nemm_material_feedback_by_age |&gt; joint_tests(by = \"age\")\n\n\n\n\n\n\n\n\n\nmodel term\nage\ndf1\ndf2\nF.ratio\np.value\n\n\n\n\n1\nmaterial\nfifth grade\n2\n72\n19.306\n0.0000002\n\n\n3\nmaterial\nsenior\n2\n72\n2.153\n0.1235609\n\n\n12\nfeedback\nfifth grade\n2\n72\n8.718\n0.0004071\n\n\n31\nfeedback\nsenior\n2\n72\n0.671\n0.5145724\n\n\n11\nmaterial:feedback\nfifth grade\n4\n72\n3.088\n0.0209781\n\n\n5\nmaterial:feedback\nsenior\n4\n72\n0.176\n0.9498252\n\n\n\n\n\n\n\nTable 9: Simple Effects of Material by Age Group\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe degrees of freedom for error (df2) remain 72 for each slice because we are using the pooled residual from the full model as the error term.\n\n\nLet’s interpret these results."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#interpreting-the-simple-effects-analysis",
    "href": "lectures/week-06_higher-order.html#interpreting-the-simple-effects-analysis",
    "title": "Higher-Order Designs",
    "section": "Interpreting the Simple Effects Analysis",
    "text": "Interpreting the Simple Effects Analysis\n\nFifth Graders:\n\nFeedback × Material: Significant interaction (p ≈ 0.021).\nInterpretation: Both feedback and material type significantly affect recall in fifth graders, with a possible interaction between the two.\nFollow-Up: We will probe the material effect further.\n\n\n\nHigh School Seniors:\n\nNote: The feedback × material interaction is not significant in seniors.\n\n\n\nSummary:\n\nIn fifth graders, both feedback and material significantly affect recall, with a possible interaction.\nIn high school seniors, neither feedback nor material significantly affects recall."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#probing-simple-effects-of-material-and-feedback-for-fifth-graders",
    "href": "lectures/week-06_higher-order.html#probing-simple-effects-of-material-and-feedback-for-fifth-graders",
    "title": "Higher-Order Designs",
    "section": "Probing Simple Effects of Material and Feedback for Fifth Graders",
    "text": "Probing Simple Effects of Material and Feedback for Fifth Graders\nThe code below is more elegant and efficient than running multiple pairwise comparisons. It provides a comprehensive summary of the pairwise differences between material types for fifth graders.\n\n\n\nemm_material_feedback_by_age_fifth &lt;- emmeans::emmeans(\n    model,\n    ~ material | feedback * age,\n    at = list(age = \"fifth grade\")\n)\n\npairs(emm_material_feedback_by_age_fifth, adjust = \"tukey\")\n\nfeedback = none, age = fifth grade:\n contrast                                           estimate    SE df t.ratio\n (low freq/low emotion) - (high freq/low emotion)       -0.4 0.869 72  -0.460\n (low freq/low emotion) - (high freq/high emotion)       0.4 0.869 72   0.460\n (high freq/low emotion) - (high freq/high emotion)      0.8 0.869 72   0.920\n p.value\n  0.8900\n  0.8900\n  0.6293\n\nfeedback = positive, age = fifth grade:\n contrast                                           estimate    SE df t.ratio\n (low freq/low emotion) - (high freq/low emotion)       -0.2 0.869 72  -0.230\n (low freq/low emotion) - (high freq/high emotion)       3.4 0.869 72   3.912\n (high freq/low emotion) - (high freq/high emotion)      3.6 0.869 72   4.142\n p.value\n  0.9712\n  0.0006\n  0.0003\n\nfeedback = negative, age = fifth grade:\n contrast                                           estimate    SE df t.ratio\n (low freq/low emotion) - (high freq/low emotion)        0.4 0.869 72   0.460\n (low freq/low emotion) - (high freq/high emotion)       4.2 0.869 72   4.832\n (high freq/low emotion) - (high freq/high emotion)      3.8 0.869 72   4.372\n p.value\n  0.8900\n  &lt;.0001\n  0.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\nTable 10: Simple Effects of Material by Age Group\n\n\n\n\nSummary:\n\nFifth Graders: Recall significantly differs between low/low and high/low, and between low/low and high/high materials. The high/high material yields the highest recall.\nHigh School Seniors: Recall does not significantly differ between material types.\nInterpretation: The material type significantly affects recall in fifth graders, with the high/high material showing the highest recall. In contrast, material type does not significantly affect high school seniors’ recall."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#interpreting-the-two-way-anova-results",
    "href": "lectures/week-06_higher-order.html#interpreting-the-two-way-anova-results",
    "title": "Higher-Order Designs",
    "section": "Interpreting the Two-Way ANOVA Results",
    "text": "Interpreting the Two-Way ANOVA Results\nAccording to the principles discussed, since the three-way interaction is not significant, we do not necessarily have to probe a three-way pattern. However, we do have a significant two-way interaction (material × age). This suggests the effect of material type on memory depends on age group (or vice versa). We will demonstrate simple effects for that two-way interaction. This will show the workflow for probing interactions using emmeans."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#reduced-model-for-two-way-interaction-material-age",
    "href": "lectures/week-06_higher-order.html#reduced-model-for-two-way-interaction-material-age",
    "title": "Higher-Order Designs",
    "section": "Reduced Model for Two-Way Interaction (Material × Age)",
    "text": "Reduced Model for Two-Way Interaction (Material × Age)\nWe will demonstrate simple effects for that two-way interaction.\n\n\n\n# Fit two-way interaction model for material × age\nmodel_material_age &lt;- aov(words ~ feedback + material * age, data = words)\nsummary(model_material_age)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfeedback      2  26.87   13.43   6.508  0.00238 ** \nmaterial      2  64.87   32.43  15.712 1.67e-06 ***\nage           1  14.40   14.40   6.976  0.00989 ** \nmaterial:age  2  16.20    8.10   3.924  0.02358 *  \nResiduals    82 169.27    2.06                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTable 11: Two-Way Interaction Model for Material × Age"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#simple-effects-analysis-for-material-age-interaction",
    "href": "lectures/week-06_higher-order.html#simple-effects-analysis-for-material-age-interaction",
    "title": "Higher-Order Designs",
    "section": "Simple Effects Analysis for Material × Age Interaction",
    "text": "Simple Effects Analysis for Material × Age Interaction\n\nemm_material_by_age &lt;- emmeans(model, ~ material | age)\nemm_material_by_age |&gt; joint_tests(by = \"age\")\n\n\n\n\n\n\n\n\n\nmodel term\nage\ndf1\ndf2\nF.ratio\np.value\n\n\n\n\n1\nmaterial\nfifth grade\n2\n72\n19.306\n0.0000002\n\n\n3\nmaterial\nsenior\n2\n72\n2.153\n0.1235609\n\n\n\n\n\n\n\nTable 12: Simple Effects of Material by Age Group\n\n\n\n\nThe results suggest that the difference in recall between material types is different for fifth graders versus seniors. For the fifth graders, material appears to have an effect on recall, whereas for seniors, material appears to have no significant effect on recall."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#probing-simple-effects-of-material-by-age",
    "href": "lectures/week-06_higher-order.html#probing-simple-effects-of-material-by-age",
    "title": "Higher-Order Designs",
    "section": "Probing Simple Effects of Material by Age",
    "text": "Probing Simple Effects of Material by Age\n\n\n\npairs(emm_material_by_age, adjust = \"tukey\")\n\nage = fifth grade:\n contrast                                           estimate    SE df t.ratio\n (low freq/low emotion) - (high freq/low emotion)    -0.0667 0.502 72  -0.133\n (low freq/low emotion) - (high freq/high emotion)    2.6667 0.502 72   5.314\n (high freq/low emotion) - (high freq/high emotion)   2.7333 0.502 72   5.447\n p.value\n  0.9903\n  &lt;.0001\n  &lt;.0001\n\nage = senior:\n contrast                                           estimate    SE df t.ratio\n (low freq/low emotion) - (high freq/low emotion)    -0.0667 0.502 72  -0.133\n (low freq/low emotion) - (high freq/high emotion)    0.8667 0.502 72   1.727\n (high freq/low emotion) - (high freq/high emotion)   0.9333 0.502 72   1.860\n p.value\n  0.9903\n  0.2022\n  0.1579\n\nResults are averaged over the levels of: feedback \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\nTable 13: Simple Effects of Material by Age Group\n\n\n\nUpon further probing of the interaction, we see that the effect of material type on recall differs between the two age groups. For the fifth graders, the high-frequency/high-emotion material (let’s call it the “difficult material”) yields the highest recall, whereas for seniors, material type does not have a significant effect on recall."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#summary-2",
    "href": "lectures/week-06_higher-order.html#summary-2",
    "title": "Higher-Order Designs",
    "section": "Summary",
    "text": "Summary\n\nFifth Graders: Recall significantly differs between low/low and high/low, and between low/low and high/high materials. The high/high material yields the highest recall.\nHigh School Seniors: Recall does not significantly differ between material types.\n\nThe results of the simple effects analysis show that the material type significantly affects recall in fifth graders, with the high-frequency/high-emotion material (let’s call it the “difficult material”) yielding the highest recall. In contrast, material type does not significantly affect high school seniors’ recall. The significant interaction between material and age suggests that the effect of material type on memory retention differs between the two age groups. Specifically, the high-frequency/high-emotion material has a more pronounced impact on recall for fifth graders compared to high school seniors. This finding highlights the importance of considering age-specific factors when designing educational interventions or memory enhancement strategies."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#addressing-complex-interactions",
    "href": "lectures/week-06_higher-order.html#addressing-complex-interactions",
    "title": "Higher-Order Designs",
    "section": "Addressing Complex Interactions",
    "text": "Addressing Complex Interactions\n\nHigher-Order Interactions\nBeyond three factors, interactions become increasingly complex and harder to interpret. It’s essential to focus on interactions that make theoretical sense and are supported by the data.\n\n\nSimplifying the Model\n\nModel Selection: Start with a full model including all interactions. If higher-order interactions are not significant, consider removing them for simpler interpretation.\nHierarchical Models: Retain lower-order interactions when higher-order interactions are present, but omit higher-order interactions if lower-order ones are not significant.\n\nExample in R:\n# Full model\nfull_fit &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data)\n\n# Reduced model without three-way interaction\nreduced_fit &lt;- aov(Response ~ FactorA * FactorB + FactorA * FactorC + FactorB * FactorC, data = data)\n\n# Compare models\nanova(reduced_fit, full_fit)"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#residual-analysis",
    "href": "lectures/week-06_higher-order.html#residual-analysis",
    "title": "Higher-Order Designs",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nResiduals (\\(e_{ijkt}\\)) are the differences between observed and predicted values. Analyzing residuals helps detect violations of model assumptions.\n\nResidual Plots\n\nResiduals vs. Fitted Values:\n\nPurpose: Check for homoscedasticity and model fit.\nInterpretation:\n\nRandom Scatter: Assumptions hold.\nPatterns (e.g., funnel shape): Indicate heteroscedasticity.\n\n\nNormal Q-Q Plot:\n\nPurpose: Assess normality of residuals.\nInterpretation:\n\nPoints near the line: Normality holds.\nDeviations: Non-normality present.\n\n\nResiduals vs. Order of Data:\n\nPurpose: Detect non-independence or autocorrelation.\nInterpretation:\n\nRandom Scatter: Independence holds.\nPatterns or Trends: Violation of independence."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#r-implementation",
    "href": "lectures/week-06_higher-order.html#r-implementation",
    "title": "Higher-Order Designs",
    "section": "R Implementation",
    "text": "R Implementation\n# Fit the model\nfit &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data)\n\n# Residuals vs Fitted\nplot(fit, which=1)\n\n# Normal Q-Q Plot\nplot(fit, which=2)\n\n# Residuals vs Order\ndata$Order &lt;- 1:nrow(data)\nplot(data$Order, resid(fit), main=\"Residuals vs. Order\", xlab=\"Order\", ylab=\"Residuals\")\nabline(h=0, col=\"red\")\nInterpretation: Examine each plot for signs of assumption violations and consider corrective actions if needed."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#remedies-for-assumption-violations",
    "href": "lectures/week-06_higher-order.html#remedies-for-assumption-violations",
    "title": "Higher-Order Designs",
    "section": "Remedies for Assumption Violations",
    "text": "Remedies for Assumption Violations\n\nTransformations:\n\nLog Transformation: Useful for stabilizing variance when it increases with the mean.\nSquare-Root Transformation: Often used for count data.\nBox-Cox Transformation: Systematic method to find the best power transformation.\n\nR Implementation:\n\nlibrary(MASS)\n# Box-Cox Transformation\nboxcox_fit &lt;- boxcox(fit, lambda=seq(-2, 2, by=0.1))\n\n# Identify optimal lambda\noptimal_lambda &lt;- boxcox_fit$x[which.max(boxcox_fit$y)]\n\n# Transform Response\ndata$Response_transformed &lt;- (data$Response^optimal_lambda - 1) / optimal_lambda\n\n# Re-fit the model with transformed data\nfit_transformed &lt;- aov(Response_transformed ~ FactorA * FactorB * FactorC, data = data)\nsummary(fit_transformed)\n\nAlternative Models:\n\nMixed-Effects Models: Address non-independence by incorporating random effects.\nNonparametric Methods: Utilize methods like the Kruskal-Wallis test when assumptions are severely violated.\n\nR Implementation:\n\nlibrary(lme4)\n\n# Mixed-Effects Model Example\nmixed_fit &lt;- lmer(Response ~ FactorA * FactorB * FactorC + (1|RandomFactor), data = data)\nsummary(mixed_fit)"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-06_higher-order.html#confidence-intervals-for-higher-order-contrasts",
    "title": "Higher-Order Designs",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor a \\(k\\)-factor interaction, the confidence interval for a contrast \\(L\\) is given by:\n\\[\n\\text{CI} = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\nDerivation:\n\nEstimator: \\(\\hat{L} = \\sum c_i \\hat{\\mu}_i\\)\nVariance: \\(\\text{Var}(\\hat{L}) = \\sigma^2 \\sum \\frac{c_i^2}{n_i}\\) (Assuming equal sample sizes and independence)\nConfidence Interval: Using the properties of the normal distribution for large sample sizes or exact t-distribution for smaller samples."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#box-cox-transformation",
    "href": "lectures/week-06_higher-order.html#box-cox-transformation",
    "title": "Higher-Order Designs",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\nThe Box-Cox transformation seeks an optimal power transformation to stabilize variance and normalize residuals.\n\\[\nY'(\\lambda) =\n\\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\\n\\ln(Y) & \\text{if } \\lambda = 0.\n\\end{cases}\n\\]\nSteps in R:\n\nFit the ANOVA model:\n\nfit &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data)\n\nApply Box-Cox:\n\nlibrary(MASS)\nboxcox_fit &lt;- boxcox(fit, lambda=seq(-2,2, by=0.1))\noptimal_lambda &lt;- boxcox_fit$x[which.max(boxcox_fit$y)]\n\nTransform the Response:\n\ndata$Response_transformed &lt;- (data$Response^optimal_lambda - 1) / optimal_lambda\n\nRe-fit the Model with Transformed Data:\n\nfit_transformed &lt;- aov(Response_transformed ~ FactorA * FactorB * FactorC, data = data)\nsummary(fit_transformed)\nInterpretation: The optimal \\(\\lambda\\) indicates the best power transformation to achieve homoscedasticity and normality. Re-fitting the model on transformed data should show improved diagnostic plots."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#mathematical-proof-of-interaction-contrasts",
    "href": "lectures/week-06_higher-order.html#mathematical-proof-of-interaction-contrasts",
    "title": "Higher-Order Designs",
    "section": "Mathematical Proof of Interaction Contrasts",
    "text": "Mathematical Proof of Interaction Contrasts\n\nInteraction Contrasts Measure Deviations from Additivity\nDefinition: In a three-factor design, the interaction contrast measures how the combined effect of three factors deviates from the sum of their individual and two-factor interaction effects.\nProof Steps:\n\nModel Setup:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}\n\\]\n\nAdditivity Condition: If \\((\\alpha\\beta\\gamma)_{ijk} = 0\\) for all \\(i,j,k\\), then:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + \\epsilon_{ijk}\n\\]\nThis represents an additive model where the combined effects of factors are purely additive without higher-order interactions.\n\nInteraction Contrast: Define the three-way interaction contrast as:\n\n\\[\nL = (\\alpha\\beta\\gamma)_{ijk}\n\\]\n\nZero Interaction Contrasts Imply Additivity: If \\(L = 0\\) for all combinations, then:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + \\epsilon_{ijk}\n\\]\nHence, the absence of three-way interactions leads to an additive relationship among factors.\n\nConclusion: The interaction contrasts \\(L\\) quantify the extent to which the actual response deviates from what would be expected under additivity. Non-zero contrasts indicate the presence of higher-order interactions."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#key-points",
    "href": "lectures/week-06_higher-order.html#key-points",
    "title": "Higher-Order Designs",
    "section": "Key Points",
    "text": "Key Points\n\nBalanced Design:\nA balanced design means that each treatment combination (or cell) has the same number of observations. This equality of replications helps in achieving uniform precision across estimates.\nOrthogonality:\nOrthogonality refers to the statistical property where the columns of the design matrix (which correspond to different effects, such as main effects or interactions) are uncorrelated. When the columns are orthogonal, the least squares estimates of the effects are independent of one another."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#relationship-between-the-two",
    "href": "lectures/week-06_higher-order.html#relationship-between-the-two",
    "title": "Higher-Order Designs",
    "section": "Relationship Between the Two",
    "text": "Relationship Between the Two\n\nBalanced → Often Orthogonal (in Full Factorials):\nIn a full factorial experiment with equal replication (a balanced design) and proper coding (such as using orthogonal contrasts), the main effects and interaction effects tend to be orthogonal. This means that the estimates for one effect do not “borrow” information from another, simplifying both interpretation and hypothesis testing. For example, in the model\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + \\epsilon_{ijk},\n\\] the main effects \\(\\alpha_i\\), \\(\\beta_j\\), and \\(\\gamma_k\\) are orthogonal to each other, as are the interaction effects \\((\\alpha\\beta)_{ij}\\) with the main effects.\nA balanced design will typically ensure that the estimates for \\(\\alpha_i\\), \\(\\beta_j\\), and \\((\\alpha\\beta)_{ij}\\) are independent—provided that the design matrix is constructed with orthogonal contrasts. This is one of the reasons why balanced designs are highly valued—they help satisfy the conditions required for the Gauss-Markov theorem, ensuring that the least squares estimators are BLUE (Box et al., 2005)."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#orthogonality-without-balance-a-caveat",
    "href": "lectures/week-06_higher-order.html#orthogonality-without-balance-a-caveat",
    "title": "Higher-Order Designs",
    "section": "Orthogonality Without Balance: A Caveat",
    "text": "Orthogonality Without Balance: A Caveat\nIt is possible, in some cases, to have orthogonal contrasts even if the design is not perfectly balanced. However, achieving this often requires careful design construction (for instance, through specific contrast coding) that compensates for unequal sample sizes. In general, though, balance makes it much easier to achieve and verify orthogonality.\n\nBalanced Does Not Always Imply Complete Orthogonality:\nNot every balanced design will automatically produce orthogonal estimates for all effects. For instance:\n\nIn fractional factorial designs or designs with certain constraints, balance in the number of observations per cell does not guarantee that the factor levels or contrasts are arranged in an orthogonal fashion.\nIf the design involves aliasing or confounding (even in a balanced structure), some effects may not be independently estimated."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#summary-3",
    "href": "lectures/week-06_higher-order.html#summary-3",
    "title": "Higher-Order Designs",
    "section": "Summary",
    "text": "Summary\n\nBalance is a favorable condition for orthogonality, especially in full factorial experiments with equal replication and proper contrast coding.\nHowever, balance alone does not guarantee orthogonality for every effect, particularly in more complex or constrained designs.\nOrthogonality ensures independent estimation of the effects (including interactions), which is essential for clear interpretation and efficient testing of hypotheses. Yet, to achieve orthogonality, the design must be both balanced and appropriately structured.\n\nThus, while a balanced design is a key step toward orthogonal estimation of effects, it is not a blanket guarantee—additional considerations in the design (such as the structure of contrasts and the absence of confounding) are necessary to ensure that all effects, including interactions, are estimated orthogonally."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#the-concept-of-orthogonality",
    "href": "lectures/week-06_higher-order.html#the-concept-of-orthogonality",
    "title": "Higher-Order Designs",
    "section": "The Concept of Orthogonality",
    "text": "The Concept of Orthogonality\nTwo vectors (or columns in a design matrix) are said to be orthogonal if their inner product is zero. In the context of experimental design, consider two contrast vectors \\(\\mathbf{c}_1\\) and \\(\\mathbf{c}_2\\) corresponding to two treatment contrasts. They are orthogonal if\n\\[\n\\mathbf{c}_1^\\top \\mathbf{c}_2 = 0.\n\\]\nOrthogonality implies that the information contained in one contrast is independent of the information in the other."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#the-role-of-balance",
    "href": "lectures/week-06_higher-order.html#the-role-of-balance",
    "title": "Higher-Order Designs",
    "section": "The Role of Balance",
    "text": "The Role of Balance\nFor a one-way ANOVA with \\(t\\) treatments and a common replication number \\(n\\) for each treatment, the design matrix \\(\\mathbf{X}\\) can be structured so that the sum of the indicator variables for each treatment is equal. In such a design, the sum of squares associated with the treatments is completely independent of the error sum of squares."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#matrix-proof-of-orthogonality",
    "href": "lectures/week-06_higher-order.html#matrix-proof-of-orthogonality",
    "title": "Higher-Order Designs",
    "section": "Matrix Proof of Orthogonality",
    "text": "Matrix Proof of Orthogonality\nConsider the following model for a balanced one-way design:\n\\[\nY_{ij} = \\mu + \\tau_i + \\epsilon_{ij}, \\quad i = 1, 2, \\dots, t,\\quad j = 1, 2, \\dots, n,\n\\]\nwhere\n\n\\(\\mu\\) is the overall mean,\n\\(\\tau_i\\) is the effect of the \\(i\\)-th treatment (subject to the constraint \\(\\sum_{i=1}^{t} \\tau_i = 0\\)),\n\\(\\epsilon_{ij}\\) are independent errors with \\(E[\\epsilon_{ij}] = 0\\) and \\(\\operatorname{Var}(\\epsilon_{ij}) = \\sigma^2\\).\n\nLet the design matrix \\(\\mathbf{X}\\) be partitioned as\n\\[\n\\mathbf{X} = \\begin{pmatrix}\n\\mathbf{1} & \\mathbf{X}_\\tau\n\\end{pmatrix},\n\\]\nwhere \\(\\mathbf{1}\\) is an \\(N \\times 1\\) vector of ones (with \\(N = tn\\)), and \\(\\mathbf{X}_\\tau\\) is the \\(N \\times (t-1)\\) matrix representing the treatment effects with an appropriate set of contrasts.\nFor a balanced design the structure of \\(\\mathbf{X}_\\tau\\) ensures that\n\\[\n\\mathbf{X}_\\tau^\\top \\mathbf{X}_\\tau = k\\, \\mathbf{I}_{t-1},\n\\]\nwhere \\(k\\) is a constant proportional to the number of replications and \\(\\mathbf{I}_{t-1}\\) is the \\((t-1)\\times(t-1)\\) identity matrix. This result shows that the columns (i.e., the treatment contrasts) are orthogonal.\n\nDetailed Steps\n\nBalanced Replication:\nEach treatment appears \\(n\\) times so that the sum of the indicator for treatment \\(i\\) is \\(n\\). For any two different treatments \\(i\\) and \\(j\\):\n\n\\[\n\\sum_{l=1}^{N} x_{l,i} \\, x_{l,j} = 0,\n\\]\nwhen the columns are centered (i.e., after imposing the constraint \\(\\sum_{i=1}^{t} \\tau_i = 0\\)). This centering is key to orthogonality.\n\nContrast Construction:\nWhen forming contrasts, one typically chooses contrast coefficients \\(c_{i}\\) such that\n\n\\[\n\\sum_{i=1}^{t} c_{i} = 0.\n\\] For two contrasts \\(\\mathbf{c}^{(1)}\\) and \\(\\mathbf{c}^{(2)}\\), the balance of the design implies\n\\[\n\\sum_{i=1}^{t} c^{(1)}_i \\, c^{(2)}_i = 0.\n\\] This follows because, with equal replication, each contrast is scaled by \\(n\\) and the orthogonality condition is preserved.\n\nProjection Interpretation:\nThe projection matrix onto the column space of \\(\\mathbf{X}_\\tau\\) is given by\n\n\\[\n\\mathbf{P} = \\mathbf{X}_\\tau \\left(\\mathbf{X}_\\tau^\\top \\mathbf{X}_\\tau\\right)^{-1}\\mathbf{X}_\\tau^\\top.\n\\]\nWhen \\(\\mathbf{X}_\\tau^\\top \\mathbf{X}_\\tau\\) is diagonal (as in the balanced case), the off-diagonal elements of the projection matrix vanish, indicating that the treatment effects are estimated independently (i.e., orthogonally)."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#intuitive-explanation",
    "href": "lectures/week-06_higher-order.html#intuitive-explanation",
    "title": "Higher-Order Designs",
    "section": "Intuitive Explanation",
    "text": "Intuitive Explanation\nImagine you have a set of equally weighted arrows (representing treatment contrasts) in a space. In a balanced design, these arrows point in mutually independent directions—none of the arrows “shadow” any other. As a result, any movement (or change) in one direction does not influence the measurement in any other direction. This geometrical interpretation is exactly what orthogonality means and why it simplifies the ANOVA table: the treatment sum of squares does not “contaminate” the error sum of squares."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#summary-4",
    "href": "lectures/week-06_higher-order.html#summary-4",
    "title": "Higher-Order Designs",
    "section": "Summary",
    "text": "Summary\nThe orthogonality in balanced designs stems from:\n\nEqual replication: Each treatment is observed an equal number of times.\nProperly constructed contrasts: With the constraint \\(\\sum_{i=1}^{t} \\tau_i = 0\\), contrasts satisfy \\(\\sum c_i = 0\\) and, hence, their inner products are zero.\nDiagonal structure of \\(\\mathbf{X}_\\tau^\\top \\mathbf{X}_\\tau\\): This ensures that the estimated effects are uncorrelated.\n\nThis property is central in simplifying the statistical analysis, as it allows the decomposition of the total variability into independent components corresponding to treatments and error."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html",
    "href": "lectures/week-04_model-diagnostics.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "When using ANOVA or regression models, the reliability and validity of inferences depend on certain assumptions:\n\nIndependence of Errors: Observations should not be correlated.\nConstant Variance (Homoscedasticity): The variability of errors should remain constant across treatments or predicted values.\nNormality of Errors: Residuals should follow a normal distribution.\n\nIf these assumptions fail, confidence intervals and hypothesis tests may yield misleading conclusions. This lecture focuses on systematic ways to verify assumptions, diagnose problems, and apply corrective measures. By conducting proper diagnostic checks, we can maintain rigor and integrity in our statistical analyses.\n\n\n\nUnderstand the importance of verifying model assumptions in ANOVA and regression.\nLearn diagnostic tools (residual plots, normal probability plots) to detect assumption violations.\nExplore transformations and alternative modeling strategies to address violations.\nDevelop proficiency in R for performing diagnostics and interpreting results.\nChallenge understanding through exercises involving medium to complex scenarios and proofs.\n\n\n\n\nTo implement MCPs and contrasts in R, we will use the following packages:\n\npacman::p_load(dplyr, ggplot2, here, readr, flextable, PMCMRplus)\n\n\ndplyr: Data manipulation and summarization.\nggplot2: Data visualization.\nlsmeans and emmeans: Post hoc comparisons and multiple comparisons.\npacman: Package management.\nhere: File path management.\nreadr: Reading data files.\nflextable: Creating tables.\nPMCMRplus: Pairwise multiple comparisons.\n\n\n\n\nAnalogy: Imagine you have a pair of binoculars (your model). The assumptions are like cleaning the lenses before use. If the lenses are dirty (violated assumptions), you see distorted images (biased conclusions). Checking assumptions and fixing issues (cleaning lenses) ensures a clear view of the underlying relationships.\nSimple Example: Suppose you test four treatments for battery life. After ANOVA suggests differences, you examine residuals. If residuals fan out like a megaphone, variance is not constant. Just as uneven ground makes walking difficult, uneven variance complicates inference. You may need transformations to “level the playing field.”\n\n\n\n\nVisual Inspection of Residuals: Residuals, defined as \\(e_{ij} = Y_{ij} - \\hat{Y}_{ij}\\), reflect unexplained variation. By plotting residuals against fitted values or explanatory variables, patterns emerge.\nStandardized Residuals: Standardized residuals \\(z_{ij} = \\frac{e_{ij}}{\\sqrt{\\text{MSE}}}\\) help detect outliers and unusual patterns because they are put on a common scale.\nNormality Checks: Normal probability plots (QQ-plots) show if residuals deviate from a straight line, indicating potential non-normality.\nIndependence Checks: Residuals plotted in the order of data collection can reveal time trends or autocorrelation, violating independence.\nVariance Homogeneity Checks: Residuals vs. fitted value plots or group-specific standard deviations highlight heteroscedasticity.\n\nBelow is a step-by-step tutorial on standardized residuals in both ANOVA and Regression contexts. We will cover:\n\nThe difference between raw residuals and standardized residuals.\nVarious formulas/definitions of standardized residuals.\nUsage in both ANOVA and regression models.\nThe relationship of these definitions to the rstandard() function in R.\n\n\n\n\n\n\n\nIn a linear model—be it for ANOVA or regression—the raw residual \\(e_i\\) for the \\(i\\)th observation is:\n\n\\[\ne_i = y_i - \\hat{y}_i,\n\\]\nwhere:\n\n\\(y_i\\) is the observed value,\n\\(\\hat{y}_i\\) is the fitted (predicted) value from the model.\n\n\n\n\n\nRaw residuals are on the same scale as the response variable, so comparing residuals across data points or across different models can be less meaningful if the variance of the error changes or the scale of the response variable changes.\nStandardized residuals (or scaled residuals) transform the raw residuals so that they take into account the model’s standard error of the residual for each point. This makes it easier to detect outliers or high-leverage points, because you put them on a comparable scale (akin to a z-score).\n\n\n\n\n\nThe exact formula for a “standardized” residual can vary across textbooks and software. The major variants you will see are:\n\nSimple Standardized Residual (often used in introductory ANOVA contexts):\n\n\\[\nr_i = \\frac{e_i}{\\sqrt{\\text{MSE}}},\n\\]\nwhere MSE is the mean squared error (also called \\(s^2\\)) from the overall model.\n\nThis approach uses the same denominator (i.e., \\(\\sqrt{MSE}\\)) for every residual.\n\n\nInternally Studentized Residual (sometimes also called the “standardized residual” in many software packages, including base R’s rstandard() for lm objects):\n\n\\[\nr_i^\\text{(int)} = \\frac{e_i}{\\sqrt{s^2(1 - h_{ii})}},\n\\]\nwhere:\n\n\\(s^2\\) is the residual (error) variance estimate from the full model,\n\\(h_{ii}\\) is the hat value (or leverage) for observation \\(i\\). (In matrix terms, \\(\\mathbf{H} = X (X^\\top X)^{-1} X^\\top\\) is the “hat matrix,” and \\(h_{ii}\\) is its \\(i\\)th diagonal element.)\n\\(1 - h_{ii}\\) is a scale adjustment that accounts for leverage.\n\n\nExternally Studentized Residual (also called the “deleted residual” or “R-student”):\n\n\\[\nr_i^\\text{(ext)} = \\frac{e_i}{\\sqrt{s_{(-i)}^2 (1 - h_{ii})}},\n\\]\nwhere:\n\n\\(s_{(-i)}^2\\) is the mean squared error calculated by fitting the model without the \\(i\\)th observation (i.e., a leave-one-out MSE).\nThis further adjusts the estimate of residual variance by removing the \\(i\\)th point from the fit, which can detect outliers more sensitively.\n\nDepending on the method or software, “standardized residuals” could refer to either the simple version or the internally studentized version. In most modern regression/ANOVA software:\n\nrstandard() typically provides the internally studentized residuals.\nrstudent() typically provides the externally studentized residuals.\n\n\n\n\nANOVA (Analysis of Variance) is essentially a special case of linear regression with categorical predictors. The concept of residuals is exactly the same: - In a one-way ANOVA, each group has a predicted mean \\(\\hat{y}_i\\). The raw residual is \\(y_i - \\hat{y}_i\\). - If you want to standardize by the overall MSE, you could compute\n\\[\nr_i = \\frac{y_i - \\hat{y}_i}{\\sqrt{MSE}}\n\\]\nor the more general (internally studentized) version:\n\\[\nr_i^\\text{(int)} = \\frac{y_i - \\hat{y}_i}{\\sqrt{MSE \\cdot (1 - h_{ii})}}.\n\\]\nIn an ANOVA model, \\(h_{ii}\\) is often simpler to compute because design matrices are typically 0/1-coded for each group, so each point’s leverage depends on how many replicates are in each group, among other factors.\nRegression (with continuous predictors) uses exactly the same approach: the raw residual is \\(e_i = y_i - \\hat{y}_i\\). - In regression, the hat value \\(h_{ii}\\) depends on the design matrix \\(X\\) (i.e., the set of predictors, their transformations, interactions, etc.). - Standardized or studentized residuals are used to: - Check for outliers (extreme residuals). - Assess model fit in residual vs. fitted-value plots. - Evaluate if residuals follow typical assumptions (normality, equal variance).\n\n\n\n\n\n\nIf you fit a linear model in R using:\n\nfit &lt;- lm(y ~ x1 + x2, data = mydata)\nThen you can get standardized residuals via:\nstd_res &lt;- rstandard(fit)\n\nBy default, rstandard() returns the internally studentized residuals. Mathematically (for observation \\(i\\)):\n\n\\[\n\\text{rstandard}(i) = \\frac{e_i}{\\sqrt{s^2 (1 - h_{ii})}},\n\\]\nwhere \\(s^2\\) is the estimated MSE from the full model, and \\(h_{ii}\\) is the leverage for observation \\(i\\).\n\n\n\n\nrstudent(fit) returns the externally studentized residuals (sometimes called the R-student residuals):\n\n\\[\n\\text{rstudent}(i) = \\frac{e_i}{\\sqrt{s_{(-i)}^2 (1 - h_{ii})}},\n\\]\n\nresiduals(fit) or simply fit$residuals returns the raw (unstandardized) residuals:\n\n\\[\ne_i = y_i - \\hat{y}_i.\n\\]\n\n\n\n\nHere’s a quick demonstration using a built-in dataset in R.\n\n\n\n# Load the built-in dataset\ndata(mtcars)\n# Fit a linear model (mpg ~ wt + hp)\nfit &lt;- lm(mpg ~ wt + hp, data = mtcars)\n# Raw residuals\nraw_res &lt;- residuals(fit)\n# Internally studentized (standardized) residuals\nstd_res &lt;- rstandard(fit)\n# Externally studentized residuals\nstud_res &lt;- rstudent(fit)\n# Compare\nhead(data.frame(raw_res, std_res, stud_res))\n\n\n\n\n\n\n\n\n\nraw_res\nstd_res\nstud_res\n\n\n\n\nMazda RX4\n-2.5723294\n-1.0145865\n-1.0151193\n\n\nMazda RX4 Wag\n-1.5834826\n-0.6233275\n-0.6166309\n\n\nDatsun 710\n-2.4758187\n-0.9847588\n-0.9842273\n\n\nHornet 4 Drive\n0.1349799\n0.0533285\n0.0524035\n\n\nHornet Sportabout\n0.3727334\n0.1464478\n0.1439539\n\n\nValiant\n-2.3738163\n-0.9476980\n-0.9459788\n\n\n\n\n\n\n\nTable 1: Standardized Residuals Example\n\n\n\n\nYou can see how the raw residuals are just differences from fitted values, while the standardized and studentized ones have been scaled differently.\n\n\n\n\n\n\n# A simple one-way ANOVA example\ndf &lt;- data.frame(\n    group = rep(c(\"A\", \"B\", \"C\"), each = 10),\n    y = c(\n        rnorm(10, mean = 5, sd = 1),\n        rnorm(10, mean = 7, sd = 1),\n        rnorm(10, mean = 6, sd = 1)\n    )\n)\n\nfit_anova &lt;- lm(y ~ group, data = df) # identical to aov(y ~ group, data = df)\nstd_res_anova &lt;- rstandard(fit_anova)\n\nhead(std_res_anova)\n\n         1          2          3          4          5          6 \n-1.8688760  1.1705936  0.7670691  0.6010791  0.9606203  0.2034584 \n\n\n\nTable 2: ANOVA Standardized Residuals Example\n\n\n\nThis again yields internally studentized residuals. The same formulas from regression hold for an ANOVA setting because ANOVA is just a linear model with categorical predictors.\n\n\n\n\n\nThresholds for potential outliers:\n\nA standardized (or studentized) residual that exceeds about \\(\\pm 2\\) could be noteworthy, and \\(\\pm 3\\) is often considered large in typical Normal-error contexts.\nHowever, the interpretation depends on your sample size and the distribution of residuals.\n\nLeverage and influence:\n\nIf you see a large standardized residual, check also the point’s leverage \\((h_{ii})\\). A high-leverage point with a large standardized residual can strongly affect the regression line or ANOVA estimates.\n\nDiagnostic Plots:\n\nPlotting standardized residuals vs. fitted values is a common practice to detect patterns in the residuals. If the model is well-specified, these points should look like random scatter around 0 with no obvious pattern and relatively consistent variance.\n\n\n\n\n\n\nRaw residuals tell you the difference between observed and fitted values on the scale of the original data.\nStandardized (or Studentized) residuals provide a way to compare residuals on a common scale that accounts for the overall error variance and, importantly, each point’s leverage.\nR’s rstandard() function by default computes internally studentized residuals, which is often what people mean by “standardized residuals” in practice.\nFor more robust outlier detection, you can use rstudent() (externally studentized residuals), which removes the \\(i\\)th point when estimating its variance.\n\n\n\nWhen someone says “standardized residual,” check which version they are referring to. In modern usage (especially in R), “standardized residual” commonly refers to the internally studentized residual \\(\\frac{e_i}{\\sqrt{s^2 (1-h_{ii})}}\\), as provided by rstandard(). For more sensitive outlier detection, consider the externally studentized residuals \\(\\frac{e_i}{\\sqrt{s_{(-i)}^2 (1-h_{ii})}}\\) from rstudent(). Both are useful for checking model assumptions and identifying influential points."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#objectives",
    "href": "lectures/week-04_model-diagnostics.html#objectives",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Understand the importance of verifying model assumptions in ANOVA and regression.\nLearn diagnostic tools (residual plots, normal probability plots) to detect assumption violations.\nExplore transformations and alternative modeling strategies to address violations.\nDevelop proficiency in R for performing diagnostics and interpreting results.\nChallenge understanding through exercises involving medium to complex scenarios and proofs."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#r-packages",
    "href": "lectures/week-04_model-diagnostics.html#r-packages",
    "title": "Model Diagnostics",
    "section": "",
    "text": "To implement MCPs and contrasts in R, we will use the following packages:\n\npacman::p_load(dplyr, ggplot2, here, readr, flextable, PMCMRplus)\n\n\ndplyr: Data manipulation and summarization.\nggplot2: Data visualization.\nlsmeans and emmeans: Post hoc comparisons and multiple comparisons.\npacman: Package management.\nhere: File path management.\nreadr: Reading data files.\nflextable: Creating tables.\nPMCMRplus: Pairwise multiple comparisons."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#motivating-example-and-intuition",
    "href": "lectures/week-04_model-diagnostics.html#motivating-example-and-intuition",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Analogy: Imagine you have a pair of binoculars (your model). The assumptions are like cleaning the lenses before use. If the lenses are dirty (violated assumptions), you see distorted images (biased conclusions). Checking assumptions and fixing issues (cleaning lenses) ensures a clear view of the underlying relationships.\nSimple Example: Suppose you test four treatments for battery life. After ANOVA suggests differences, you examine residuals. If residuals fan out like a megaphone, variance is not constant. Just as uneven ground makes walking difficult, uneven variance complicates inference. You may need transformations to “level the playing field.”"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#strategies-for-checking-assumptions",
    "href": "lectures/week-04_model-diagnostics.html#strategies-for-checking-assumptions",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Visual Inspection of Residuals: Residuals, defined as \\(e_{ij} = Y_{ij} - \\hat{Y}_{ij}\\), reflect unexplained variation. By plotting residuals against fitted values or explanatory variables, patterns emerge.\nStandardized Residuals: Standardized residuals \\(z_{ij} = \\frac{e_{ij}}{\\sqrt{\\text{MSE}}}\\) help detect outliers and unusual patterns because they are put on a common scale.\nNormality Checks: Normal probability plots (QQ-plots) show if residuals deviate from a straight line, indicating potential non-normality.\nIndependence Checks: Residuals plotted in the order of data collection can reveal time trends or autocorrelation, violating independence.\nVariance Homogeneity Checks: Residuals vs. fitted value plots or group-specific standard deviations highlight heteroscedasticity.\n\nBelow is a step-by-step tutorial on standardized residuals in both ANOVA and Regression contexts. We will cover:\n\nThe difference between raw residuals and standardized residuals.\nVarious formulas/definitions of standardized residuals.\nUsage in both ANOVA and regression models.\nThe relationship of these definitions to the rstandard() function in R."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#raw-residuals-vs.-standardized-residuals",
    "href": "lectures/week-04_model-diagnostics.html#raw-residuals-vs.-standardized-residuals",
    "title": "Model Diagnostics",
    "section": "",
    "text": "In a linear model—be it for ANOVA or regression—the raw residual \\(e_i\\) for the \\(i\\)th observation is:\n\n\\[\ne_i = y_i - \\hat{y}_i,\n\\]\nwhere:\n\n\\(y_i\\) is the observed value,\n\\(\\hat{y}_i\\) is the fitted (predicted) value from the model.\n\n\n\n\n\nRaw residuals are on the same scale as the response variable, so comparing residuals across data points or across different models can be less meaningful if the variance of the error changes or the scale of the response variable changes.\nStandardized residuals (or scaled residuals) transform the raw residuals so that they take into account the model’s standard error of the residual for each point. This makes it easier to detect outliers or high-leverage points, because you put them on a comparable scale (akin to a z-score)."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#formulasdefinitions-of-standardized-residuals",
    "href": "lectures/week-04_model-diagnostics.html#formulasdefinitions-of-standardized-residuals",
    "title": "Model Diagnostics",
    "section": "",
    "text": "The exact formula for a “standardized” residual can vary across textbooks and software. The major variants you will see are:\n\nSimple Standardized Residual (often used in introductory ANOVA contexts):\n\n\\[\nr_i = \\frac{e_i}{\\sqrt{\\text{MSE}}},\n\\]\nwhere MSE is the mean squared error (also called \\(s^2\\)) from the overall model.\n\nThis approach uses the same denominator (i.e., \\(\\sqrt{MSE}\\)) for every residual.\n\n\nInternally Studentized Residual (sometimes also called the “standardized residual” in many software packages, including base R’s rstandard() for lm objects):\n\n\\[\nr_i^\\text{(int)} = \\frac{e_i}{\\sqrt{s^2(1 - h_{ii})}},\n\\]\nwhere:\n\n\\(s^2\\) is the residual (error) variance estimate from the full model,\n\\(h_{ii}\\) is the hat value (or leverage) for observation \\(i\\). (In matrix terms, \\(\\mathbf{H} = X (X^\\top X)^{-1} X^\\top\\) is the “hat matrix,” and \\(h_{ii}\\) is its \\(i\\)th diagonal element.)\n\\(1 - h_{ii}\\) is a scale adjustment that accounts for leverage.\n\n\nExternally Studentized Residual (also called the “deleted residual” or “R-student”):\n\n\\[\nr_i^\\text{(ext)} = \\frac{e_i}{\\sqrt{s_{(-i)}^2 (1 - h_{ii})}},\n\\]\nwhere:\n\n\\(s_{(-i)}^2\\) is the mean squared error calculated by fitting the model without the \\(i\\)th observation (i.e., a leave-one-out MSE).\nThis further adjusts the estimate of residual variance by removing the \\(i\\)th point from the fit, which can detect outliers more sensitively.\n\nDepending on the method or software, “standardized residuals” could refer to either the simple version or the internally studentized version. In most modern regression/ANOVA software:\n\nrstandard() typically provides the internally studentized residuals.\nrstudent() typically provides the externally studentized residuals."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#usage-in-anova-vs.-regression",
    "href": "lectures/week-04_model-diagnostics.html#usage-in-anova-vs.-regression",
    "title": "Model Diagnostics",
    "section": "",
    "text": "ANOVA (Analysis of Variance) is essentially a special case of linear regression with categorical predictors. The concept of residuals is exactly the same: - In a one-way ANOVA, each group has a predicted mean \\(\\hat{y}_i\\). The raw residual is \\(y_i - \\hat{y}_i\\). - If you want to standardize by the overall MSE, you could compute\n\\[\nr_i = \\frac{y_i - \\hat{y}_i}{\\sqrt{MSE}}\n\\]\nor the more general (internally studentized) version:\n\\[\nr_i^\\text{(int)} = \\frac{y_i - \\hat{y}_i}{\\sqrt{MSE \\cdot (1 - h_{ii})}}.\n\\]\nIn an ANOVA model, \\(h_{ii}\\) is often simpler to compute because design matrices are typically 0/1-coded for each group, so each point’s leverage depends on how many replicates are in each group, among other factors.\nRegression (with continuous predictors) uses exactly the same approach: the raw residual is \\(e_i = y_i - \\hat{y}_i\\). - In regression, the hat value \\(h_{ii}\\) depends on the design matrix \\(X\\) (i.e., the set of predictors, their transformations, interactions, etc.). - Standardized or studentized residuals are used to: - Check for outliers (extreme residuals). - Assess model fit in residual vs. fitted-value plots. - Evaluate if residuals follow typical assumptions (normality, equal variance)."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#relationship-to-rstandard-in-r",
    "href": "lectures/week-04_model-diagnostics.html#relationship-to-rstandard-in-r",
    "title": "Model Diagnostics",
    "section": "",
    "text": "If you fit a linear model in R using:\n\nfit &lt;- lm(y ~ x1 + x2, data = mydata)\nThen you can get standardized residuals via:\nstd_res &lt;- rstandard(fit)\n\nBy default, rstandard() returns the internally studentized residuals. Mathematically (for observation \\(i\\)):\n\n\\[\n\\text{rstandard}(i) = \\frac{e_i}{\\sqrt{s^2 (1 - h_{ii})}},\n\\]\nwhere \\(s^2\\) is the estimated MSE from the full model, and \\(h_{ii}\\) is the leverage for observation \\(i\\).\n\n\n\n\nrstudent(fit) returns the externally studentized residuals (sometimes called the R-student residuals):\n\n\\[\n\\text{rstudent}(i) = \\frac{e_i}{\\sqrt{s_{(-i)}^2 (1 - h_{ii})}},\n\\]\n\nresiduals(fit) or simply fit$residuals returns the raw (unstandardized) residuals:\n\n\\[\ne_i = y_i - \\hat{y}_i.\n\\]"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#example-in-r",
    "href": "lectures/week-04_model-diagnostics.html#example-in-r",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Here’s a quick demonstration using a built-in dataset in R.\n\n\n\n# Load the built-in dataset\ndata(mtcars)\n# Fit a linear model (mpg ~ wt + hp)\nfit &lt;- lm(mpg ~ wt + hp, data = mtcars)\n# Raw residuals\nraw_res &lt;- residuals(fit)\n# Internally studentized (standardized) residuals\nstd_res &lt;- rstandard(fit)\n# Externally studentized residuals\nstud_res &lt;- rstudent(fit)\n# Compare\nhead(data.frame(raw_res, std_res, stud_res))\n\n\n\n\n\n\n\n\n\nraw_res\nstd_res\nstud_res\n\n\n\n\nMazda RX4\n-2.5723294\n-1.0145865\n-1.0151193\n\n\nMazda RX4 Wag\n-1.5834826\n-0.6233275\n-0.6166309\n\n\nDatsun 710\n-2.4758187\n-0.9847588\n-0.9842273\n\n\nHornet 4 Drive\n0.1349799\n0.0533285\n0.0524035\n\n\nHornet Sportabout\n0.3727334\n0.1464478\n0.1439539\n\n\nValiant\n-2.3738163\n-0.9476980\n-0.9459788\n\n\n\n\n\n\n\nTable 1: Standardized Residuals Example\n\n\n\n\nYou can see how the raw residuals are just differences from fitted values, while the standardized and studentized ones have been scaled differently.\n\n\n\n\n\n\n# A simple one-way ANOVA example\ndf &lt;- data.frame(\n    group = rep(c(\"A\", \"B\", \"C\"), each = 10),\n    y = c(\n        rnorm(10, mean = 5, sd = 1),\n        rnorm(10, mean = 7, sd = 1),\n        rnorm(10, mean = 6, sd = 1)\n    )\n)\n\nfit_anova &lt;- lm(y ~ group, data = df) # identical to aov(y ~ group, data = df)\nstd_res_anova &lt;- rstandard(fit_anova)\n\nhead(std_res_anova)\n\n         1          2          3          4          5          6 \n-1.8688760  1.1705936  0.7670691  0.6010791  0.9606203  0.2034584 \n\n\n\nTable 2: ANOVA Standardized Residuals Example\n\n\n\nThis again yields internally studentized residuals. The same formulas from regression hold for an ANOVA setting because ANOVA is just a linear model with categorical predictors."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#interpretation-and-best-practices",
    "href": "lectures/week-04_model-diagnostics.html#interpretation-and-best-practices",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Thresholds for potential outliers:\n\nA standardized (or studentized) residual that exceeds about \\(\\pm 2\\) could be noteworthy, and \\(\\pm 3\\) is often considered large in typical Normal-error contexts.\nHowever, the interpretation depends on your sample size and the distribution of residuals.\n\nLeverage and influence:\n\nIf you see a large standardized residual, check also the point’s leverage \\((h_{ii})\\). A high-leverage point with a large standardized residual can strongly affect the regression line or ANOVA estimates.\n\nDiagnostic Plots:\n\nPlotting standardized residuals vs. fitted values is a common practice to detect patterns in the residuals. If the model is well-specified, these points should look like random scatter around 0 with no obvious pattern and relatively consistent variance."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#summary",
    "href": "lectures/week-04_model-diagnostics.html#summary",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Raw residuals tell you the difference between observed and fitted values on the scale of the original data.\nStandardized (or Studentized) residuals provide a way to compare residuals on a common scale that accounts for the overall error variance and, importantly, each point’s leverage.\nR’s rstandard() function by default computes internally studentized residuals, which is often what people mean by “standardized residuals” in practice.\nFor more robust outlier detection, you can use rstudent() (externally studentized residuals), which removes the \\(i\\)th point when estimating its variance.\n\n\n\nWhen someone says “standardized residual,” check which version they are referring to. In modern usage (especially in R), “standardized residual” commonly refers to the internally studentized residual \\(\\frac{e_i}{\\sqrt{s^2 (1-h_{ii})}}\\), as provided by rstandard(). For more sensitive outlier detection, consider the externally studentized residuals \\(\\frac{e_i}{\\sqrt{s_{(-i)}^2 (1-h_{ii})}}\\) from rstudent(). Both are useful for checking model assumptions and identifying influential points."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#checking-independence",
    "href": "lectures/week-04_model-diagnostics.html#checking-independence",
    "title": "Model Diagnostics",
    "section": "Checking Independence",
    "text": "Checking Independence\nContext: Consider a balloon inflation experiment where each time a subject inflates a balloon, they learn and get faster. Residuals vs. observation order might show a trend, indicating non-independence (learning effect).\nR Implementation:\n\ndataset &lt;- readr::read_table(here(\"data\", \"dean2017\", \"balloon.txt\"))\ndataset |&gt; head()\n\n\n\n\n\n\nOrder\nColor\nTime\n\n\n\n\n1\n1\n22.0\n\n\n2\n3\n24.6\n\n\n3\n1\n20.3\n\n\n4\n4\n19.8\n\n\n5\n3\n24.3\n\n\n6\n2\n22.2\n\n\n\nBalloon Inflation Data\n\n\n\nmodel &lt;- lm(Time ~ Order, data = dataset)\nsummary(model)\n\n\nCall:\nlm(formula = Time ~ Order, data = dataset)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.086 -2.252 -1.155  2.252  7.399 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.71613    1.16260  20.399  &lt; 2e-16 ***\nOrder       -0.21045    0.06149  -3.423  0.00181 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.212 on 30 degrees of freedom\nMultiple R-squared:  0.2808,    Adjusted R-squared:  0.2568 \nF-statistic: 11.71 on 1 and 30 DF,  p-value: 0.001813"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#checking-independence-1",
    "href": "lectures/week-04_model-diagnostics.html#checking-independence-1",
    "title": "Model Diagnostics",
    "section": "Checking Independence",
    "text": "Checking Independence\nVisual Clue: A “megaphone” shape in residuals vs. time suggests non-independence.\n\nplot(resid(model) ~ Time, data = dataset)\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\nFigure 1: Residuals vs. Time Plot\n\n\n\n\n\nIf non-independence arises, consider mixed-effects models or time-series methods."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#checking-equal-variance-homoscedasticity",
    "href": "lectures/week-04_model-diagnostics.html#checking-equal-variance-homoscedasticity",
    "title": "Model Diagnostics",
    "section": "Checking Equal Variance (Homoscedasticity)",
    "text": "Checking Equal Variance (Homoscedasticity)\nVisual Clue: A “megaphone” shape in residuals vs. fitted values suggests variance grows with the mean. Rule of Thumb: If ratio of largest to smallest sample variance &gt; 3, consider a transformation.\nR Implementation:\n\nplot(fitted(model), resid(model))\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\nFigure 2: Residuals vs. Fitted Values Plot\n\n\n\n\n\nIf variance is not constant, transformations (like log or square-root) can stabilize it. The Box-Cox method in R helps find a suitable lambda for transforming responses."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#checking-normality",
    "href": "lectures/week-04_model-diagnostics.html#checking-normality",
    "title": "Model Diagnostics",
    "section": "Checking Normality",
    "text": "Checking Normality\nQQ-Plot: If residuals follow a normal distribution, they align closely with the QQ-line.\nR Implementation:\n\nqqnorm(resid(model))\nqqline(resid(model), col = \"blue\")\n\n\n\n\n\n\n\nFigure 3: Normal Probability Plot\n\n\n\n\n\nIf deviations are severe, consider transformations or nonparametric methods."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#remedies-for-violations",
    "href": "lectures/week-04_model-diagnostics.html#remedies-for-violations",
    "title": "Model Diagnostics",
    "section": "Remedies for Violations",
    "text": "Remedies for Violations\n\nIndependence Violation: Use mixed-effects models or time-series methods to account for correlation.\nHeteroscedasticity:\n\nTransformations: Log, square-root, arcsine, or Box-Cox to stabilize variance.\nSatterthwaite’s Approximation: For unequal variances in ANOVA.\nWeighted Least Squares: Assign weights inversely proportional to variance– not discussed here.\nRobust Methods: Huber-White or bootstrapping to correct standard errors. We only discuss the former.\nGeneralized Least Squares (GLS): Model variance structure explicitly - not covered here."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#transformation",
    "href": "lectures/week-04_model-diagnostics.html#transformation",
    "title": "Model Diagnostics",
    "section": "Transformation",
    "text": "Transformation\nTransformations to stabilize variance in ANOVA are most effective when there is a systematic relationship between group means and variances (e.g., variance increases with the mean). If there is no apparent relationship between the mean and variance, a standard variance-stabilizing transformation (such as a log, square-root, or arcsine transform) often does not help and may even complicate interpretation. In such scenarios, other methods designed for heterogeneous variances (e.g., Welch’s ANOVA, generalized linear models, or nonparametric tests) may be more appropriate."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#why-transform-data",
    "href": "lectures/week-04_model-diagnostics.html#why-transform-data",
    "title": "Model Diagnostics",
    "section": "Why Transform Data?",
    "text": "Why Transform Data?\n\nHomoscedasticity (equal variance assumption)\nClassic (fixed-effects) one-way ANOVA assumes that the error variance is constant across groups (homoscedasticity). Violating this assumption can inflate Type I or Type II error rates.\nVariance-Stabilizing Transformations\n\nCommon transformations include:\n\nLogarithmic: used when the standard deviation is roughly proportional to the mean.\nSquare root: used for count data, especially when counts are low.\nArcsine: commonly used for proportional data with values between 0 and 1.\n\nThese transformations are meant to make the variance more constant across groups if the variance depends on the mean in a predictable way.\n\nRole of Relationship Between Mean and Variance\n\nWhen group means and variances show a positive relationship (e.g., bigger means have bigger variances in proportionate fashion), a variance-stabilizing transformation often helps.\n\nIf the variance is unrelated to the mean, applying a traditional “mean-based” transformation (like a log) may have little or no effect on stabilizing the variance."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#when-the-transformation-works",
    "href": "lectures/week-04_model-diagnostics.html#when-the-transformation-works",
    "title": "Model Diagnostics",
    "section": "When the Transformation Works",
    "text": "When the Transformation Works\n\nDetecting a relationship\nPlot the group standard deviations (or group variances) against group means. If you see a pattern such as an increasing linear or non-linear trend, you can often guess an appropriate transformation (log, sqrt, etc.).\nSelecting a transformation\n\nLog transformation: Common if \\(\\text{variance} \\propto (\\text{mean})^\\alpha\\).\n\nBox-Cox approach: A systematic way to find \\(\\lambda\\) in a power transformation \\(y^\\lambda\\).\n\nArcsine: For data restricted between 0 and 1 (e.g., proportions).\n\nChecking success\nAfter transforming, re-check the group variances (e.g., with a residual plot or a test for homogeneity). If the transformation was effective, the variances should appear more uniform across groups."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#does-the-transformation-work-when-there-is-no-meanvariance-relationship",
    "href": "lectures/week-04_model-diagnostics.html#does-the-transformation-work-when-there-is-no-meanvariance-relationship",
    "title": "Model Diagnostics",
    "section": "Does the Transformation Work When There is No Mean–Variance Relationship?",
    "text": "Does the Transformation Work When There is No Mean–Variance Relationship?\n\nLikely not. If the group variances do not systematically depend on the group means, a standard transformation aimed at stabilizing “mean-related” variance typically will not help.\n\nIn some cases, a transformation could still help if the original data are very skewed or if there are other distributional issues. However, if variance is truly unrelated to the mean, transformations specifically intended to fix mean-variance dependence will not systematically improve homogeneity of variance.\n\nAlternatives: Instead of forcing a transformation, consider:\n\nWelch’s ANOVA (unequal variances ANOVA), which does not assume equal variances across groups.\n\nGeneralized Linear Models (GLMs), if the data structure suggests a particular distribution (e.g., Poisson for counts, binomial for proportions).\n\nNonparametric tests (e.g., Kruskal–Wallis test) when distributional assumptions are severely violated.\n\n\n\nKey Takeaways\n\nTransformation “success” depends on whether the data’s variance problem stems from an identifiable mean–variance relationship.\n\nNo relationship → transformations designed for mean-related variance may not help.\n\nConsider alternative methods (Welch’s ANOVA, GLMs, or nonparametric) when classical ANOVA’s equal variance assumption is clearly violated and transformations prove ineffective."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#transformations",
    "href": "lectures/week-04_model-diagnostics.html#transformations",
    "title": "Model Diagnostics",
    "section": "Transformations",
    "text": "Transformations\nBelow is a detailed example of using the Box-Cox transformation in R. The Box-Cox transformation is a family of power transformations designed to stabilize the variance and make the data more normally distributed. This example will simulate a dataset with heteroscedasticity, fit a linear model, use the Box-Cox procedure to identify an optimal transformation, and then re-fit the model on the transformed response."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#background-on-box-cox-transformation",
    "href": "lectures/week-04_model-diagnostics.html#background-on-box-cox-transformation",
    "title": "Model Diagnostics",
    "section": "Background on Box-Cox Transformation",
    "text": "Background on Box-Cox Transformation\nThe Box-Cox transformation is defined as:\n\\[\nY^{(\\lambda)} =\n\\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0, \\\\\n\\ln(Y), & \\lambda = 0.\n\\end{cases}\n\\]\n\nPurpose\n\nStabilize Variance: When the variance of the response variable changes (i.e., heteroscedasticity is present).\nImprove Normality: To make the distribution of the response more closely approximate a normal distribution."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#simulate-a-dataset-with-heteroscedasticity",
    "href": "lectures/week-04_model-diagnostics.html#simulate-a-dataset-with-heteroscedasticity",
    "title": "Model Diagnostics",
    "section": "Simulate a Dataset with Heteroscedasticity",
    "text": "Simulate a Dataset with Heteroscedasticity\nIn this example, we simulate data where the variance of the errors increases with the predictor \\(X\\).\n\nset.seed(123)\nn &lt;- 100\nX &lt;- runif(n, 1, 10)\n# Create Heteroscedastic Errors: Error variance Increases with X\nerrors &lt;- rnorm(n, mean = 0, sd = 0.5 * X)\nY &lt;- 2 + 3 * X + errors # Linear relationship with increasing variance\ndata_sim &lt;- data.frame(X = X, Y = Y)\ndata_sim |&gt; head()\n\n\n\n\n\n\nX\nY\n\n\n\n\n3.588198\n13.219072\n\n\n8.094746\n26.168699\n\n\n4.680792\n15.942043\n\n\n8.947157\n34.964019\n\n\n9.464206\n29.324245\n\n\n1.410008\n7.299144\n\n\n\nSimulated Data with Heteroscedasticity\n\n\n\n# Visualize the Raw Data\nggplot(data_sim, aes(x = X, y = Y)) +\n    geom_point() +\n    labs(\n        title = \"Simulated Data with Heteroscedasticity\",\n        x = \"Predictor (X)\",\n        y = \"Response (Y)\"\n    )\n\n\n\n\n\n\n\nFigure 4: Simulated Data with Heteroscedasticity\n\n\n\n\n\nExplanation:\nThe above code creates 100 observations where the error standard deviation increases with \\(X\\). This pattern may lead to heteroscedasticity in a linear regression model."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#fit-an-initial-linear-model",
    "href": "lectures/week-04_model-diagnostics.html#fit-an-initial-linear-model",
    "title": "Model Diagnostics",
    "section": "Fit an Initial Linear Model",
    "text": "Fit an Initial Linear Model\nWe first fit a simple linear regression model to the simulated data and then examine its residuals.\n\nmodel_initial &lt;- lm(Y ~ X, data = data_sim)\nsummary(model_initial)\n\n\nCall:\nlm(formula = Y ~ X, data = data_sim)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1305 -1.6273  0.0422  1.2916 10.1121 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.9557     0.6977   2.803   0.0061 ** \nX             2.9753     0.1153  25.805   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.943 on 98 degrees of freedom\nMultiple R-squared:  0.8717,    Adjusted R-squared:  0.8704 \nF-statistic: 665.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\nReview the model summary and diagnostic plots to check for heteroscedasticity. A residual vs. fitted plot (see next code) may reveal a “fanning out” pattern.\n\npar(mfrow = c(1, 2))\nplot(model_initial$fitted.values, residuals(model_initial),\n    xlab = \"Fitted Values\", ylab = \"Residuals\",\n    main = \"Residuals vs Fitted\"\n)\nabline(h = 0, col = \"red\")\nqqnorm(residuals(model_initial), main = \"QQ Plot\")\nqqline(residuals(model_initial), col = \"blue\")\n\n\n\n\n\n\n\nFigure 5: Diagnostic Plots for the Initial Model"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#apply-the-box-cox-transformation",
    "href": "lectures/week-04_model-diagnostics.html#apply-the-box-cox-transformation",
    "title": "Model Diagnostics",
    "section": "Apply the Box-Cox Transformation",
    "text": "Apply the Box-Cox Transformation\nWe use the boxcox() function from the MASS package to determine the optimal \\(\\lambda\\) value for the transformation.\n\nlibrary(MASS)\nboxcox_result &lt;- MASS::boxcox(model_initial, lambda = seq(-2, 2, 0.1))\n\n\n\n\n\n\n\n\nExplanation:\n\nThe boxcox() function examines a range of \\(\\lambda\\) values (here from -2 to 2) and calculates the log-likelihood for each transformation.\nThe optimal \\(\\lambda\\) is the one that maximizes the log-likelihood (the peak of the plot).\n\nObservation:\nAfter running the code, you should see a plot with \\(\\lambda\\) on the horizontal axis and the corresponding log-likelihood on the vertical axis. Suppose the plot suggests an optimal \\(\\lambda \\approx 0\\) (i.e., a log transformation) or a value different from 0."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#re-fit-the-model-with-the-transformed-response",
    "href": "lectures/week-04_model-diagnostics.html#re-fit-the-model-with-the-transformed-response",
    "title": "Model Diagnostics",
    "section": "Re-Fit the Model with the Transformed Response",
    "text": "Re-Fit the Model with the Transformed Response\nAssume the Box-Cox plot indicates that \\(\\lambda = 0\\) is optimal (a log transformation). We then transform \\(Y\\) and re-fit the model.\n\n\n\ndata_sim &lt;- data_sim |&gt; mutate(Y_trans = log(Y))\nmodel_transformed &lt;- lm(Y_trans ~ X, data = data_sim)\nsummary(model_transformed)\n\n\nCall:\nlm(formula = Y_trans ~ X, data = data_sim)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39876 -0.11500 -0.00162  0.13085  0.39509 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.803965   0.041251   43.73   &lt;2e-16 ***\nX           0.180587   0.006817   26.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.174 on 98 degrees of freedom\nMultiple R-squared:  0.8775,    Adjusted R-squared:  0.8762 \nF-statistic: 701.8 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\nTable 3: Transformed Linear Model"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#compare-diagnostic-plots-for-the-transformed-model",
    "href": "lectures/week-04_model-diagnostics.html#compare-diagnostic-plots-for-the-transformed-model",
    "title": "Model Diagnostics",
    "section": "Compare Diagnostic Plots for the Transformed Model",
    "text": "Compare Diagnostic Plots for the Transformed Model\nAfter refitting the model, we check whether the transformation improved the model assumptions.\n\npar(mfrow = c(1, 2))\nplot(model_transformed$fitted.values, residuals(model_transformed),\n    xlab = \"Fitted Values\", ylab = \"Residuals\",\n    main = \"Residuals vs Fitted (Transformed)\"\n)\nabline(h = 0, col = \"red\")\nqqnorm(residuals(model_transformed), main = \"QQ Plot (Transformed)\")\nqqline(residuals(model_transformed), col = \"blue\")\n\n\n\n\n\n\n\n\nInterpretation:\n- Residuals vs. Fitted Plot: Look for a more random scatter with no apparent pattern (indicating stabilized variance). - Q-Q Plot: Check whether the residuals are more normally distributed compared to the initial model."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#summary-of-findings",
    "href": "lectures/week-04_model-diagnostics.html#summary-of-findings",
    "title": "Model Diagnostics",
    "section": "Summary of Findings",
    "text": "Summary of Findings\n\nBefore Transformation:\nThe initial model showed signs of heteroscedasticity as the variance of the residuals increased with the fitted values.\nBox-Cox Analysis:\nThe Box-Cox procedure suggested an optimal \\(\\lambda\\) value (in our example, \\(\\lambda \\approx 0\\), which corresponds to a log transformation).\nAfter Transformation:\nThe re-fitted model using the log-transformed response showed improved diagnostic plots with more constant variance and residuals closer to normality."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#conclusion",
    "href": "lectures/week-04_model-diagnostics.html#conclusion",
    "title": "Model Diagnostics",
    "section": "Conclusion",
    "text": "Conclusion\nThe Box-Cox transformation is a powerful tool to address non-constant variance and non-normality in regression models. By applying the transformation, selecting the optimal \\(\\lambda\\) based on the maximum log-likelihood, and re-fitting the model, we can often achieve a model that better meets the assumptions required for valid inference.\nIn practice, it’s essential to interpret the Box-Cox plot, choose a transformation that makes sense for the data, and verify that the transformed model improves the diagnostic plots. When the assumptions of homoscedasticity and normality are met, the model’s estimates and inferences are more reliable."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#the-statistical-model",
    "href": "lectures/week-04_model-diagnostics.html#the-statistical-model",
    "title": "Model Diagnostics",
    "section": "The Statistical Model",
    "text": "The Statistical Model\nA one-way layout with \\(v\\) treatments can be written:\n\\[\nY_{it} \\;=\\; \\mu \\;+\\; \\tau_i \\;+\\; \\varepsilon_{it},\n\\quad\n\\varepsilon_{it} \\,\\sim\\, N(0, \\sigma_i^2),\n\\quad\n\\text{independently for each } i,t.\n\\]\n\n\\(i = 1, 2, \\dots, v\\) is the treatment index.\n\\(t = 1, 2, \\dots, r_i\\) indexes observations within the \\(i\\)-th treatment.\n\nWe do not assume \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_v^2\\). Instead, each group has its own variance \\(\\sigma_i^2\\).\nThe sample mean and variance for group \\(i\\) are\n\\[\n\\bar{Y}_{i\\cdot} \\;=\\; \\frac{1}{r_i} \\sum_{t=1}^{r_i} Y_{it},\n\\quad\\quad\ns_i^2 \\;=\\; \\frac{1}{r_i - 1} \\sum_{t=1}^{r_i} \\bigl(Y_{it} - \\bar{Y}_{i\\cdot}\\bigr)^2.\n\\]"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#satterthwaites-approximation-for-a-contrast",
    "href": "lectures/week-04_model-diagnostics.html#satterthwaites-approximation-for-a-contrast",
    "title": "Model Diagnostics",
    "section": "Satterthwaite’s Approximation for a Contrast",
    "text": "Satterthwaite’s Approximation for a Contrast\nA contrast in the group means \\(\\{\\tau_i\\}\\) is a linear combination \\(\\sum_{i=1}^v c_i \\tau_i\\) such that \\(\\sum_i c_i = 0\\). For pairwise differences, you can think of \\(c_h = +1\\), \\(c_i = -1\\), and the rest 0, to compare \\(\\tau_h - \\tau_i\\).\nThe least squares estimate of a contrast is\n\\[\n\\widehat{\\sum_{i} c_i \\tau_i}\n\\;=\\;\n\\sum_{i} c_i \\,\\bar{Y}_{i\\cdot}.\n\\]"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#estimated-variance-of-the-contrast",
    "href": "lectures/week-04_model-diagnostics.html#estimated-variance-of-the-contrast",
    "title": "Model Diagnostics",
    "section": "Estimated Variance of the Contrast",
    "text": "Estimated Variance of the Contrast\nBecause group \\(i\\) has variance \\(\\sigma_i^2\\), the variance of \\(\\bar{Y}_{i\\cdot}\\) is \\(\\sigma_i^2 / r_i\\). Thus,\n\\[\n\\text{Var}\\!\\Bigl(\\sum_{i} c_i \\,\\bar{Y}_{i\\cdot}\\Bigr)\n\\;=\\;\n\\sum_{i} \\frac{c_i^2 \\,\\sigma_i^2}{r_i}.\n\\]\nIn practice, we estimate \\(\\sigma_i^2\\) by \\(s_i^2\\). Hence,\n\\[\n\\widehat{\\text{Var}}\\Bigl(\\sum_{i} c_i \\,\\bar{Y}_{i\\cdot}\\Bigr)\n\\;=\\;\n\\sum_{i} \\frac{c_i^2 \\,s_i^2}{r_i}.\n\\]"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#degrees-of-freedom-welchsatterthwaite-formula",
    "href": "lectures/week-04_model-diagnostics.html#degrees-of-freedom-welchsatterthwaite-formula",
    "title": "Model Diagnostics",
    "section": "Degrees of Freedom (Welch–Satterthwaite Formula)",
    "text": "Degrees of Freedom (Welch–Satterthwaite Formula)\nWe do not pool variances; each group’s variance is separately estimated. So the approximate degrees of freedom is given by:\n\\[\n\\text{df}\n\\;\\approx\\;\n\\frac{\\left(\\sum_{i} \\frac{c_i^2\\,s_i^2}{r_i}\\right)^2}\n{\\sum_{i} \\frac{\\left(\\frac{c_i^2\\,s_i^2}{r_i}\\right)^2}{r_i-1}},\n\\]\nsometimes called the Satterthwaite or Welch–Satterthwaite formula."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#confidence-intervals",
    "href": "lectures/week-04_model-diagnostics.html#confidence-intervals",
    "title": "Model Diagnostics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nTherefore, an approximate \\((1 - \\alpha)100\\%\\) confidence interval for \\(\\sum_i c_i \\tau_i\\) becomes:\n\\[\n\\sum_i c_i \\,\\bar{Y}_{i\\cdot}\n\\;\\pm\\;\nt_{\\text{df},\\,\\alpha/2}\n\\;\\sqrt{\\sum_i \\frac{c_i^2 \\, s_i^2}{r_i}},\n\\]\nwhere \\(t_{\\text{df},\\,\\alpha/2}\\) is the usual \\(t\\)-quantile with \\(\\text{df}\\) degrees of freedom from above."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#multiple-comparisons-the-gameshowell-method",
    "href": "lectures/week-04_model-diagnostics.html#multiple-comparisons-the-gameshowell-method",
    "title": "Model Diagnostics",
    "section": "Multiple Comparisons: The Games–Howell Method",
    "text": "Multiple Comparisons: The Games–Howell Method\nWhen you want all pairwise comparisons, you can adapt the usual Tukey “Honestly Significant Difference” (HSD) approach by:\n\nComputing the standard error \\(\\sqrt{(s_h^2/r_h) + (s_i^2/r_i)}\\) for each pair \\((h,i)\\).\nComputing the approximate df from Satterthwaite’s formula for that pair.\nUsing the studentized-range critical value \\(q_{v,\\text{df},\\alpha}\\) (the same one used in Tukey’s HSD) but evaluating it at each pairwise df if you want to be extra precise, or at a conservative minimum among the pairs.\nHence getting the “minimum significant difference” (\\(\\text{msd}\\)) for each pair and deciding if \\(|\\bar{Y}_{h\\cdot} - \\bar{Y}_{i\\cdot}|\\) exceeds that msd.\n\nIn R, this is usually done through built-in or add-on functions called the Games–Howell test."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#example-trout-experiment-illustrative-data",
    "href": "lectures/week-04_model-diagnostics.html#example-trout-experiment-illustrative-data",
    "title": "Model Diagnostics",
    "section": "Example: Trout Experiment (Illustrative Data)",
    "text": "Example: Trout Experiment (Illustrative Data)\nIn the “Trout Experiment” example from many textbooks, there are 4 treatment levels (let’s label them A, B, C, D) each with \\(n=10\\) observations. Suppose the raw data are as follows (fictitious for illustration). In practice, you would read them from a file or have them given in a data frame.\nBelow, we create a simulated dataset with unequal group variances roughly resembling the example’s summary:\n\nTreatment A: \\(\\bar{Y} \\approx 7.7\\), variance \\(\\approx 1.0\\)\nTreatment B: \\(\\bar{Y} \\approx 9.5\\), variance \\(\\approx 3.0\\)\nTreatment C: \\(\\bar{Y} \\approx 9.2\\), variance \\(\\approx 1.3\\)\nTreatment D: \\(\\bar{Y} \\approx 8.8\\), variance \\(\\approx 1.0\\)"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#trout-experiment-data",
    "href": "lectures/week-04_model-diagnostics.html#trout-experiment-data",
    "title": "Model Diagnostics",
    "section": "Trout Experiment Data",
    "text": "Trout Experiment Data\nBelow is a plain-language summary of the trout experiment data to help someone with little background in fish biology or statistics understand the context and technical terms:\n\nWhat Is the Experiment About?\nResearchers are trying out four different treatments (labeled “1,” “2,” “3,” and “4”) on trout. These treatments are various sulfa-based or sulfonamide medications (often used to control bacterial infections or other conditions in fish). The main goal is to see whether different treatments lead to different outcomes in the trout’s health."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#key-terms-explained",
    "href": "lectures/week-04_model-diagnostics.html#key-terms-explained",
    "title": "Model Diagnostics",
    "section": "Key Terms Explained",
    "text": "Key Terms Explained\n\nTrout\n\nA common type of freshwater fish.\n\nIn the experiment, trout are the “test subjects,” similar to patients in a medical study.\n\nSulfa\n\nThis refers to sulfonamide treatments (a type of medication).\n\nThe data labels (1, 2, 3, or 4) tell us which specific sulfa treatment each trout received.\n\nHemo\n\nShort for “hemoglobin” (or a similar blood measure).\n\nHemoglobin is the substance in blood that carries oxygen. A higher or lower level can indicate different states of health.\n\nIn the experiment, Hemo is the response variable— what the researchers measure in each trout to see if the treatment had any effect.\n\nExperimental Setup\n\nEach of the four treatments was given to 10 different trout (total of 40 fish).\n\nAfter some time, the Hemo level was measured for each fish, giving one reading per trout.\n\n\n\nWhy Are They Doing This?\nTrout farmers or fisheries might want a medication to keep fish healthier or improve survival. By comparing different sulfa treatments:\n\nThey can see if one treatment leads to higher (better) or lower Hemo measurements than another.\nThey can learn which treatments seem to work best, or if they all perform about the same.\n\n\n\nWhat Does the Data Look Like?\nThe data table has two columns: 1. Sulfa – which of the four treatments the trout received.\n2. Hemo – the measured blood value after treatment.\nEach row in the table is a single trout’s record: - Sulfa (1, 2, 3, or 4)\n- Hemo (a number, like 7.0 or 10.6)"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#loading-the-data-in-r",
    "href": "lectures/week-04_model-diagnostics.html#loading-the-data-in-r",
    "title": "Model Diagnostics",
    "section": "Loading the Data in R",
    "text": "Loading the Data in R\n\ntroutData &lt;- readr::read_table(here(\"data\", \"dean2017\", \"trout.txt\")) |&gt; mutate(Sulfa = factor(Sulfa))\nhead(troutData)\n\n\n\n\n\n\n\n\nSulfa\nHemo\n\n\n\n\n1\n6.7\n\n\n1\n7.8\n\n\n1\n5.5\n\n\n1\n8.4\n\n\n1\n7.0\n\n\n1\n7.8\n\n\n\n\n\n\n\nTable 4: Trout Experiment Data: Sulfa Treatment and Hemoglobin Levels\n\n\n\n\n\nSulfa\n\nDefinition: Indicates the type or level of sulfonamide treatment.\nValues: 1, 2, 3, or 4 (categorical factor with four levels).\nEach level (1 through 4) appears exactly 10 times, so we have 10 trout in each of the four treatments.\n\nHemo\n\nDefinition: The response variable measured on each trout (for example, “hemoglobin” concentration or “hematocrit” level; depending on the specific study description).\nValues: Numeric, typically representing some biological measurement linked to trout health or oxygen-carrying capacity in the blood."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#exploratory-plots",
    "href": "lectures/week-04_model-diagnostics.html#exploratory-plots",
    "title": "Model Diagnostics",
    "section": "Exploratory Plots",
    "text": "Exploratory Plots\n\n# Quick Boxplot to See Group means and Variation\nboxplot(Hemo ~ Sulfa,\n    data = troutData,\n    main = \"Trout Experiment Data\",\n    ylab = \"Hemoglobin\", xlab = \"Sulfonamide\"\n)\n\n\n\n\n\n\n\nFigure 6: Boxplot of Hemoglobin Levels by Sulfa Treatment"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#check-group-means-and-variances",
    "href": "lectures/week-04_model-diagnostics.html#check-group-means-and-variances",
    "title": "Model Diagnostics",
    "section": "Check Group Means and Variances",
    "text": "Check Group Means and Variances\n\ntroutSummary &lt;- troutData |&gt;\n    group_by(Sulfa) |&gt;\n    summarize(\n        n       = n(),\n        meanVal = mean(Hemo),\n        varVal  = var(Hemo),\n        sdVal   = sd(Hemo)\n    )\ntroutSummary\n\n\n\n\n\n\nSulfa\nn\nmeanVal\nvarVal\nsdVal\n\n\n\n\n1\n10\n7.20\n1.037778\n1.018714\n\n\n2\n10\n9.33\n2.946778\n1.716618\n\n\n3\n10\n9.03\n1.289000\n1.135341\n\n\n4\n10\n8.69\n1.001000\n1.000500\n\n\n\nSummary Statistics for Trout Experiment Data\n\n\nThis roughly replicates the scenario in the textbook example (\\(s_1^2 \\approx 1.04, s_2^2 \\approx 2.95,\\) etc.)."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#manual-gameshowell-multiple-comparisons-step-by-step-in-r",
    "href": "lectures/week-04_model-diagnostics.html#manual-gameshowell-multiple-comparisons-step-by-step-in-r",
    "title": "Model Diagnostics",
    "section": "Manual Games–Howell Multiple Comparisons (Step-by-Step in R)",
    "text": "Manual Games–Howell Multiple Comparisons (Step-by-Step in R)\nWe can either:\n\nManually compute each pairwise difference, its standard error, degrees of freedom, and the corresponding “minimum significant difference.”\nUse an R package function that automates these calculations.\n\nBelow, we first show a brief “manual” approach, then a dedicated function from a package."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#manual-calculations",
    "href": "lectures/week-04_model-diagnostics.html#manual-calculations",
    "title": "Model Diagnostics",
    "section": "Manual Calculations",
    "text": "Manual Calculations"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#create-a-summary-table",
    "href": "lectures/week-04_model-diagnostics.html#create-a-summary-table",
    "title": "Model Diagnostics",
    "section": "Create a Summary Table",
    "text": "Create a Summary Table\n\n# Extract Means, Variances for Each Group\nmeans &lt;- troutSummary$meanVal\nvars &lt;- troutSummary$varVal\nnames(means) &lt;- troutSummary$Sulfa\nnames(vars) &lt;- troutSummary$Sulfa\n\n# We'll Use 'combn' to Get All Pairs: This yields pairs like `A-B`, `A-C`, `A-D`, `B-C`, `B-D`, `C-D`.\n\ngroups &lt;- levels(troutData$Sulfa)\npairs &lt;- combn(groups, 2, simplify = TRUE)\npairs\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] \"1\"  \"1\"  \"1\"  \"2\"  \"2\"  \"3\" \n[2,] \"2\"  \"3\"  \"4\"  \"3\"  \"4\"  \"4\" \n\n\nThis yields pairs like A-B, A-C, A-D, B-C, B-D, C-D."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#compute-each-pairs-stats",
    "href": "lectures/week-04_model-diagnostics.html#compute-each-pairs-stats",
    "title": "Model Diagnostics",
    "section": "Compute Each Pair’s Stats",
    "text": "Compute Each Pair’s Stats\nWe can write a small function to compute:\n\nThe difference in means.\nThe standard error \\(\\sqrt{\\frac{s_h^2}{n_h} + \\frac{s_i^2}{n_i}}\\).\nThe approximate df from Satterthwaite’s formula.\n\n\n# Create a function that takes a data frame with a grouping column (factor)\n# and a numeric response column, then computes pairwise comparisons\n# (mean difference, standard error, and Satterthwaite df) for each pair.\n\npairwise_satterthwaite &lt;- function(data, group_col, response_col) {\n    # Ensure the grouping column is a factor\n    data[[group_col]] &lt;- as.factor(data[[group_col]])\n\n    # Summarize mean, variance, n for each group\n    summary_df &lt;- data |&gt;\n        group_by(.data[[group_col]]) |&gt;\n        summarize(\n            n       = n(),\n            meanVal = mean(.data[[response_col]]),\n            varVal  = var(.data[[response_col]]),\n            .groups = \"drop\"\n        )\n\n    # For convenience\n    group_levels &lt;- levels(data[[group_col]])\n    means &lt;- setNames(summary_df$meanVal, summary_df[[group_col]])\n    vars &lt;- setNames(summary_df$varVal, summary_df[[group_col]])\n    ns &lt;- setNames(summary_df$n, summary_df[[group_col]])\n\n    # Define Satterthwaite function\n    satterthwaite_df &lt;- function(var1, var2, n1, n2) {\n        num &lt;- (var1 / n1 + var2 / n2)^2\n        denom &lt;- (var1^2 / (n1^2 * (n1 - 1))) + (var2^2 / (n2^2 * (n2 - 1)))\n        num / denom\n    }\n\n    # Get all pairs of groups\n    pair_mat &lt;- combn(group_levels, 2)\n\n    # Calculate stats for each pair\n    pair_list &lt;- apply(pair_mat, 2, function(pair) {\n        g1 &lt;- pair[1]\n        g2 &lt;- pair[2]\n        diff &lt;- means[g2] - means[g1]\n        se &lt;- sqrt(vars[g1] / ns[g1] + vars[g2] / ns[g2])\n        df &lt;- satterthwaite_df(vars[g1], vars[g2], ns[g1], ns[g2])\n\n        data.frame(\n            group1 = g1,\n            group2 = g2,\n            meanDiff = diff,\n            SE = se,\n            df = round(df, 1)\n        )\n    })\n\n    # Combine into a single data frame\n    do.call(rbind, pair_list)\n}\n\n# Example usage (assuming your data frame is called 'troutData'\n# and has columns 'Group' and 'Response'):\npairwiseDF &lt;- pairwise_satterthwaite(troutData, group_col = \"Sulfa\", response_col = \"Hemo\")\nprint(pairwiseDF)\n\n   group1 group2 meanDiff        SE   df\n2       1      2     2.13 0.6312334 14.6\n3       1      3     1.83 0.4823668 17.8\n4       1      4     1.49 0.4515283 18.0\n31      2      3    -0.30 0.6508285 15.6\n41      2      4    -0.64 0.6283134 14.5\n42      3      4    -0.34 0.4785394 17.7"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#compute-the-gameshowell-test-statistic-and-p-values",
    "href": "lectures/week-04_model-diagnostics.html#compute-the-gameshowell-test-statistic-and-p-values",
    "title": "Model Diagnostics",
    "section": "Compute the Games–Howell Test Statistic and p-Values",
    "text": "Compute the Games–Howell Test Statistic and p-Values\nFor each pair \\((h,i)\\), we have:\n\\[\nt_{hi}^* = \\frac{\\bar{Y}_{i\\cdot} - \\bar{Y}_{h\\cdot}}{\\text{SE}_{(h,i)}},\n\\quad\n\\text{df}_{(h,i)} \\approx \\text{(Satterthwaite formula)},\n\\]\nand the p-value is based on the studentized range distribution if you want to replicate “Tukey-like” inference, or you can do an approximate t-test approach. For a full Games–Howell:\n\nStudentized range approach:\n\n\\[\nq = \\frac{ |\\bar{Y}_{i} - \\bar{Y}_{h}| }{SE_{(h,i)}},\n\\]\ncompare with \\(q_{v,\\text{df},\\alpha}\\).\n\nAlternative: Use a t-approximation with df from above. (This is a bit simpler to code manually.)\n\nBelow, we do the simpler t-based approach for each pair:\n\nalpha &lt;- 0.05\npairwiseDF &lt;- pairwiseDF |&gt;\n    mutate(\n        tval = meanDiff / SE,\n        pval_satt = 2 * pt(-abs(tval), df) # two-sided p-value from t-distribution with Satterthwaite df\n    )\npairwiseDF\n\n\n\n\n\n\n\ngroup1\ngroup2\nmeanDiff\nSE\ndf\ntval\npval_satt\n\n\n\n\n2\n1\n2\n2.13\n0.6312334\n14.6\n3.3743464\n0.0043106\n\n\n3\n1\n3\n1.83\n0.4823668\n17.8\n3.7937930\n0.0013522\n\n\n4\n1\n4\n1.49\n0.4515283\n18.0\n3.2999041\n0.0039821\n\n\n31\n2\n3\n-0.30\n0.6508285\n15.6\n-0.4609509\n0.6511949\n\n\n41\n2\n4\n-0.64\n0.6283134\n14.5\n-1.0185999\n0.3250884\n\n\n42\n3\n4\n-0.34\n0.4785394\n17.7\n-0.7104952\n0.4866541\n\n\n\nPairwise t-Tests: t-Value and p-Value\n\n\nThat yields approximate pairwise t-tests that do not assume equal variances, with Satterthwaite df. However, be aware that controlling familywise error strictly with a t-test approach requires a correction or the studentized-range approach."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#minimum-significant-difference-msd-for-each-pair-tukey-likeness",
    "href": "lectures/week-04_model-diagnostics.html#minimum-significant-difference-msd-for-each-pair-tukey-likeness",
    "title": "Model Diagnostics",
    "section": "Minimum Significant Difference (msd) for Each Pair (Tukey-likeness)",
    "text": "Minimum Significant Difference (msd) for Each Pair (Tukey-likeness)\nTo emulate the Games–Howell approach specifically, you compute:\n\\[\n\\text{msd}_{(h,i)}\n\\;=\\;\n\\sqrt{\\frac{s_h^2}{n_h} + \\frac{s_i^2}{n_i}}\n\\;\\times\\;\n\\frac{q_{v,\\,\\text{df}_{(h,i)},\\,\\alpha}}{\\sqrt{2}},\n\\]\nwhere \\(v=4\\) is the number of treatments. Then you see if \\(|\\bar{Y}_{h\\cdot} - \\bar{Y}_{i\\cdot}|\\) is larger than msd.\nA small snippet:\n\n# We'll Define a Function that Approximates q_{v, Df, alpha}\n# Because the 'df' is not Straightforward, We Might Do a Conservative Approach\n# Or We Can Do Per-pair if We Like.\ngamesHowell_msd &lt;- function(var_hi, df_hi, alpha = 0.05, v = 4) {\n    # We Have Pair-specific Df, but the 'Tukey' Distribution with Non-integer Df\n    # Sometimes Requires Approximation.\n    # We'll Do a Rough Approach: q_{v, floor(df_hi), alpha}\n\n    df_floor &lt;- floor(df_hi)\n    df_round &lt;- round(df_hi, 1)\n    # q_{v, Df, alpha} = qtukey(1 - Alpha, nmeans=v, df_floor)\n    # Then We Scale by 1/sqrt(2).\n\n    qval &lt;- qtukey(1 - alpha, nmeans = v, df = df_floor)\n    msd &lt;- sqrt(var_hi) * (qval / sqrt(2))\n    msd\n}\n\n# Calculate group-specific variances and sample sizes\nvars &lt;- tapply(troutData$Hemo, troutData$Sulfa, var)\nnvec &lt;- tapply(troutData$Hemo, troutData$Sulfa, length)\n\n# Now We Apply it Pairwise:\npairwiseDF &lt;- pairwiseDF |&gt;\n    rowwise() |&gt;\n    mutate(\n        var_hi = (vars[group1] / nvec[group1] + vars[group2] / nvec[group2]),\n        qval = qtukey(1 - alpha, nmeans = 4, df = floor(df)),\n        msd = sqrt(var_hi) * (qval / sqrt(2)),\n        diffSignificant = (abs(meanDiff) &gt; msd),\n        pval_ght = ptukey(abs(tval) * sqrt(2), nmeans = 4, df = floor(df), lower.tail = FALSE)\n    ) |&gt;\n    ungroup()\n\npairwiseDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup1\ngroup2\nmeanDiff\nSE\ndf\ntval\npval_satt\nvar_hi\nqval\nmsd\ndiffSignificant\npval_ght\n\n\n\n\n1\n2\n2.13\n0.6312334\n14.6\n3.3743464\n0.0043106\n0.3984556\n4.110506\n1.834722\nTRUE\n0.0208959\n\n\n1\n3\n1.83\n0.4823668\n17.8\n3.7937930\n0.0013522\n0.2326778\n4.019985\n1.371156\nTRUE\n0.0071669\n\n\n1\n4\n1.49\n0.4515283\n18.0\n3.2999041\n0.0039821\n0.2038778\n3.996978\n1.276150\nTRUE\n0.0189679\n\n\n2\n3\n-0.30\n0.6508285\n15.6\n-0.4609509\n0.6511949\n0.4235778\n4.075974\n1.875785\nFALSE\n0.9664090\n\n\n2\n4\n-0.64\n0.6283134\n14.5\n-1.0185999\n0.3250884\n0.3947778\n4.110506\n1.826235\nFALSE\n0.7415785\n\n\n3\n4\n-0.34\n0.4785394\n17.7\n-0.7104952\n0.4866541\n0.2290000\n4.019985\n1.360276\nFALSE\n0.8915554\n\n\n\nPairwise Comparisons: Minimum Significant Difference (msd)"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#using-a-ready-made-r-function",
    "href": "lectures/week-04_model-diagnostics.html#using-a-ready-made-r-function",
    "title": "Model Diagnostics",
    "section": "Using a Ready-Made R Function",
    "text": "Using a Ready-Made R Function\nSeveral R packages implement the Games-Howell test automatically. Two common approaches:\n\nPairwiseTTest() from the DescTools package with the pool.sd = FALSE argument.\n\ngamesHowellTest() from the PMCMRplus packages. See Games-Howell Post-Hoc Comparison for more details.\n\nBelow, we demonstrate the PMCMRplus package’s gamesHowellTest() function.\n\n\n\nfit &lt;- aov(Hemo ~ Sulfa, data = troutData)\ngh_test &lt;- PMCMRplus::gamesHowellTest(fit)\nsummary(gh_test)\n\n           q value Pr(&gt;|q|)   \n2 - 1 == 0   4.772 0.019942  *\n3 - 1 == 0   5.365 0.006735 **\n4 - 1 == 0   4.667 0.018973  *\n3 - 2 == 0  -0.652 0.966450   \n4 - 2 == 0  -1.441 0.741476   \n4 - 3 == 0  -1.005 0.891609   \n\n\n\nTable 5: Games–Howell Test: Pairwise Comparisons\n\n\n\nThis will print out pairwise comparisons, confidence intervals, and p-values, controlling familywise error at alpha = 0.05. You’ll see a compact letter display or a table showing which pairs are significantly different.\nNOTE: Different packages might handle rounding or approximate df differently, so minor numerical differences from manual calculations can occur."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#interpretation-and-conclusion",
    "href": "lectures/week-04_model-diagnostics.html#interpretation-and-conclusion",
    "title": "Model Diagnostics",
    "section": "Interpretation and Conclusion",
    "text": "Interpretation and Conclusion\n\nInterpretation: If \\(\\bar{Y}_{h\\cdot} - \\bar{Y}_{i\\cdot}\\) exceeds the corresponding Games–Howell msd in absolute value, we conclude that the treatments \\(h\\) and \\(i\\) differ significantly at the chosen familywise error rate (often 5%).\n\nAdvantages: No need to assume equal variances; results stay in original data units without forcing a transformation.\n\nDisadvantages: Typically has lower power than methods that rely on a correct assumption of equal variances (or a well-chosen variance-stabilizing transformation).\n\nIn the trout example, we often find that Treatment B is significantly different from A, that A differs from B and C, etc., while B, C, and D might be close or show smaller pairwise differences (depending on the data)."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#summary-1",
    "href": "lectures/week-04_model-diagnostics.html#summary-1",
    "title": "Model Diagnostics",
    "section": "Summary",
    "text": "Summary\n\nSatterthwaite’s Approximation gives an approximate \\(t\\)-distribution for each contrast with a “floating” degrees of freedom.\n\nThe Games–Howell procedure adapts Tukey’s HSD to unequal variances, computing pair-specific standard errors and degrees of freedom.\n\nIn R, you can do these calculations manually (as demonstrated) or use a dedicated function from packages like DescTools or rcompanion to streamline the process.\n\nAlways check the assumptions (normality, independence, etc.). When in doubt, consider robust or nonparametric approaches as well."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#conclusion-1",
    "href": "lectures/week-04_model-diagnostics.html#conclusion-1",
    "title": "Model Diagnostics",
    "section": "Conclusion",
    "text": "Conclusion\nAssumption checking in ANOVA and regression is not a mere formality—it safeguards against invalid conclusions. By using residual diagnostics, normality checks, and transformations, we ensure models accurately represent reality. While no single approach solves all problems, a combination of visual and formal diagnostic tools, along with informed corrective strategies, leads to more trustworthy and robust statistical inferences."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html",
    "href": "lectures/week-02_crd-overview.html",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "",
    "text": "In mathematical statistics, well-structured experimental designs and robust analytical methods are essential for drawing valid conclusions about treatment effects. One of the simplest yet fundamental designs is the completely randomized design (CRD). Under a CRD, each experimental unit is randomly assigned to one of several treatment conditions without any blocking factors. This approach ensures that all treatments have an equal chance of being applied to each unit, thereby mitigating systematic biases and facilitating straightforward inference.\nA key tool for analyzing CRDs is the one-way analysis of variance (ANOVA). The one-way ANOVA tests whether at least two of several treatment means differ significantly. Building upon principles of linear models, least squares estimation, and classical hypothesis testing, the one-way ANOVA provides a comprehensive framework to quantify uncertainty and make decisions about treatments based on observed data.\n\n\n\nUnderstand the structure and rationale behind the completely randomized design.\nLearn the randomization procedure and the role of coding treatments.\nFormulate the one-way ANOVA model and understand its assumptions.\nExplore concepts of estimability, least squares estimation, and the distribution of estimators.\nApply the ANOVA test to decide whether treatments differ significantly.\nCalculate sample size and power, and incorporate these insights when planning experiments.\nDemonstrate procedures using R, including randomization, data input, fitting models, and producing ANOVA tables.\n\n\n\n\nThis lecture uses several R packages for data manipulation, analysis, and plotting:\n\npacman::p_load(emmeans, pwr, here)\n\n\npacman: A package management tool for installing and loading packages.\nemmeans: For estimating treatment means and testing contrasts.\npwr: For power calculations in hypothesis testing.\nhere: For managing file paths in R projects."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#objectives",
    "href": "lectures/week-02_crd-overview.html#objectives",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "",
    "text": "Understand the structure and rationale behind the completely randomized design.\nLearn the randomization procedure and the role of coding treatments.\nFormulate the one-way ANOVA model and understand its assumptions.\nExplore concepts of estimability, least squares estimation, and the distribution of estimators.\nApply the ANOVA test to decide whether treatments differ significantly.\nCalculate sample size and power, and incorporate these insights when planning experiments.\nDemonstrate procedures using R, including randomization, data input, fitting models, and producing ANOVA tables."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#r-packages",
    "href": "lectures/week-02_crd-overview.html#r-packages",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "",
    "text": "This lecture uses several R packages for data manipulation, analysis, and plotting:\n\npacman::p_load(emmeans, pwr, here)\n\n\npacman: A package management tool for installing and loading packages.\nemmeans: For estimating treatment means and testing contrasts.\npwr: For power calculations in hypothesis testing.\nhere: For managing file paths in R projects."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#defining-experimental-design",
    "href": "lectures/week-02_crd-overview.html#defining-experimental-design",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Defining Experimental Design",
    "text": "Defining Experimental Design\nAn experimental design is a rule determining how experimental units are assigned to treatments. In a completely randomized design (CRD), no blocking factors are considered; all experimental units are assigned to treatments purely at random. This simplicity allows:\n\nEqual probability of treatment assignment.\nAvoidance of experimenter bias.\nStraightforward statistical analysis.\n\nExamples include testing different battery types or comparing rotary pump speeds to measure fluid flow. When no major nuisance factors (like day-to-day variation or spatial differences) need controlling, a CRD is often appropriate."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#randomization-procedure",
    "href": "lectures/week-02_crd-overview.html#randomization-procedure",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Randomization Procedure",
    "text": "Randomization Procedure\nRandomization ensures fairness and the validity of the statistical distributions underlying standard tests. Treatments, labeled 1 through \\(v\\), are assigned to experimental units by generating random numbers and sorting them. Modern software (R) or random number tables can be used. For instance, if testing three treatment levels (A, B, C) with equal sample sizes, we can:\n\nGenerate random numbers for each unit.\nRank units by their random numbers.\nAssign treatments in order based on the desired allocation.\n\nThis procedure ensures every arrangement is equally likely, aligning with the assumptions in ANOVA and related inference."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#randomization-concept",
    "href": "lectures/week-02_crd-overview.html#randomization-concept",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Randomization Concept",
    "text": "Randomization Concept\nRandomization ensures that each experimental unit has an equal chance of receiving any treatment. This prevents systematic bias and justifies the assumptions underlying standard statistical tests.\nExample: Use a random number generator to assign treatments to plots, ensuring no predetermined pattern."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#how-to-perform-randomization",
    "href": "lectures/week-02_crd-overview.html#how-to-perform-randomization",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "How to Perform Randomization",
    "text": "How to Perform Randomization\n\nProcedure\n\nINPUT: List of experimental units, List of treatments\nOUTPUT: Randomized assignments of treatments to units\n\n\nDefine \\(N\\) = Total number of experimental units (e.g., 12).\nDefine \\(T\\) = List of treatments (e.g., A, B, C).\nRepeat \\(T\\) to match the number of experimental units.\nShuffle the list of the repeated treatmentsn from step 3 randomly.\nAssign shuffled treatments to the experimental units.\nReturn randomized assignments"
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#r-implementation-randomization-examples-in-crd",
    "href": "lectures/week-02_crd-overview.html#r-implementation-randomization-examples-in-crd",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "R Implementation: Randomization Examples in CRD",
    "text": "R Implementation: Randomization Examples in CRD\n\n\n\nN &lt;- 12 # Number of experimental units, Step 1\n# Define experimental units and treatments\nexperimental_units &lt;- 1:N ## 10 experimental units\ntreatments &lt;- c(\"A\", \"B\", \"C\") ## 3 treatments, Step 2\n\n# Repeat treatments to match the number of units. The code covers both balanced and unbalanced designs.\ntreatments &lt;- rep(treatments, length.out = N)  ## Step 3\n\n# For a balanced design, we can also use:\n# treatments &lt;- rep(treatments, each = N/length(treatments))\n\n# Shuffle (randomize) treatments randomly to assign to units, Step 4\nset.seed(123) ## For reproducibility\nrandomized_treatments &lt;- sample(treatments)\n\n# Create a data frame with randomized assignments\nrandomization_crd &lt;- data.frame(Unit = experimental_units, Treatment = randomized_treatments)  ## Step 5\nprint(randomization_crd)\n\n   Unit Treatment\n1     1         C\n2     2         C\n3     3         A\n4     4         B\n5     5         C\n6     6         B\n7     7         B\n8     8         A\n9     9         C\n10   10         B\n11   11         A\n12   12         A\n\n\n\nTable 1: Randomized Assignments in CRD"
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#model-formulation",
    "href": "lectures/week-02_crd-overview.html#model-formulation",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Model Formulation",
    "text": "Model Formulation\nConsider \\(v\\) treatments with \\(n\\) observations each (for simplicity). Let \\(Y_{ij}\\) denote the response from the \\(j\\)-th unit receiving treatment \\(i\\), with \\(i=1,\\dots,v\\) and \\(j=1,\\dots,n\\). A common model is:\n\\[\nY_{ij} = \\mu + \\tau_i + \\varepsilon_{ij},\n\\qquad(1)\\]\nwhere:\n\n\\(\\mu\\) is the overall mean response.\n\\(\\tau_i\\) is the effect of the \\(i\\)-th treatment.\n\\(\\varepsilon_{ij}\\) are independent, normally distributed errors with mean 0 and variance \\(\\sigma^2\\).\n\nThe assumptions are:\n\nErrors \\(\\varepsilon_{ij}\\) are i.i.d. \\(N(0,\\sigma^2)\\).\nTreatment effects sum to zero: \\(\\sum_{i=1}^v \\tau_i = 0\\) for identifiability.\n\nThis linear model model underpins the ANOVA. The null hypothesis for testing no treatment differences is \\(H_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_v = 0\\).\n\n\n\nidentifiability:"
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#estimability-and-least-squares-estimation",
    "href": "lectures/week-02_crd-overview.html#estimability-and-least-squares-estimation",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Estimability and Least Squares Estimation",
    "text": "Estimability and Least Squares Estimation\nNot all model parameters (\\(\\tau_i\\)) are independently estimable. However, estimable functions, linear combinations of parameters that correspond to expected values of observed data, can be estimated uniquely.\nThe least squares estimators (LSEs) solve the normal equations obtained by minimizing the sum of squared errors (SSE):\n\\[\n\\text{SSE} = \\sum_{i=1}^v \\sum_{j=1}^n (Y_{ij} - \\hat{\\mu}_i)^2,\n\\qquad(2)\\]\nwhere \\(\\hat{\\mu}_i\\) is the estimated mean for treatment \\(i\\). The LSE for each treatment mean \\(\\mu_i = \\mu + \\tau_i\\) is simply the sample mean \\(\\bar{Y}_i = \\frac{1}{n}\\sum_{j=1}^n Y_{ij}\\).\nBecause the normal equations are linearly dependent, an additional constraint is needed (e.g., \\(\\sum \\tau_i = 0\\)). Still, any estimable function (like differences between treatment means) can be uniquely estimated."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#distribution-of-estimators",
    "href": "lectures/week-02_crd-overview.html#distribution-of-estimators",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Distribution of Estimators",
    "text": "Distribution of Estimators\nUnder the normal error assumption, each estimator \\(\\bar{Y}_i\\) is normally distributed, with:\n\\[\nE(\\bar{Y}_i) = \\mu_i, \\quad \\text{Var}(\\bar{Y}_i) = \\frac{\\sigma^2}{n}.\n\\qquad(3)\\]\nThe error variance \\(\\sigma^2\\) is estimated by the mean square error (MSE):\n\\[\n\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{N - v},\n\\qquad(4)\\]\nwhere \\(N = v \\cdot n\\) is the total number of observations."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#the-anova-f-test",
    "href": "lectures/week-02_crd-overview.html#the-anova-f-test",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "The ANOVA F-Test",
    "text": "The ANOVA F-Test\nTo test \\(H_0: \\tau_1 = \\cdots = \\tau_v = 0\\) against \\(H_a:\\) at least two \\(\\tau_i\\) differ, we use the ANOVA F-statistic:\n\\[\nF = \\frac{\\text{MST}}{\\text{MSE}},\n\\qquad(5)\\]\nwhere:\n\n\\(\\text{SST}\\) = treatment sum of squares measures variation among treatment means.\n\\(\\text{MSE}\\) = error mean square, \\(\\text{MSE} = \\text{SSE}/(N-v)\\).\n\\(\\text{MST} = \\text{SST}/(v-1)\\).\n\nUnder \\(H_0\\), \\(F \\sim F_{v-1, N-v}\\). If \\(F\\)-value exceeds a critical value (or the p-value is below a chosen \\(\\alpha\\)), we reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#anova-table",
    "href": "lectures/week-02_crd-overview.html#anova-table",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "ANOVA Table",
    "text": "ANOVA Table\nThe ANOVA table summarizes the decomposition of total variability:\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF-ratio\n\n\n\n\nTreatment\n\\(v-1\\)\nSST\nMST=SST/(v-1)\nMST/MSE\n\n\nError\n\\(N - v\\)\nSSE\nMSE=SSE/(N-v)\n\n\n\nTotal\n\\(N-1\\)\nSSTotal\n\n\n\n\n\n\n\nTable 2: ANOVA Table for a Completely Randomized Design\n\n\n\nThe table provides a concise summary of the ANOVA results, including degrees of freedom (DF), sum of squares (SS), mean squares (MS), and the F-ratio.\nRejecting \\(H_0\\) indicates that not all treatment means are equal."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#determining-sample-size-and-power",
    "href": "lectures/week-02_crd-overview.html#determining-sample-size-and-power",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Determining Sample Size and Power",
    "text": "Determining Sample Size and Power\nTo ensure the experiment can detect meaningful differences, consider the power analysis. Power calculations (detailed in referenced texts) help determine the number of replicates \\(n\\) per treatment to achieve a desired probability of detecting a specified difference at a given significance level \\(\\alpha\\).\nUse methods from Montgomery, Peck & Vining (2020) or Dean, Voss & Draguljić (2017), along with pilot data, to estimate \\(\\sigma^2\\) and decide on sample size. If resources are limited, one may have to adjust objectives or accept lower power."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#confidence-limits-for-variance",
    "href": "lectures/week-02_crd-overview.html#confidence-limits-for-variance",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Confidence Limits for Variance",
    "text": "Confidence Limits for Variance\nA one-sided confidence interval on \\(\\sigma^2\\) prevents underestimating required observations. The upper confidence limit is derived from the chi-squared distribution:\n\\[\n\\frac{(N-v)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{N-v}.\n\\qquad(6)\\]\nSuch intervals guide planning by addressing uncertainty in the variance estimate."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#randomization-in-r",
    "href": "lectures/week-02_crd-overview.html#randomization-in-r",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Randomization in R",
    "text": "Randomization in R\nSuppose we have 3 treatments (A, B, C) and total 9 units (3 per treatment):\n\nset.seed(123) # Total units\nn &lt;- 9 # Assign treatments to units, e.g., AAABBBCCC\ntreatments &lt;- rep(c(\"A\", \"B\", \"C\"), each = 3) # Assign random numbers\nrand_nums &lt;- runif(9) # Genrates random numbers\ndata_frame &lt;- data.frame(Unit = 1:9, Treatment = treatments, RN = rand_nums)\ndata_frame_sorted &lt;- data_frame[order(data_frame$RN), ] # Sort by RN\ndata_frame_sorted\n\n\n\n\n\n\n\n\n\nUnit\nTreatment\nRN\n\n\n\n\n6\n6\nB\n0.0455565\n\n\n1\n1\nA\n0.2875775\n\n\n3\n3\nA\n0.4089769\n\n\n7\n7\nC\n0.5281055\n\n\n9\n9\nC\n0.5514350\n\n\n2\n2\nA\n0.7883051\n\n\n4\n4\nB\n0.8830174\n\n\n8\n8\nC\n0.8924190\n\n\n5\n5\nB\n0.9404673\n\n\n\n\n\n\n\nTable 3: Randomized Treatment Allocation Data\n\n\n\n\nThis yields a random allocation order.\n\nExplanation of the Key Functions in R\n\nset.seed(123): Sets a seed for reproducibility.\nrep(c(\"A\", \"B\", \"C\"), each = 3): Creates a balanced treatment allocation, where each = 3 assigns 3 units to each treatment.\nrunif(9): Generates 9 random numbers from a uniform distribution.\norder: Sorts the data frame based on the random numbers. The result of the function is row indices that would sort the data frame."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#fitting-a-one-way-anova-model",
    "href": "lectures/week-02_crd-overview.html#fitting-a-one-way-anova-model",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Fitting a One-Way ANOVA Model",
    "text": "Fitting a One-Way ANOVA Model\nAssume we have data from a soap experiment (soap.txt) with three soap types (regular, deodorant, moisturizing):\n\npacman::p_load(here)\n# Read data\nsoap.data &lt;- read.table(here(\"data\", \"dean2017\", \"soap.txt\"), header = TRUE)\nsoap.data$fSoap &lt;- factor(soap.data$Soap)\n\n# Fit a one-way ANOVA\nfit &lt;- aov(WtLoss ~ fSoap, data = soap.data)\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfSoap        2 16.122   8.061   104.5 5.91e-07 ***\nResiduals    9  0.695   0.077                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA summary shows if soap type affects weight loss significantly."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#estimating-means-and-contrasts",
    "href": "lectures/week-02_crd-overview.html#estimating-means-and-contrasts",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Estimating Means and Contrasts",
    "text": "Estimating Means and Contrasts\nWe can estimate treatment means (least squares means) and test contrasts using emmeans or multcomp packages. For example:\n\npacman::p_load(emmeans)\nemmeans(fit, ~fSoap)\n\n fSoap emmean    SE df lower.CL upper.CL\n 1     -0.035 0.139  9   -0.349    0.279\n 2      2.700 0.139  9    2.386    3.014\n 3      1.992 0.139  9    1.678    2.307\n\nConfidence level used: 0.95 \n\n\nThis provides estimates and confidence intervals for each soap type’s mean weight loss."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#conclusion",
    "href": "lectures/week-02_crd-overview.html#conclusion",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Conclusion",
    "text": "Conclusion\nA completely randomized design and its corresponding one-way ANOVA analysis form the bedrock of experimental inference when blocking is unnecessary. By carefully randomizing treatments, modeling responses, and applying least squares estimation, one can confidently infer whether treatments differ.\nWe introduced the conceptual framework and key formulae, showed how to conduct hypothesis tests and interpret ANOVA results, discussed sample size and power considerations, and provided R code for practical execution. Mastery of these fundamentals empowers researchers to design efficient experiments, maximize the utility of collected data, and confidently draw conclusions about treatment effects."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#references",
    "href": "lectures/week-02_crd-overview.html#references",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "References",
    "text": "References\n\nChristensen, R. (2018). Analysis of variance, design, and regression: Linear modeling for unbalanced data (2nd Ed.). Chapman and Hall/CRC.\nDean, A., Voss, D., & Draguljić, D. (2017). Design and analysis of experiments. Springer. https://doi.org/10.1007/978-3-319-52250-0\nKempthorne, O. (1977). Design and Analysis of Experiments. Wiley.\nMontgomery, D. C., Peck, E. A., & Vining, G. G. (2020). Introduction to linear regression analysis (5th Ed.). Wiley."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#a.1-normal-equations-and-estimability",
    "href": "lectures/week-02_crd-overview.html#a.1-normal-equations-and-estimability",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "A.1 Normal Equations and Estimability",
    "text": "A.1 Normal Equations and Estimability\nThe one-way ANOVA model can be written as:\n\\[\nY_{ij} = \\mu_i + \\varepsilon_{ij}, \\quad \\text{with } \\mu_i = \\mu + \\tau_i.\n\\qquad(7)\\]\nThe least squares approach forms normal equations from partial derivatives of SSE. Because \\(\\sum \\tau_i = 0\\), only differences in means are estimable. The space of estimable functions includes contrasts like \\(\\mu_1 - \\mu_2\\), but not individual \\(\\tau_i\\) alone."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#a.2-distribution-of-the-f-statistic",
    "href": "lectures/week-02_crd-overview.html#a.2-distribution-of-the-f-statistic",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "A.2 Distribution of the F-Statistic",
    "text": "A.2 Distribution of the F-Statistic\nUnder \\(H_0\\):\n\\[\nF = \\frac{MST}{MSE} \\sim F_{v-1, N-v}.\n\\qquad(8)\\]\nIf treatments truly differ, F follows a noncentral F-distribution, enabling power calculations and minimum detectable differences."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#a.3-power-calculations",
    "href": "lectures/week-02_crd-overview.html#a.3-power-calculations",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "A.3 Power Calculations",
    "text": "A.3 Power Calculations\nPower for detecting a difference \\(\\delta\\) in means involves the noncentrality parameter of the F-distribution. By selecting \\(\\alpha, \\sigma^2, \\delta,\\) and desired power, tables or software (e.g., pwr package in R) guide sample size choices.\n\npacman::p_load(pwr)\npwr.anova.test(k = 3, f = 0.4, sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 21.10364\n              f = 0.4\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nThis computes required sample size per treatment to achieve 80% power with a given effect size \\(f\\)."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html",
    "href": "lectures/week-01_intro-design_part1.html",
    "title": "Fundamentals of Experimental Design",
    "section": "",
    "text": "In experimental design, the goal is to identify how various factors or treatments influence a response variable. Unlike observational studies, well-planned experiments leverage randomization, blocking, and replication to control and isolate sources of variation, thereby allowing robust causal conclusions. By thoughtfully designing experiments, researchers can more efficiently use resources, reduce systematic biases, and interpret data using classical statistical frameworks.\nThe following lecture notes will cover the fundamental principles of experimental design in the context of mathematical statistics. We will discuss the logic and practice of key techniques—randomization, blocking, and replication—and explore their role in ensuring valid and efficient inference. We will also highlight the interplay between design and analysis, referencing classical texts in design of experiments and illustrating concepts through examples and R code."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#introduction",
    "href": "lectures/week-01_intro-design_part1.html#introduction",
    "title": "Fundamentals of Experimental Design",
    "section": "",
    "text": "In experimental design, the goal is to identify how various factors or treatments influence a response variable. Unlike observational studies, well-planned experiments leverage randomization, blocking, and replication to control and isolate sources of variation, thereby allowing robust causal conclusions. By thoughtfully designing experiments, researchers can more efficiently use resources, reduce systematic biases, and interpret data using classical statistical frameworks.\nThe following lecture notes will cover the fundamental principles of experimental design in the context of mathematical statistics. We will discuss the logic and practice of key techniques—randomization, blocking, and replication—and explore their role in ensuring valid and efficient inference. We will also highlight the interplay between design and analysis, referencing classical texts in design of experiments and illustrating concepts through examples and R code."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#objectives",
    "href": "lectures/week-01_intro-design_part1.html#objectives",
    "title": "Fundamentals of Experimental Design",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the theoretical foundations and objectives of experimental design in statistics.\nDistinguish between observational and experimental studies, emphasizing the advantages experiments provide.\nLearn the fundamental techniques of replication, blocking, and randomization.\nDiscuss how factorial experiments efficiently explore multiple factors simultaneously.\nUnderstand how design decisions affect data analysis and the validity of inferential procedures.\nReinforce concepts with R-based numerical examples and demonstrate their application to real or simulated data.\nExplore advanced mathematical derivations and proofs in the Appendix.\n\n\nReadings\n\nDean et al. (2017, Ch. 1 and 2)"
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#experimentation-vs.-observation",
    "href": "lectures/week-01_intro-design_part1.html#experimentation-vs.-observation",
    "title": "Fundamentals of Experimental Design",
    "section": "Experimentation vs. Observation",
    "text": "Experimentation vs. Observation\n\nObservational studies record data without intervention, often leaving confounding factors unaccounted for. Causal interpretations are limited.\nControlled experiments, on the other hand, manipulate factor levels and apply treatments to subjects (experimental units). By controlling the allocation of treatments, experiments more reliably identify causes of variation and support causal conclusions."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#goals-of-experimentation",
    "href": "lectures/week-01_intro-design_part1.html#goals-of-experimentation",
    "title": "Fundamentals of Experimental Design",
    "section": "Goals of Experimentation",
    "text": "Goals of Experimentation\n\nDetermine causes of variation in response.\nFind optimal conditions and treatment combinations.\nCompare responses across different treatments.\nDevelop predictive models that accurately reflect the underlying process.\n\nEffective experimental design ensures that maximum information is gained while efficiently using resources. These objectives directly influence how we choose the number of observations, treatment combinations, and the structure of the experimental layout."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#importance-of-experimental-design",
    "href": "lectures/week-01_intro-design_part1.html#importance-of-experimental-design",
    "title": "Fundamentals of Experimental Design",
    "section": "Importance of Experimental Design",
    "text": "Importance of Experimental Design\n\nCause-and-Effect: Only experiments can directly assess causal relationships, unlike observational studies prone to confounding.\nResource Allocation: Deciding how many observations to collect and how to arrange them ensures the best return on time, effort, and material.\nEfficient Analysis: A well-designed experiment simplifies subsequent data analysis and makes model assumptions (e.g., distributional forms, independence) more credible."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#replication",
    "href": "lectures/week-01_intro-design_part1.html#replication",
    "title": "Fundamentals of Experimental Design",
    "section": "Replication",
    "text": "Replication\n\nDefinition: Replication means repeating the entire set of treatments independently on different experimental units.\nPurpose: Replication estimates the experimental error, allowing generalization of conclusions to a broader population of similar units.\nContrast with Repeated Measurements: Repeated measurements on the same unit (e.g., measuring the same subject multiple times) do not constitute independent replications. True replication involves distinct subjects or units.\n\nWithout adequate replication, it is difficult to separate true treatment effects from random noise."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#blocking",
    "href": "lectures/week-01_intro-design_part1.html#blocking",
    "title": "Fundamentals of Experimental Design",
    "section": "Blocking",
    "text": "Blocking\n\nConcept: Blocking is used to control extraneous variation by grouping experimental units into homogeneous subsets called blocks. Treatments are then compared within these blocks.\nExample: If environmental conditions vary across different times of day, you may form blocks of time periods and randomly assign treatments within each block. This reduces noise and increases the precision of comparisons among treatments.\nOutcome: Blocking improves the precision of inference by removing a known source of variability, making it easier to detect treatment differences."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#randomization",
    "href": "lectures/week-01_intro-design_part1.html#randomization",
    "title": "Fundamentals of Experimental Design",
    "section": "Randomization",
    "text": "Randomization\n\nRationale: Random assignment of treatments to experimental units is fundamental. It ensures that no systematic bias influences treatment allocation.\nBenefits:\n\n  - Prevents experimenter preferences or unconscious patterns from influencing results.   - Justifies the use of standard statistical distributions (F, t) for inference.   - Confirms that observed effects can be attributed to treatments and not assignment bias.\n\n\nMethods of Randomization\n\nRandom Number Generators (RNG): Computer programs or calculators produce pseudo-random digits, used to assign treatments.\nRandom Number Tables: Historically used, a random table combined with random starting points can ensure unbiased allocations.\nEnsuring True Randomness: Use objective random devices (dice, RNGs) and avoid human-chosen “random” sequences, as humans are prone to inadvertent patterns."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#example-random-assignment-and-anova",
    "href": "lectures/week-01_intro-design_part1.html#example-random-assignment-and-anova",
    "title": "Fundamentals of Experimental Design",
    "section": "Example: Random Assignment and ANOVA",
    "text": "Example: Random Assignment and ANOVA\nSuppose we have three treatments (A, B, C) and want to randomly assign them to 9 experimental units.\n\n# Set seed for reproducibility\n\nset.seed(123)\n# Treatments\n\ntreatments &lt;- c(\"A\", \"B\", \"C\")\n# We have 9 units, randomly assign treatments\n\nassignments &lt;- sample(rep(treatments, each=3))\nassignments\n\n[1] \"A\" \"B\" \"C\" \"A\" \"C\" \"B\" \"C\" \"A\" \"B\"\n\n\nThis code generates a random permutation of A, B, and C assigned to the units.\nNow assume we collected data (simulated responses):\n\n# Simulate responses with a true difference: A &lt;-  B &lt;-  C\ntrue_means &lt;- c(A = 10, B = 12, C = 15)\nresponses &lt;- rnorm(9, mean = rep(true_means, each = 3), sd = 2)\ndata &lt;- data.frame(unit = 1:9, treatment = assignments, response = responses)\ndata |&gt; head()\n\n\n\n\n\n\n\n\nunit\ntreatment\nresponse\n\n\n\n\n1\nA\n9.765516\n\n\n2\nB\n10.366165\n\n\n3\nC\n12.561110\n\n\n4\nA\n8.545459\n\n\n5\nC\n15.380369\n\n\n6\nB\n13.007625\n\n\n\n\n\n\n\nTable 1: Simulated Data\n\n\n\n\nWe can now fit an ANOVA model to test for treatment effects:\n\n\n\nfit &lt;- aov(response ~ treatment, data=data)\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ntreatment    2  32.01   16.00   1.287  0.343\nResiduals    6  74.61   12.44               \n\n\n\nTable 2: ANOVA Results\n\n\n\nThe ANOVA table will show how between-treatment variability compares to within-treatment variability. With a large enough difference and adequate replication, we expect a significant treatment effect.\nConclusion\nExperimental design is a cornerstone of scientific inquiry. By employing replication, blocking, and randomization, statisticians and researchers can draw clearer causal conclusions, improve the efficiency of experimentation, and ensure the validity of statistical tests.\nKey Takeaways:\n\nRandomization prevents systematic bias and validates inferential methods.\nReplication provides robust estimates of variability and supports generalizability.\nBlocking controls known sources of variation, improving precision.\nFactorial designs offer a more comprehensive view of multiple factors simultaneously.\n\nFuture sessions may delve deeper into specific complex designs (e.g., Latin squares, split-plots) and advanced analysis methods, ensuring students appreciate the rich interplay between design and analysis."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#references",
    "href": "lectures/week-01_intro-design_part1.html#references",
    "title": "Fundamentals of Experimental Design",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#a-proof-of-randomizations-role-in-distributional-assumptions",
    "href": "lectures/week-01_intro-design_part1.html#a-proof-of-randomizations-role-in-distributional-assumptions",
    "title": "Fundamentals of Experimental Design",
    "section": "A Proof of Randomization’s Role in Distributional Assumptions",
    "text": "A Proof of Randomization’s Role in Distributional Assumptions\nUnder a completely randomized design, every unit is equally likely to receive any treatment. Let \\(Y_{ij}\\) represent the response from the \\(j\\)-th replicate of treatment \\(i\\). Assuming \\(Y_{ij} = \\mu + \\alpha_i + \\varepsilon_{ij}\\) with \\(\\varepsilon_{ij} \\sim N(0,\\sigma^2)\\):\n\nRandomization ensures that \\((\\varepsilon_{ij})\\) are exchangeable random variables.\nUnder \\(H_0: \\alpha_i=0\\), the test statistic (e.g., ANOVA F-statistic) follows an F-distribution derived from the ratio of quadratic forms in normal variables (Scheffé, 1959).\nWithout randomization, the assumptions that justify the standard F and t distributions could be invalid, making inferences incorrect."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#a-derivation-of-expected-mean-squares",
    "href": "lectures/week-01_intro-design_part1.html#a-derivation-of-expected-mean-squares",
    "title": "Fundamentals of Experimental Design",
    "section": "A Derivation of Expected Mean Squares",
    "text": "A Derivation of Expected Mean Squares\nFor the simple one-way ANOVA model:\n\\[\nY_{ij} = \\mu + \\alpha_i + \\varepsilon_{ij},\n\\]\nwith \\(\\sum \\alpha_i = 0\\), the expected mean squares are:\n\\[\nE(MS_{\\text{Treatment}}) = \\sigma^2 + n \\sum \\alpha_i^2/(g-1),\n\\]\n\\[\nE(MS_{\\text{Error}}) = \\sigma^2.\n\\]\nThese derivations rely on properties of orthogonal decompositions of sums of squares and can be found in Montgomery (2019) and Scheffé (1959)."
  },
  {
    "objectID": "home_lectures.html",
    "href": "home_lectures.html",
    "title": "📝 Lecture Notes",
    "section": "",
    "text": "Welcome to the Lecture Notes page for STAT 454/545: Analysis of Variance and Experimental Design. Here, you will find weekly lecture notes and additional resources in HTML format, accessible via the course GitHub page.\n\n\n\n\n\nWeek\nTopic\nLecture Notes Link\n\n\n\n\n1\nIntroduction to Experimental Design\nWeek 1 Notes: Introduction to Experimental Design\n\n\n1\nIntroduction to Experimental Design\nWeek 1 Notes: Introduction to Experimental Design\n\n\n2\nCompletely Randomized Designs (CRD)\nWeek 2 Notes: Completely Randomized Designs (CRD)\n\n\n3\nMultiple Comparisons\nWeek 3 Notes: Multiple Comparisons\n\n\n4\nChecking Model Assumptions\nWeek 4 Notes: Checking Model Assumptions\n\n\n5\nTwo-Way Factorial Designs\nWeek 5 Notes: Two-Way Factorial Designs\n\n\n6\nHigher-Order Factorial Designs\nWeek 6 Notes: Higher-Order Factorial Designs\n\n\n7\nMidterm Review\nWeek 7 Notes: Midterm Review\n\n\n8\nBlocking & RCBD\nWeek 8 Notes: Blocking & RCBD\n\n\n9\nPower Analysis\nWeek 9 Notes: Power Analysis\n\n\n10\nAnalysis of Covariance (ANCOVA)\nWeek 10 Notes: Analysis of Covariance (ANCOVA) and Causal Inference\n\n\n10\nAnalysis of Covariance (ANCOVA)\nWeek 10 Notes: Analysis of Covariance (ANCOVA)\n\n\n11\nRow–Column Designs (Latin Square)\nWeek 11 Notes: Row–Column Designs (Latin Square)\n\n\n12\nRandom & Mixed Models\nWeek 12 Notes: Random & Mixed Models\n\n\n13\nNested Models\nWeek 13 Notes: Nested Models\n\n\n14\nSplit-Plot Designs\nWeek 14 Notes: Split-Plot Designs\n\n\n15\nCatch-Up and Review\nWeek 15 Notes: Catch-Up and Review\n\n\n16\nFinal Exam Review Notes\nFinal Exam Review Notes\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nWorksheet Link\n\n\n\n\n1\nIntroduction to Experimental Design\n[Week 1 Worksheet: Introduction to Experimental Design\n\n\n2\nCompletely Randomized Designs (CRD)\n[Week 2 Worksheet: Completely Randomized Designs (CRD)]\n\n\n3\nMultiple Comparisons\n[Week 3 Worksheet: Multiple Comparisons]\n\n\n4\nChecking Model Assumptions\n[Week 4 Worksheet: Model Diagnostics]\n\n\n5\nTwo-Way Factorial Designs\n[Week 5 Worksheet: Two-Way Factorial Designs]\n\n\n6\nHigher-Order Factorial Designs\n[Week 6 Worksheet: Higher-Order Factorial Designs]\n\n\n8\nBlocking & RCBD\n[Week 8 Worksheet: Blocking & RCBD]\n\n\n9\nPower Analysis\n[Week 9 Worksheet: Power Analysis]\n\n\n10\nAnalysis of Covariance (ANCOVA)\nWeek 10 Worksheet: ANCOVA\n\n\n11\nRow–Column Designs\nWeek 11 Worksheet: Row–Column Designs\n\n\n12\nRandom & Mixed Models\n[Week 12 Worksheet: Mixed Models]\n\n\n13\nNested Models\n[Week 13 Worksheet: Nested Models]\n\n\n14\nSplit-Plot Designs\n[Week 14 Worksheet: Split-Plot Designs]\n\n\n\n\n\n\nIn addition to lecture notes, supplemental materials such as examples, data files, and problem sets will be provided as needed.\n\n📁 GitHub Repository: Course Repository\n\n🔗 Supplemental Readings: Available on Canvas or shared links.\n\n\n\n\n\nPre-Class Preparation: Read the lecture notes and supplemental readings before each class to familiarize yourself with the topics.\n\nDuring Class: Focus on key points highlighted in the notes. Participate in group activities and discussions.\n\nPost-Class Review: Revisit the lecture notes and solve related assignments. Use the notes for exam preparation.\n\n\n\n\nIf you experience issues accessing the lecture notes:\n1. Refresh your browser as noted earlier.\n2. Try accessing the page in incognito mode.\n3. Clear your browser cache using the appropriate settings (Chrome, Firefox, Edge).\nIf problems persist, contact the instructor via Canvas Email for further assistance.\n\n\n\n\nInstructor: Dr. Davood Tofighi\n\nOffice Hours: By appointment\n\nCourse GitHub Page: Access Here\n\nCanvas Course Page: UNM Canvas",
    "crumbs": [
      "Home",
      "Course Materials",
      "Lecture Notes"
    ]
  },
  {
    "objectID": "home_lectures.html#weekly-lecture-notes",
    "href": "home_lectures.html#weekly-lecture-notes",
    "title": "📝 Lecture Notes",
    "section": "",
    "text": "Week\nTopic\nLecture Notes Link\n\n\n\n\n1\nIntroduction to Experimental Design\nWeek 1 Notes: Introduction to Experimental Design\n\n\n1\nIntroduction to Experimental Design\nWeek 1 Notes: Introduction to Experimental Design\n\n\n2\nCompletely Randomized Designs (CRD)\nWeek 2 Notes: Completely Randomized Designs (CRD)\n\n\n3\nMultiple Comparisons\nWeek 3 Notes: Multiple Comparisons\n\n\n4\nChecking Model Assumptions\nWeek 4 Notes: Checking Model Assumptions\n\n\n5\nTwo-Way Factorial Designs\nWeek 5 Notes: Two-Way Factorial Designs\n\n\n6\nHigher-Order Factorial Designs\nWeek 6 Notes: Higher-Order Factorial Designs\n\n\n7\nMidterm Review\nWeek 7 Notes: Midterm Review\n\n\n8\nBlocking & RCBD\nWeek 8 Notes: Blocking & RCBD\n\n\n9\nPower Analysis\nWeek 9 Notes: Power Analysis\n\n\n10\nAnalysis of Covariance (ANCOVA)\nWeek 10 Notes: Analysis of Covariance (ANCOVA) and Causal Inference\n\n\n10\nAnalysis of Covariance (ANCOVA)\nWeek 10 Notes: Analysis of Covariance (ANCOVA)\n\n\n11\nRow–Column Designs (Latin Square)\nWeek 11 Notes: Row–Column Designs (Latin Square)\n\n\n12\nRandom & Mixed Models\nWeek 12 Notes: Random & Mixed Models\n\n\n13\nNested Models\nWeek 13 Notes: Nested Models\n\n\n14\nSplit-Plot Designs\nWeek 14 Notes: Split-Plot Designs\n\n\n15\nCatch-Up and Review\nWeek 15 Notes: Catch-Up and Review\n\n\n16\nFinal Exam Review Notes\nFinal Exam Review Notes",
    "crumbs": [
      "Home",
      "Course Materials",
      "Lecture Notes"
    ]
  },
  {
    "objectID": "home_lectures.html#worksheets",
    "href": "home_lectures.html#worksheets",
    "title": "📝 Lecture Notes",
    "section": "",
    "text": "Week\nTopic\nWorksheet Link\n\n\n\n\n1\nIntroduction to Experimental Design\n[Week 1 Worksheet: Introduction to Experimental Design\n\n\n2\nCompletely Randomized Designs (CRD)\n[Week 2 Worksheet: Completely Randomized Designs (CRD)]\n\n\n3\nMultiple Comparisons\n[Week 3 Worksheet: Multiple Comparisons]\n\n\n4\nChecking Model Assumptions\n[Week 4 Worksheet: Model Diagnostics]\n\n\n5\nTwo-Way Factorial Designs\n[Week 5 Worksheet: Two-Way Factorial Designs]\n\n\n6\nHigher-Order Factorial Designs\n[Week 6 Worksheet: Higher-Order Factorial Designs]\n\n\n8\nBlocking & RCBD\n[Week 8 Worksheet: Blocking & RCBD]\n\n\n9\nPower Analysis\n[Week 9 Worksheet: Power Analysis]\n\n\n10\nAnalysis of Covariance (ANCOVA)\nWeek 10 Worksheet: ANCOVA\n\n\n11\nRow–Column Designs\nWeek 11 Worksheet: Row–Column Designs\n\n\n12\nRandom & Mixed Models\n[Week 12 Worksheet: Mixed Models]\n\n\n13\nNested Models\n[Week 13 Worksheet: Nested Models]\n\n\n14\nSplit-Plot Designs\n[Week 14 Worksheet: Split-Plot Designs]",
    "crumbs": [
      "Home",
      "Course Materials",
      "Lecture Notes"
    ]
  },
  {
    "objectID": "home_lectures.html#supplemental-resources",
    "href": "home_lectures.html#supplemental-resources",
    "title": "📝 Lecture Notes",
    "section": "",
    "text": "In addition to lecture notes, supplemental materials such as examples, data files, and problem sets will be provided as needed.\n\n📁 GitHub Repository: Course Repository\n\n🔗 Supplemental Readings: Available on Canvas or shared links.",
    "crumbs": [
      "Home",
      "Course Materials",
      "Lecture Notes"
    ]
  },
  {
    "objectID": "home_lectures.html#how-to-use-lecture-notes",
    "href": "home_lectures.html#how-to-use-lecture-notes",
    "title": "📝 Lecture Notes",
    "section": "",
    "text": "Pre-Class Preparation: Read the lecture notes and supplemental readings before each class to familiarize yourself with the topics.\n\nDuring Class: Focus on key points highlighted in the notes. Participate in group activities and discussions.\n\nPost-Class Review: Revisit the lecture notes and solve related assignments. Use the notes for exam preparation.",
    "crumbs": [
      "Home",
      "Course Materials",
      "Lecture Notes"
    ]
  },
  {
    "objectID": "home_lectures.html#troubleshooting-github-pages",
    "href": "home_lectures.html#troubleshooting-github-pages",
    "title": "📝 Lecture Notes",
    "section": "",
    "text": "If you experience issues accessing the lecture notes:\n1. Refresh your browser as noted earlier.\n2. Try accessing the page in incognito mode.\n3. Clear your browser cache using the appropriate settings (Chrome, Firefox, Edge).\nIf problems persist, contact the instructor via Canvas Email for further assistance.",
    "crumbs": [
      "Home",
      "Course Materials",
      "Lecture Notes"
    ]
  },
  {
    "objectID": "home_lectures.html#contact-information",
    "href": "home_lectures.html#contact-information",
    "title": "📝 Lecture Notes",
    "section": "",
    "text": "Instructor: Dr. Davood Tofighi\n\nOffice Hours: By appointment\n\nCourse GitHub Page: Access Here\n\nCanvas Course Page: UNM Canvas",
    "crumbs": [
      "Home",
      "Course Materials",
      "Lecture Notes"
    ]
  },
  {
    "objectID": "home_appendix.html",
    "href": "home_appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "This appendix provides supplementary resources and support materials to help you succeed in the course.",
    "crumbs": [
      "Home",
      "Resources",
      "Appendix"
    ]
  },
  {
    "objectID": "home_appendix.html#r-help-and-documentation",
    "href": "home_appendix.html#r-help-and-documentation",
    "title": "Appendix",
    "section": "R Help and Documentation",
    "text": "R Help and Documentation\nFor the whole list of R functions and packages, refer to the R help Page for the Course.",
    "crumbs": [
      "Home",
      "Resources",
      "Appendix"
    ]
  },
  {
    "objectID": "home_appendix.html#r-code-and-data-files-from-the-textbook",
    "href": "home_appendix.html#r-code-and-data-files-from-the-textbook",
    "title": "Appendix",
    "section": "R Code and Data Files from the Textbook",
    "text": "R Code and Data Files from the Textbook\nHere is a page for the R program files used in the textbook:\n\nR Code and Data Files from the Textbook\nData Files from the Textbook",
    "crumbs": [
      "Home",
      "Resources",
      "Appendix"
    ]
  },
  {
    "objectID": "home_appendix.html#statistical-topics",
    "href": "home_appendix.html#statistical-topics",
    "title": "Appendix",
    "section": "Statistical Topics",
    "text": "Statistical Topics\n\nGames-Howell post-hoc test\nStandardized Range Distribution and ANOVA\nMarginal and Conditional Effects in GLM and GLMM\nTesting Linear and Quadratic Contrasts with Unequal Variances",
    "crumbs": [
      "Home",
      "Resources",
      "Appendix"
    ]
  },
  {
    "objectID": "contents.html",
    "href": "contents.html",
    "title": "Course Contents",
    "section": "",
    "text": "The course syllabus contains all essential details about the course structure, policies, assignments, and exam dates.\n\n📄 Download Full Course Syllabus (PDF)\nImportant: You must read the syllabus carefully, as it contains critical information not available elsewhere.",
    "crumbs": [
      "Home",
      "Resources",
      "Contents"
    ]
  },
  {
    "objectID": "contents.html#course-syllabus",
    "href": "contents.html#course-syllabus",
    "title": "Course Contents",
    "section": "",
    "text": "The course syllabus contains all essential details about the course structure, policies, assignments, and exam dates.\n\n📄 Download Full Course Syllabus (PDF)\nImportant: You must read the syllabus carefully, as it contains critical information not available elsewhere.",
    "crumbs": [
      "Home",
      "Resources",
      "Contents"
    ]
  },
  {
    "objectID": "contents.html#lecture-notes",
    "href": "contents.html#lecture-notes",
    "title": "Course Contents",
    "section": "Lecture Notes",
    "text": "Lecture Notes\nLecture notes and slides will be posted on the Course GitHub Page in HTML format.\n\n📚 Access Lecture Notes\n\nNote: GitHub Pages may cache content. Refresh your browser if the notes appear outdated.",
    "crumbs": [
      "Home",
      "Resources",
      "Contents"
    ]
  },
  {
    "objectID": "contents.html#course-schedule",
    "href": "contents.html#course-schedule",
    "title": "Course Contents",
    "section": "Course Schedule",
    "text": "Course Schedule\nThe course schedule provides a week-by-week breakdown of topics, reading assignments, and due dates.\n\n📅 View Course Schedule",
    "crumbs": [
      "Home",
      "Resources",
      "Contents"
    ]
  },
  {
    "objectID": "contents.html#assignments",
    "href": "contents.html#assignments",
    "title": "Course Contents",
    "section": "Assignments",
    "text": "Assignments\n\nAccess the Assignments Page to view the list of assignments and due dates.\nAll assignments require the use of the Quarto template provided.\n\n🔗 Quarto Template",
    "crumbs": [
      "Home",
      "Resources",
      "Contents"
    ]
  },
  {
    "objectID": "contents.html#r-code-and-data-files-from-the-textbook",
    "href": "contents.html#r-code-and-data-files-from-the-textbook",
    "title": "Course Contents",
    "section": "R Code and Data Files from the Textbook",
    "text": "R Code and Data Files from the Textbook\nThe R code and data files from the textbook are available in the Appendix.",
    "crumbs": [
      "Home",
      "Resources",
      "Contents"
    ]
  },
  {
    "objectID": "contents.html#r-help-and-tutorials-provided-by-the-instructor",
    "href": "contents.html#r-help-and-tutorials-provided-by-the-instructor",
    "title": "Course Contents",
    "section": "R Help and Tutorials Provided by the Instructor",
    "text": "R Help and Tutorials Provided by the Instructor\nI provided a list of R tutorials and resources that you may find helpful in the R Help",
    "crumbs": [
      "Home",
      "Resources",
      "Contents"
    ]
  },
  {
    "objectID": "assignments/assignment8_complete_block_designs.html",
    "href": "assignments/assignment8_complete_block_designs.html",
    "title": "Assignment 8: Complete Block Designs",
    "section": "",
    "text": "Objective: Analyze complete block designs, including block-treatment interactions, post-hoc tests, and trend analyses.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 50"
  },
  {
    "objectID": "assignments/assignment8_complete_block_designs.html#instructions",
    "href": "assignments/assignment8_complete_block_designs.html#instructions",
    "title": "Assignment 8: Complete Block Designs",
    "section": "",
    "text": "Objective: Analyze complete block designs, including block-treatment interactions, post-hoc tests, and trend analyses.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 50"
  },
  {
    "objectID": "assignments/assignment8_complete_block_designs.html#insole-cushion-experiment",
    "href": "assignments/assignment8_complete_block_designs.html#insole-cushion-experiment",
    "title": "Assignment 8: Complete Block Designs",
    "section": "Insole Cushion Experiment",
    "text": "Insole Cushion Experiment\nData Source: Dean et al. (2017), Chapter 10, Exercise 15\nThis experiment, conducted in 1995 by V. Agresti, S. Decker, T. Karakostas, E. Patterson, and S. Schwartz, investigated how different insole cushions and brands affect the vertical ground reaction force.\n\nFactors and Levels:\n\nCushion Type (C): 1 (Regular), 2 (Heel)\nBrand (D): 1 (Name Brand), 2 (Store Brand)\n\nDesign:\n\nSingle participant, alternating between dominant (right) and non-dominant (left) legs over two blocks (days).\nEach of the four treatment combinations measured five times in randomized order per block.\n\nResponse Variable: Maximum deceleration of the vertical ground reaction force (in Newtons).\n\nThe data is provided in Table 10.28.\n\nData for the insole cushion experiment (Order of observation in parentheses)\n\n\n\n\n\n\n\nC\nD\nResponse in Newtons (Order)\n\n\n\n\n\n\nBlock I (Right Leg)\n\n\n1\n1\n899.99 (3), 910.81 (5), 927.79 (10), 888.77 (11), 911.93 (16)\n\n\n1\n2\n924.92 (2), 900.10 (6), 923.55 (12), 891.56 (17), 885.73 (20)\n\n\n2\n1\n888.09 (4), 954.11 (7), 937.41 (9), 911.85 (14), 908.41 (18)\n\n\n2\n2\n884.01 (1), 918.36 (8), 880.23 (13), 891.16 (15), 917.16 (19)\n\n\n\n\nBlock II (Left Leg)\n\n\n1\n1\n852.94 (22), 866.28 (27), 886.65 (28), 851.14 (33), 869.80 (34)\n\n\n1\n2\n882.95 (21), 865.58 (24), 868.15 (25), 893.82 (37), 875.98 (38)\n\n\n2\n1\n920.93 (26), 880.26 (31), 897.10 (35), 893.78 (39), 885.80 (40)\n\n\n2\n2\n872.50 (23), 892.76 (29), 895.93 (30), 899.44 (32), 912.00 (36)\n\n\n\n\nQuestions\n\nFit a Model with Interaction:\n\n\nInclude block-treatment interactions in the ANOVA model.\nCreate and interpret an analysis of variance (ANOVA) table.\n\n\nGenerate Interaction Plots:\n\n\nCreate plots for:\n\nCushion (C) × Brand (D) interaction.\nCushion (C) × Block interaction.\nBrand (D) × Block interaction.\nTreatment combination × Block interaction.\n\nIdentify and describe contrasts of interest for further analysis.\n\n\nConfidence Intervals:\n\n\nCompute confidence intervals for means and contrasts identified in (b). Use these intervals to validate observed differences or interactions.\n\n\nRe-examine the Data:\n\n\nCheck assumptions, focusing on the two potential outliers.\nRe-analyze the data excluding one or both outliers.\nDiscuss whether the conclusions differ between analyses and justify which analysis should be reported."
  },
  {
    "objectID": "assignments/assignment8_complete_block_designs.html#grading-allocation",
    "href": "assignments/assignment8_complete_block_designs.html#grading-allocation",
    "title": "Assignment 8: Complete Block Designs",
    "section": "Grading Allocation",
    "text": "Grading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nInsole Cushion Experiment\n(a)\n10\n\n\n\n(b)\n20\n\n\n\n(c)\n10\n\n\n\n(d)\n10"
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html",
    "href": "assignments/assignment6_higher_order_factorial_designs.html",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "",
    "text": "Objective: Explore higher-order factorial designs and interactions in experimental data.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 140"
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#instructions",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#instructions",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "",
    "text": "Objective: Explore higher-order factorial designs and interactions in experimental data.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 140"
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.7-coating-experiment",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.7-coating-experiment",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "Exercise 7.7: Coating Experiment",
    "text": "Exercise 7.7: Coating Experiment\nThis exercise examines the effects of spray parameters on the thermal spray coating properties of alumina (\\(\\text{Al}_2\\text{O}_3\\)). The experimental factors and levels are as follows:\n\nFuel ratio (A): 1:2.8 and 1:2.0\nCarrier gas flow rate (B): 1.33 and 3.21 \\(\\text{L/s}^{-1}\\)\nFrequency of detonations (C): 2 and 4 Hz\nSpray distance (D): 180 and 220 mm\n\nThe response variable is porosity (vol. %). Data are shown in Table 7.19.\nQuestions:\n\nDesign an Analysis\n\nAssuming negligible threeand four-factor interactions, outline the steps required to analyze the data (refer to Step (g) of the checklist in Chapter 2).\n\nCheck Model Assumptions\n\nEvaluate whether the assumptions underlying the model are valid.\n\nPerform the Analysis\n\nConduct the analysis you outlined in part (1), including interaction plots if appropriate. Clearly state your conclusions.\nTable 7.19: Data for the Coating Experiment\n\n\n\nA\nB\nC\nD\n\\(y_{ijkl}\\)\n\n\n\n\n2\n2\n2\n2\n5.95\n\n\n2\n2\n2\n1\n4.57\n\n\n2\n2\n1\n2\n4.03\n\n\n…\n…\n…\n…\n…\n\n\n\nSource: Adapted from Saravanan et al. (2001), Journal of Physics D: Applied Physics."
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.11-antifungal-antibiotic-experiment",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.11-antifungal-antibiotic-experiment",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "Exercise 7.11: Antifungal Antibiotic Experiment",
    "text": "Exercise 7.11: Antifungal Antibiotic Experiment\nThis exercise investigates the effects of incubation conditions on the yield of an antifungal antibiotic.\n\nFactors and Levels:\nIncubation temperature (A): 25, 30, and 37 \\(^\\circ \\text{C}\\)\nCarbon concentration (B): 2%, 5%, and 7.5%\nNitrogen concentration (C): 0.5%, 1%, and 3%\n\nThe response variable is the antifungal yield (measured in activity against Candida albicans). Data are shown in Table 7.23.\nQuestions:\n\nAssess Main Effects\n\nConstruct plots to assess the significance of main effects of \\(A\\), \\(B\\), and \\(C\\) on the response. What are your conclusions?\n\nInteraction Assumptions\n\nState the assumptions you made regarding interactions while analyzing main effects in part (1).\n\nTwo-Way Interactions\n\nConstruct plots to assess the significance of two-way interactions. Do they alter your conclusions from part (1)?\n\nFit a Model\n\nAssuming no three-way interaction, fit a model with all main effects and two-way interactions. Discuss the significance of the effects and compare with your conclusions from part (3).\n\nModel Diagnostics\n\nEvaluate whether the assumptions of normality and equal error variances are satisfied. Identify potential outliers.\nTable 7.23: Data for Antifungal Antibiotic Experiment\n\n\n\nA\nB\nC\n\\(y_{ijk}\\)\n\n\n\n\n25\n2\n0.5\n25.84\n\n\n25\n2\n1\n51.86\n\n\n25\n2\n3\n32.59\n\n\n30\n5\n1\n41.11\n\n\n37\n7.5\n0.5\n51.86\n\n\n…\n…\n…\n…\n\n\n\nSource: Gupte and Kulkarni (2003), Journal of Chemical Technology and Biotechnology."
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.12-antifungal-antibiotic-experiment-extended",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.12-antifungal-antibiotic-experiment-extended",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "Exercise 7.12: Antifungal Antibiotic Experiment (Extended)",
    "text": "Exercise 7.12: Antifungal Antibiotic Experiment (Extended)\nFor this exercise, consider the data from Table 7.23 but assume no negligible three-factor interaction. Modify levels of \\(A\\) and \\(B\\) so their third levels are 35 and 8, respectively, to ensure equal spacing.\nQuestions:\n\nTrend Contrasts\n\nCreate a table listing the 27 treatment combinations. Include contrast coefficients for:\n\nLinear and quadratic trends in \\(A\\) and \\(B\\)\nInteraction trends (\\(A \\times B\\): Linear-Linear, Linear-Quadratic, etc.)\n\n\nNormalize Contrasts\n\nIdentify divisors to normalize each contrast and manually compute least squares estimates for normalized Linear \\(A\\) and Quadratic \\(A\\) contrasts.\n\nOrthogonal Contrasts for \\(C\\)\n\nPropose two orthogonal contrasts for \\(C\\) to address its uneven spacing and add them to the table in part (1).\n\nOrthogonal Contrasts Analysis\n\nUse software to calculate least squares estimates for all 26 orthogonal contrasts. Generate a half-normal probability plot and interpret the results.\n\nAlternative Analysis\n\nApply the Voss and Wang method to analyze the orthogonal contrasts. Compare these findings to part (4)."
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#table-of-grades-and-rubric",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#table-of-grades-and-rubric",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "Table of Grades and Rubric",
    "text": "Table of Grades and Rubric\n\n\n\n\n\n\n\n\n\nExercise\nCriteria\nMax Points\nDescription\n\n\n\n\nExercise 7.7\nPart (a): Analysis Design\n10\nOutline the analysis steps, assumptions, and appropriate statistical methods (e.g., ANOVA, interaction plots).\n\n\n\nPart (b): Model Assumptions\n10\nAssess normality, homoscedasticity, and independence of residuals using statistical tests and plots.\n\n\n\nPart (c): Analysis and Plots\n20\nConduct ANOVA, interpret significant effects, create interaction plots, and draw meaningful conclusions.\n\n\nTotal for Exercise 7.7\n\n40\n\n\n\nExercise 7.11\nPart (a): Main Effects\n10\nConstruct main effects plots and evaluate their significance.\n\n\n\nPart (b): Interaction Assumptions\n5\nClearly state and justify interaction assumptions.\n\n\n\nPart (c): Two-Way Interactions\n15\nConstruct interaction plots, interpret results, and update conclusions from main effects analysis.\n\n\n\nPart (d): Model Fitting\n15\nFit a model including all main effects and two-way interactions, interpret significant effects.\n\n\n\nPart (e): Diagnostics\n10\nEvaluate residual diagnostics for normality, homoscedasticity, and outliers.\n\n\nTotal for Exercise 7.11\n\n55\n\n\n\nExercise 7.12\nPart (a): Trend Contrasts\n15\nCreate a table with contrast coefficients for linear, quadratic, and interaction terms.\n\n\n\nPart (b): Normalization\n10\nCompute normalization divisors and least squares estimates.\n\n\n\nPart (c): Orthogonal Contrasts\n10\nPropose and justify two orthogonal contrasts for CC.\n\n\n\nPart (d): Computer Analysis\n15\nCompute estimates, create a half-normal probability plot, and interpret results.\n\n\n\nPart (e): Voss and Wang Method\n10\nApply the method to examine orthogonal contrasts and compare with results from the half-normal plot.\n\n\nTotal for Exercise 7.12\n\n60"
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html",
    "href": "assignments/assignment4_checking_model_assumptions.html",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "",
    "text": "Objective: Evaluate the assumptions of the one-way ANOVA model and apply transformations to address violations.\nSubmit your completed assignment on Canvas by the due date.\nEnsure your submission includes all required components and is formatted correctly.\nPost your questions on Canvas or attend office hours for assistance.\n\nTotal points: 135"
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html#instructions",
    "href": "assignments/assignment4_checking_model_assumptions.html#instructions",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "",
    "text": "Objective: Evaluate the assumptions of the one-way ANOVA model and apply transformations to address violations.\nSubmit your completed assignment on Canvas by the due date.\nEnsure your submission includes all required components and is formatted correctly.\nPost your questions on Canvas or attend office hours for assistance.\n\nTotal points: 135"
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html#exercise-3-margarine-experiment-amy-l.-phelps-1987",
    "href": "assignments/assignment4_checking_model_assumptions.html#exercise-3-margarine-experiment-amy-l.-phelps-1987",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "Exercise 3: Margarine Experiment (Amy L. Phelps, 1987)",
    "text": "Exercise 3: Margarine Experiment (Amy L. Phelps, 1987)\nThe data in Table 5.16 show the melting times in seconds for three different brands of margarine (coded 1–3) and one brand of butter (coded 4). The butter was included for comparison purposes. The initial sizes and shapes of the margarine/butter pats were as similar as possible, and they were melted individually in a clean frying pan under constant heat.\nTable 5.16 provides the melting times for each brand of margarine and butter. The experimenter fitted the one-way ANOVA model (3.3.1) to the data and obtained the following results:\n\n\n\n\n\n\n\n\n\nBrand\nTimes\n\\(\\bar{Y}_i\\)\n\\(s_i\\)\n\n\n\n\n1\n167, 171, 178, 175, 184, 176, 185, 172, 178, 178\n176.4\n5.56\n\n\n2\n231, 233, 236, 252, 233, 225, 241, 248, 239, 248\n238.6\n8.66\n\n\n3\n176, 168, 171, 172, 178, 176, 169, 164, 169, 171\n171.4\n4.27\n\n\n4\n201, 199, 196, 211, 209, 223, 209, 219, 212, 210\n208.9\n8.45\n\n\n\nTasks:\n\nCheck the Equal-Variance Assumption\n\n\nEvaluate the equal-variance assumption for the model (3.3.1) using these data.\nIf the assumption is violated, choose the best transformation of the form: \\(y' = y^\\lambda\\) to stabilize the variances.\nRecheck the assumptions using the transformed data.\n\n\nCompute a Confidence Interval with Transformed Data\n\n\nUsing the transformed data from part (a), compute a 95% confidence interval comparing the average melting times for the margarines with the average melting time for the butter.\n\n\nRepeat Confidence Interval with Untransformed Data\n\n\nRepeat part (b) using the untransformed data and Satterthwaite’s approximation for unequal variances.\nCompare the results with those obtained in part (b).\n\n\nPreferred Analysis\n\n\nBased on the results from parts (b) and (c), determine which analysis you prefer and explain your reasoning."
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html#exercise-6-bicycle-experiment-debra-schomer-1987",
    "href": "assignments/assignment4_checking_model_assumptions.html#exercise-6-bicycle-experiment-debra-schomer-1987",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "Exercise 6: Bicycle Experiment (Debra Schomer, 1987)",
    "text": "Exercise 6: Bicycle Experiment (Debra Schomer, 1987)\nThe bicycle experiment investigated the crank rates required to maintain specific speeds while riding a bicycle in twelfth gear on flat ground. The chosen speeds were 5, 10, 15, 20, and 25 mph (coded 1–5). The data are provided in Table 5.19. The experimenter fitted the one-way ANOVA model (3.3.1) and plotted the standardized residuals. She noted:\n“The larger spread of data at lower speeds is due to the difficulty of maintaining such low speeds consistently in such a high gear. This also causes additional strain on the bicycle.”\nAs a result, the experimenter observed differences in variances of the error variables across treatment levels.\nTasks:\n\nEvaluate Error Variances\n\n\nPlot the standardized residuals against the fitted values (\\(\\hat{y}_i\\)).\nCompare the sample variances and evaluate the equality of error variances across treatments.\n\n\nApply a Transformation\n\n\nSelect the best transformation of the data of the form: \\(y' = y^\\lambda\\)\nUsing the transformed data, test the hypotheses that the linear and quadratic trends in crank rates due to the different speeds are negligible.\nUse an overall significance level of \\(\\alpha = 0.01\\).\n\n\nTest Trends with Untransformed Data\n\n\nRepeat part (b) using the untransformed data and Satterthwaite’s approximation for unequal variances.\n\n\nDiscuss Methods\n\n\nCompare the results obtained in parts (b) and (c).\nDiscuss the relative merits of the methods applied and provide a recommendation based on your findings.\n\n\nTable 5.19 for Exercise 6: Data for the Bicycle Experiment\n\nData for the bicycle experiment investigating crank rates at different speeds.\n\n\n\n\n\n\n\n\n\nCode\nTreatment (Speed, mph)\nCrank Rates\nCrank Rates\nCrank Rates\n\n\n\n\n1\n5\n15\n19\n22\n\n\n2\n10\n32\n34\n27\n\n\n3\n15\n44\n47\n44\n\n\n4\n20\n59\n61\n61\n\n\n5\n25\n75\n73\n75"
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html#grading-rubric",
    "href": "assignments/assignment4_checking_model_assumptions.html#grading-rubric",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\n\n\n\n\n\n\n\nExercise\nTask\nPoints Allocated\nTotal Points\nKey Learning Objective\n\n\n\n\n3\n(a) Check Equal-Variance Assumption\n15\n65\nAssess variance assumptions, identify violations, and apply suitable data transformation.\n\n\n\n(b) Confidence Interval with Transformed Data\n20\n\nCompute and interpret confidence intervals with transformed data.\n\n\n\n(c) Confidence Interval with Untransformed Data\n20\n\nUse Satterthwaite’s approximation to address unequal variances.\n\n\n\n(d) Preferred Analysis\n10\n\nEvaluate methods and justify a preferred analysis.\n\n\n6\n(a) Evaluate Error Variances\n15\n70\nAnalyze residuals and assess variance equality.\n\n\n\n(b) Transformation and Hypothesis Testing\n25\n\nApply data transformations and test for trends with appropriate significance levels.\n\n\n\n(c) Untransformed Data Hypothesis Testing\n20\n\nConduct hypothesis testing using untransformed data with unequal variance adjustments.\n\n\n\n(d) Compare Methods\n10\n\nCompare merits of different analytical approaches.\n\n\n\n\nTotal Points Available: 135"
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html",
    "href": "assignments/assignment2_completely_randomized_design.html",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "",
    "text": "Complete all questions below.\nUse the R programming language for computational problems. Include annotated R code and relevant output.\nProvide clear and concise explanations and interpretations for all tasks.\n\nTotal points: 100"
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html#instructions",
    "href": "assignments/assignment2_completely_randomized_design.html#instructions",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "",
    "text": "Complete all questions below.\nUse the R programming language for computational problems. Include annotated R code and relevant output.\nProvide clear and concise explanations and interpretations for all tasks.\n\nTotal points: 100"
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html#set-1-foundational-questions-suitable-for-undergraduate-and-graduate-students",
    "href": "assignments/assignment2_completely_randomized_design.html#set-1-foundational-questions-suitable-for-undergraduate-and-graduate-students",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "Set 1: Foundational Questions (Suitable for Undergraduate and Graduate Students)",
    "text": "Set 1: Foundational Questions (Suitable for Undergraduate and Graduate Students)\n\n1.1 Balloon Experiment\nData Source: Dean et al. (2017), Chapter 3, Exercise 12\nData Overview: The data for this experiment is provided below:\n\nBalloon inflation times for different colors\n\n\nTime Order\nCoded Color\nInflation Time (seconds)\n\n\n\n\n1\n1\n22.0\n\n\n2\n3\n24.6\n\n\n3\n1\n20.3\n\n\n4\n4\n19.8\n\n\n5\n3\n24.3\n\n\n6\n2\n22.2\n\n\n7\n2\n28.5\n\n\n8\n2\n25.7\n\n\n9\n3\n20.2\n\n\n10\n1\n19.6\n\n\n11\n2\n28.8\n\n\n12\n4\n24.0\n\n\n13\n4\n17.1\n\n\n14\n4\n19.3\n\n\n15\n3\n24.2\n\n\n16\n1\n15.8\n\n\n17\n2\n18.3\n\n\n18\n1\n17.5\n\n\n19\n4\n18.7\n\n\n20\n3\n22.9\n\n\n21\n1\n16.3\n\n\n22\n4\n14.0\n\n\n23\n4\n16.6\n\n\n24\n2\n18.1\n\n\n25\n2\n18.9\n\n\n26\n4\n16.0\n\n\n27\n2\n20.1\n\n\n28\n3\n22.5\n\n\n29\n3\n16.0\n\n\n30\n1\n19.3\n\n\n31\n1\n15.9\n\n\n32\n3\n20.3\n\n\n\nThe codes for balloon colors are as follows:\n\n1 = Pink\n2 = Yellow\n3 = Orange\n4 = Blue\n\nTasks:\n\n\nPlot inflation time versus balloon color and comment on the results.\n\n\nEstimate the mean inflation time for each balloon color and add these estimates to the plot from part (a).\n\n\nConstruct an analysis of variance (ANOVA) table and test the hypothesis that balloon color has no effect on inflation time (\\(\\alpha = 0.05\\)).\n\n\nPlot the data for each color in the order it was collected. Discuss whether the assumptions of the ANOVA model (e.g., normality, independence) appear satisfied.\n\n\nEvaluate whether the analysis in part (c) is adequate. If not, suggest improvements.\n\n\n\n\n1.2 Conceptual Question\n\nDiscuss how increasing the number of replicates affects the power of a one-way ANOVA test.\n\n\n\n1.3 Graphical Analysis\n\nCreate a boxplot of inflation times for each balloon color. Based on the plot, comment on visual evidence of differences among the treatments."
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html#set-2-advanced-computational-and-proof-based-questions-graduate-level",
    "href": "assignments/assignment2_completely_randomized_design.html#set-2-advanced-computational-and-proof-based-questions-graduate-level",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "Set 2: Advanced Computational and Proof-Based Questions (Graduate Level)",
    "text": "Set 2: Advanced Computational and Proof-Based Questions (Graduate Level)\n\n2.1 Derivation of \\(t\\)-Statistic for Contrasts in ANOVA\nData Source: Christensen (2018)., Exercise 12.7.12\nIn a one-way ANOVA, the sum of squares for error (\\(SSE\\)) divided by the population variance (\\(\\sigma^2\\)) follows a chi-squared distribution with degrees of freedom equal to \\(df_E\\):\n\\[\n\\frac{SSE}{\\sigma^2} \\sim \\chi^2(df_E),\n\\]\nand the mean square error (\\(MSE\\)) is independent of all sample means (\\(\\bar{y}_i\\)). Show that the following expression follows a \\(t\\)-distribution with \\(df_E\\) degrees of freedom:\n\\[\n\\frac{\\sum_{i=1}^a \\lambda_i \\bar{y}_i - \\sum_{i=1}^a \\lambda_i \\mu_i}{\\sqrt{MSE \\sum_{i=1}^a \\frac{\\lambda_i^2}{N_i}}} \\sim t(df_E).\n\\]\nTasks:\n\nDerive the above expression step-by-step, starting with the variance of the contrast \\(\\sum_{i=1}^a \\lambda_i \\bar{y}_i\\).\nExplain why \\(MSE\\) serves as an unbiased estimate of \\(\\sigma^2\\).\nUsing the result, construct a hypothesis test for a contrast in a one-way ANOVA and provide a practical interpretation of the result.\n\n\n\n2.2 Implications of Assumption Violations\n\nDiscuss the implications of violating the assumption of homogeneity of variances in a one-way ANOVA. Suggest two methods to address such violations.\n\n\n\n2.3 Power Analysis\n\nCalculate the sample size required for a one-way ANOVA with four groups to detect a medium effect size (\\(f = 0.25\\)) at a significance level of \\(\\alpha = 0.05\\) and a power of \\(1 - \\beta = 0.8\\)."
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html#grading-rubric",
    "href": "assignments/assignment2_completely_randomized_design.html#grading-rubric",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\nTask\nPoints\n\n\n\n\nSet 1: Foundational Questions\n50\n\n\n1.1 Balloon Experiment\n30\n\n\n1.2 Conceptual Question\n10\n\n\n1.3 Graphical Analysis\n10\n\n\nSet 2: Advanced Questions\n50\n\n\n2.1 Derivation of \\(t\\)-Statistic\n20\n\n\n2.2 Implications of Assumption Violations\n10\n\n\n2.3 Power Analysis\n20\n\n\nTotal\n100"
  },
  {
    "objectID": "assignments/assignment13_split_plot_designs.html",
    "href": "assignments/assignment13_split_plot_designs.html",
    "title": "Assignment 13 - Split Plot Designs",
    "section": "",
    "text": "Objective: Analyze split-plot designs, including whole-plot and split-plot effects, interaction plots, and post-hoc tests.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\ntotal: 100 points"
  },
  {
    "objectID": "assignments/assignment13_split_plot_designs.html#instructions",
    "href": "assignments/assignment13_split_plot_designs.html#instructions",
    "title": "Assignment 13 - Split Plot Designs",
    "section": "",
    "text": "Objective: Analyze split-plot designs, including whole-plot and split-plot effects, interaction plots, and post-hoc tests.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\ntotal: 100 points"
  },
  {
    "objectID": "assignments/assignment13_split_plot_designs.html#drug-experiment",
    "href": "assignments/assignment13_split_plot_designs.html#drug-experiment",
    "title": "Assignment 13 - Split Plot Designs",
    "section": "19.1 Drug Experiment",
    "text": "19.1 Drug Experiment\nData Source: Dean et al. (2017), Chapter 19, Exercise 1\nAn experiment designed as a split-plot design evaluated eight drugs (Factor \\(A\\)) for treating arthritis. The second factor was dose (Factor \\(B\\)) at 2 levels, and the third was time (Factor \\(C\\)) at 2 levels. The experimental units were 64 rats, and the response was the amount of fluid (in mL) measured in the pleural cavity. The experiment was structured into blocks, whole plots, and split plots:\n\nBlocks: Two blocks of size 32, measured on two separate days.\nWhole Plots: Each block was divided into four whole plots of size 8.\nSplit Plots: Each whole plot was subdivided, with one drug assigned to each rat.\n\nThe logarithms of the response data were used for analysis.\n\n\n\n\n\n\n\n\n\n\n\nWhole Plot\nBlock\nDose (\\(B\\))\nTime (\\(C\\))\nDrug (\\(A\\))\nData\n\n\n\n\n1\nBlock I\n1\n1\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.7, 8.6, 6.9, 6.6, 6.7, 7.4, 5.7, 6.7\\)\n\n\n2\nBlock I\n1\n2\n1, 2, 3, 4, 5, 6, 7, 8\n\\(8.4, 9.6, 9.3, 11.1, 12.5, 8.7, 9.3, 9.5\\)\n\n\n3\nBlock I\n2\n1\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.1, 7.2, 6.8, 6.4, 6.6, 8.7, 6.7, 7.0\\)\n\n\n4\nBlock I\n2\n2\n1, 2, 3, 4, 5, 6, 7, 8\n\\(7.3, 8.7, 7.9, 6.9, 8.9, 9.5, 8.3, 11.3\\)\n\n\n5\nBlock II\n1\n1\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.8, 6.8, 7.0, 8.5, 7.8, 7.3, 6.4, 8.5\\)\n\n\n6\nBlock II\n1\n2\n1, 2, 3, 4, 5, 6, 7, 8\n\\(9.1, 10.8, 6.9, 12.2, 9.9, 10.4, 10.6, 10.5\\)\n\n\n7\nBlock II\n2\n1\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.4, 7.9, 8.0, 6.4, 8.4, 7.1, 6.4, 7.2\\)\n\n\n8\nBlock II\n2\n2\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.3, 10.4, 8.2, 8.1, 10.9, 9.8, 8.4, 14.6\\)\n\n\n\n\nQuestions:\n\nWrite out a model for this experiment.\nCalculate an analysis of variance table using the logarithms of the data. Distinguish between the effects measured on the whole plots and those measured on the split plots. Identify the whole-plot error and split-plot error.\nTest any hypotheses of interest and state your conclusions clearly.\nExamine interaction plots of any important interactions. Calculate a set of 95% confidence intervals for the differences between pairs of drugs. State your conclusions."
  },
  {
    "objectID": "assignments/assignment13_split_plot_designs.html#fishing-line-experiment",
    "href": "assignments/assignment13_split_plot_designs.html#fishing-line-experiment",
    "title": "Assignment 13 - Split Plot Designs",
    "section": "19.2 Fishing Line Experiment",
    "text": "19.2 Fishing Line Experiment\nData Source: Dean et al. (2017), Chapter 19, Exercise 2\nThe experiment compared the strengths of two brands of fishing line exposed to two stress levels (stressed, \\(S\\); non-stressed, \\(N\\)). Each brand corresponded to two reels (whole plots), and each reel provided four sections of fishing line (split plots). The response was the weight (lbs) required to break the line.\n\n\n\n\n\n\n\n\n\nWhole Plot\nBrand (\\(A\\))\nStress Level (\\(B\\))\nData (Weight in lbs)\n\n\n\n\n1\n1\nN, S, S, N\n\\(6.70, 6.40, 7.20, 7.00\\)\n\n\n2\n2\nS, S, N, N\n\\(8.10, 8.90, 8.00, 6.10\\)\n\n\n3\n2\nS, S, N, N\n\\(8.00, 8.00, 8.75, 8.50\\)\n\n\n4\n1\nN, S, N, S\n\\(8.50, 9.50, 9.70, 9.40\\)\n\n\n\n\nQuestions:\n\nWrite down a model for this experiment.\nConstruct an analysis of variance table and test the hypotheses of interest. State your conclusions.\nIf you were to repeat this experiment, suggest improvements.\n\n\n\nGrading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nDrug Experiment\n(a)\n10\n\n\n\n(b)\n15\n\n\n\n(c)\n10\n\n\n\n(d)\n15\n\n\nFishing Line Experiment\n(a)\n10\n\n\n\n(b)\n15\n\n\n\n(c)\n10\n\n\n\nTotal Score: 100%"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "",
    "text": "Objective: Analyze random and mixed effects models, including random effects, interactions, and post-hoc tests.\nUse R tool to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 60"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html#instructions",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html#instructions",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "",
    "text": "Objective: Analyze random and mixed effects models, including random effects, interactions, and post-hoc tests.\nUse R tool to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 60"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html#buttermilk-biscuit-experiment",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html#buttermilk-biscuit-experiment",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "12.4 Buttermilk Biscuit Experiment",
    "text": "12.4 Buttermilk Biscuit Experiment\nData Source: Dean et al. (2017), Chapter 12, Exercise 4\nThe buttermilk biscuit experiment was conducted by Stacie Taylor in 1995 to compare three brands of refrigerated buttermilk biscuits (Factor \\(A\\), 3 levels, fixed effect) on their fluffiness. The biscuits were baked on a tray for 7 minutes at 425°F, with 6 biscuits baked at a time. The experiment was run as a general complete block design with blocks of size \\(k = 6\\).\n\nQuestions\n\nUse a mixed model with interaction to represent the data, where the random effect represents the block (run of the oven) and the fixed effect represents the biscuit brand. Write out the model including all assumptions.\nCheck the assumptions of the model for the data in Table 17.20 as far as possible.\nWrite out the expected mean squares for all terms in the model.\nCreate a block × brand interaction plot for the blocks observed in the experiment.\nTest the hypothesis that the variance in height due to block × brand interactions is negligible. Interpret your conclusions in terms of the plot in part (d).\nCalculate a set of 95% simultaneous confidence intervals for the pairwise comparisons between the brands. State your conclusions.\n\n\nTable 17.20: Biscuit Experiment Data\n\n\n\n\n\n\n\n\n\n\n\nBlock\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n2 (150.0)\n1 (188.2)\n2 (177.8)\n3 (166.7)\n3 (187.5)\n1 (182.4)\n\n\n2\n1 (183.3)\n2 (183.3)\n2 (183.3)\n3 (176.5)\n1 (160.0)\n3 (187.5)\n\n\n3\n1 (178.9)\n3 (182.4)\n2 (193.8)\n3 (176.5)\n2 (188.9)\n1 (188.9)\n\n\n4\n2 (177.8)\n1 (145.5)\n3 (155.0)\n1 (173.7)\n3 (200.0)\n2 (187.5)\n\n\n5\n1 (205.6)\n3 (188.2)\n3 (142.9)\n2 (161.9)\n2 (177.8)\n1 (159.1)"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html#grading-allocation",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html#grading-allocation",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "Grading Allocation",
    "text": "Grading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nBiscuit Experiment\n(a)\n10\n\n\n\n(b)\n10\n\n\n\n(c)\n10\n\n\n\n(d)\n10\n\n\n\n(e)\n10\n\n\n\n(f)\n10"
  },
  {
    "objectID": "appendix/stat-topics/testing-linear-quad-unequal-var.html",
    "href": "appendix/stat-topics/testing-linear-quad-unequal-var.html",
    "title": "Testing Linear and Quadratic Trends with Unequal Variance in ANOVA",
    "section": "",
    "text": "Concept: In ANOVA, when groups have a natural order (e.g., drug doses: low, medium, high), we often test if the outcome follows a linear or quadratic trend. A linear trend implies a steady increase/decrease across groups, while a quadratic trend suggests a U-shaped or inverted U-shaped pattern.\nProblem with Unequal Variance: Standard ANOVA assumes equal variance across groups. When variances differ (heteroscedasticity), the usual F-test becomes unreliable. For example, groups with larger variances may dominate the analysis, leading to incorrect conclusions.\nSolution (Satterthwaite’s Approximation): Satterthwaite’s method adjusts the degrees of freedom (DF) for the t-test used to evaluate contrasts (e.g., linear/quadratic trends). This adjustment accounts for unequal variances, producing a more accurate p-value. Think of it as “balancing” the influence of each group based on its variability."
  },
  {
    "objectID": "appendix/stat-topics/testing-linear-quad-unequal-var.html#intuitive-understanding",
    "href": "appendix/stat-topics/testing-linear-quad-unequal-var.html#intuitive-understanding",
    "title": "Testing Linear and Quadratic Trends with Unequal Variance in ANOVA",
    "section": "",
    "text": "Concept: In ANOVA, when groups have a natural order (e.g., drug doses: low, medium, high), we often test if the outcome follows a linear or quadratic trend. A linear trend implies a steady increase/decrease across groups, while a quadratic trend suggests a U-shaped or inverted U-shaped pattern.\nProblem with Unequal Variance: Standard ANOVA assumes equal variance across groups. When variances differ (heteroscedasticity), the usual F-test becomes unreliable. For example, groups with larger variances may dominate the analysis, leading to incorrect conclusions.\nSolution (Satterthwaite’s Approximation): Satterthwaite’s method adjusts the degrees of freedom (DF) for the t-test used to evaluate contrasts (e.g., linear/quadratic trends). This adjustment accounts for unequal variances, producing a more accurate p-value. Think of it as “balancing” the influence of each group based on its variability."
  },
  {
    "objectID": "appendix/stat-topics/testing-linear-quad-unequal-var.html#mathematical-foundations",
    "href": "appendix/stat-topics/testing-linear-quad-unequal-var.html#mathematical-foundations",
    "title": "Testing Linear and Quadratic Trends with Unequal Variance in ANOVA",
    "section": "Mathematical Foundations",
    "text": "Mathematical Foundations\n\nLinear And Quadratic Contrasts\nFor \\(k\\) groups with means \\(\\bar{y}_1, \\dots, \\bar{y}_k\\), define contrasts using coefficients \\(c_i\\):\n\nLinear: \\(c_i = \\text{linear scaling (e.g., -1, 0, 1)}\\)\nQuadratic: \\(c_i = \\text{quadratic scaling (e.g., 1, -2, 1)}\\)\n\nContrast Estimate:\n\\[\n\\psi = \\sum_{i=1}^k c_i \\bar{y}_i\n\\]\nStandard Error (Unequal Variances):\n\\[\nSE(\\psi) = \\sqrt{\\sum_{i=1}^k \\frac{c_i^2 s_i^2}{n_i}}\n\\]\nwhere \\(s_i^2\\) is the variance of group \\(i\\), and \\(n_i\\) is its sample size.\nSatterthwaite’s Degrees of Freedom:\n\\[\n\\nu = \\frac{\\left( \\sum_{i=1}^k \\frac{c_i^2 s_i^2}{n_i} \\right)^2}{\\sum_{i=1}^k \\frac{c_i^4 s_i^4}{n_i^2 (n_i - 1)}}\n\\]\nThis formula approximates the effective DF by combining group-specific variances, derived from Welch’s t-test extension.\nTest Statistic:\n\\[\nt = \\frac{\\psi}{SE(\\psi)} \\sim t_{\\nu}\n\\]\nKey Theorems:\n\nWelch-Satterthwaite Equation: Approximates the distribution of a weighted sum of variances as a scaled chi-square distribution.\nCentral Limit Theorem: Justifies using the t-distribution for large samples."
  },
  {
    "objectID": "appendix/stat-topics/testing-linear-quad-unequal-var.html#numerical-example-in-r",
    "href": "appendix/stat-topics/testing-linear-quad-unequal-var.html#numerical-example-in-r",
    "title": "Testing Linear and Quadratic Trends with Unequal Variance in ANOVA",
    "section": "Numerical Example in R",
    "text": "Numerical Example in R\nData Generation: Simulate three groups with unequal variances:\n\nset.seed(123)\ngroup &lt;- factor(rep(1:3, each = 5))\ny &lt;- c(\n    rnorm(5, 10, 1), # Group 1: μ=10, σ=1\n    rnorm(5, 12, 2), # Group 2: μ=12, σ=2\n    rnorm(5, 15, 3)\n) # Group 3: μ=15, σ=3\n\nManual Calculation:\n\n# Group statistics\nmeans &lt;- tapply(y, group, mean) # 10.19357 11.91136 15.92371\nvars &lt;- tapply(y, group, var) # 0.6577564 5.4148322 3.6802184\nns &lt;- tapply(y, group, length) # 5 5 5\n\n# Linear contrast coefficients (from contr.poly)\nc_linear &lt;- contr.poly(3)[, 1] # [-0.707, 0, 0.707]\n\n# Contrast estimate\ncontrast_est &lt;- sum(c_linear * means) # ≈ 4.051817\n\n# Standard error\nse &lt;- sqrt(sum((c_linear^2 * vars) / ns)) # 0.658633\n\n# Satterthwaite DF\ndf_num &lt;- (sum((c_linear^2 * vars) / ns))^2\ndf_den &lt;- sum((c_linear^4 * vars^2) / (ns^2 * (ns - 1)))\ndf_satt &lt;- df_num / df_den # ≈ 5.6\n\n# t-test\nt_ratio &lt;- contrast_est / se # ≈ 6.15\np_value &lt;- 2 * pt(abs(t_ratio), df_satt, lower.tail = FALSE)\n\n# output create a table\ndata.frame(contrast_est, se, df_satt, t_ratio, p_value)\n\n\n\n\n\ncontrast_est\nse\ndf_satt\nt_ratio\np_value\n\n\n\n\n4.051817\n0.658633\n5.385561\n6.151859\n0.0012627\n\n\n\n\n\n\n\n\n\n\npacman::p_load(emmeans)\nlibrary(nlme)\n\n# Fit a model allowing unequal variances\nmodel_gls &lt;- gls(y ~ group, weights = varIdent(form = ~ 1 | group))\nemm &lt;- emmeans(model_gls, ~group)\n\n# Test linear trend\ncontrast(emm, method = \"poly\", adjust = \"none\")\n\n contrast  estimate    SE   df t.ratio p.value\n linear        5.73 0.931 5.39   6.152  0.0013\n quadratic     2.29 2.280 5.60   1.006  0.3558\n\nDegrees-of-freedom method: satterthwaite \n\n\n\nTable 1: Conducting a trend test with unequal variance"
  },
  {
    "objectID": "appendix/stat-topics/testing-linear-quad-unequal-var.html#comprehension-questions-appendix",
    "href": "appendix/stat-topics/testing-linear-quad-unequal-var.html#comprehension-questions-appendix",
    "title": "Testing Linear and Quadratic Trends with Unequal Variance in ANOVA",
    "section": "Comprehension Questions (Appendix)",
    "text": "Comprehension Questions (Appendix)\nConceptual:\n\nWhy does unequal variance violate ANOVA assumptions?\nHow does Satterthwaite’s approximation improve trend testing?\n\nComputational: 3. Calculate the standard error for a quadratic contrast with coefficients \\((1, -2, 1)\\), variances \\((4, 1, 9)\\), and \\(n_i = 5\\).\nApplication: 4. Interpret the R output: \\(t = 2.5, df = 8.3, p = 0.03\\)."
  },
  {
    "objectID": "appendix/stat-topics/testing-linear-quad-unequal-var.html#detailed-solutions",
    "href": "appendix/stat-topics/testing-linear-quad-unequal-var.html#detailed-solutions",
    "title": "Testing Linear and Quadratic Trends with Unequal Variance in ANOVA",
    "section": "Detailed Solutions",
    "text": "Detailed Solutions\n1. Answer: Unequal variances distort the pooled variance estimate, inflating Type I/II errors. Groups with larger variances dominate the error term, reducing test reliability.\n2. Answer: Satterthwaite’s method adjusts the DF to account for heterogeneous variances, ensuring the t-test better approximates the true sampling distribution.\n3. Solution:\n\\[\nSE = \\sqrt{\\frac{1^2 \\cdot 4}{5} + \\frac{(-2)^2 \\cdot 1}{5} + \\frac{1^2 \\cdot 9}{5}} = \\sqrt{\\frac{4 + 4 + 9}{5}} = \\sqrt{3.4} \\approx 1.84\n\\]\n4. Answer: The linear trend is significant (\\(t(8.3) = 2.5, p = 0.03\\)), suggesting a steady increase/decrease across groups despite unequal variances."
  },
  {
    "objectID": "appendix/stat-topics/marginal-conditional-effects.html",
    "href": "appendix/stat-topics/marginal-conditional-effects.html",
    "title": "Marginal and Conditional Effects in GLM and GLMM",
    "section": "",
    "text": "Below is an integrated overview of marginal versus conditional effects, highlighting how each concept is defined and interpreted in statistics, biostatistics, and social science contexts. We then consider how the notion of “effect” changes when moving from linear to non-linear models."
  },
  {
    "objectID": "appendix/stat-topics/marginal-conditional-effects.html#marginal-vs.-conditional-effects",
    "href": "appendix/stat-topics/marginal-conditional-effects.html#marginal-vs.-conditional-effects",
    "title": "Marginal and Conditional Effects in GLM and GLMM",
    "section": "1. Marginal vs. Conditional Effects",
    "text": "1. Marginal vs. Conditional Effects\n\n1.1 In General (Statistics)\n\nMarginal Effect\n\nDefinition: A marginal effect is usually understood as an overall or population-averaged effect of a predictor on an outcome, either by:\n\nIgnoring other variables entirely, or\nAveraging (integrating) over the distribution of other variables or random effects.\n\nUsage:\n\nIn a simple regression setting (e.g., ordinary least squares with one predictor \\(x\\)), the marginal effect is often the slope from regressing \\(y\\) on \\(x\\) alone (i.e., not conditioning on other covariates).\nIn more complex settings (especially with mixed models or multi-level models), marginal effects often refer to population-averaged effects when random effects or other covariates have been integrated out.\n\n\nConditional Effect\n\nDefinition: A conditional effect is the partial or subject-specific effect of a predictor on the outcome given specific values (or levels) of the other covariates (and possibly given a specific value of any random effect).\nUsage:\n\nIn a multiple regression with predictors \\(\\{x_1, x_2, \\ldots\\}\\), the coefficient \\(\\beta_1\\) is typically referred to as the conditional effect of \\(x_1\\) on \\(y\\), holding all other covariates fixed.\nIn multi-level or longitudinal models, “conditional” often means “subject-specific,” as in a random-intercept or random-slope model.\n\n\n\n\nWhy the Terminology Matters\n\nMarginal = Averaged over or ignoring other variables (or random effects).\nConditional = Given or holding constant other variables (or random effects).\n\n\n\n\n1.2 Biostatistics\nIn biostatistics, the terms marginal and conditional often surface in the context of Generalized Estimating Equations (GEE) and mixed-effects GLMs for correlated data:\n\nMarginal (Population-Averaged) Models\n\nFit via GEE (Generalized Estimating Equations).\nThe coefficients represent population-averaged (or population-level) effects.\nExample: A logistic GEE for binary outcomes yields marginal odds ratios, which interpret changes in odds at the population level.\n\nConditional (Subject-Specific) Models\n\nTypically fit via mixed-effects (random effects) models (e.g., generalized linear mixed models, GLMMs).\nCoefficients represent subject-specific (conditional) effects, i.e., the effect of a predictor on the outcome within a particular subject, conditioning on that subject’s random intercept (and/or slope).\nExample: A logistic mixed model yields subject-specific odds ratios, meaning how the odds change within an individual, controlling for that individual’s random effect.\n\n\nInterpretation Differences - A marginal odds ratio (or risk ratio, or difference) for a binary outcome can differ significantly from a subject-specific odds ratio in non-linear models because averaging on the link scale does not equal averaging on the response scale. This is sometimes referred to as the “ecological” or “Simpson’s paradox”-type phenomena when aggregated or marginal relationships differ from within-subject or conditional relationships.\n\n\n1.3 Social Science\nIn social science research (e.g., econometrics, sociology, political science):\n\nMarginal Effects\n\nOften refers to a partial derivative (or discrete difference) of the outcome with respect to a predictor, averaged across the observed data or at specific “typical” values of other covariates.\nIn nonlinear models (e.g., logistic or probit models), researchers frequently calculate “average marginal effects” (AMEs) or “marginal effects at the mean” (MEMs) to communicate how a 1-unit change in \\(x\\) affects the probability of an outcome on average in the population.\n\nConditional Effects\n\nTypically refers to the partial effect of a predictor on the outcome given certain levels of other predictors—commonly expressed as “holding other things constant.”\nFor interaction terms, “conditional effect” highlights how the effect of one variable depends on a particular level of another variable.\n\n\nMoreover, in social science, “effect” often implies a causal interpretation if certain identification assumptions are met (e.g., from a randomized experiment or strong quasi-experimental design). However, without these assumptions, “effect” may be used loosely to mean an association controlling for other covariates."
  },
  {
    "objectID": "appendix/stat-topics/marginal-conditional-effects.html#what-does-effect-mean-in-each-field",
    "href": "appendix/stat-topics/marginal-conditional-effects.html#what-does-effect-mean-in-each-field",
    "title": "Marginal and Conditional Effects in GLM and GLMM",
    "section": "2. What Does “Effect” Mean in Each Field?",
    "text": "2. What Does “Effect” Mean in Each Field?\n\nStatistics\n\n“Effect” can be purely descriptive: the change in the expected value (or another parameter, like odds or rate) of the response given a change in the predictor. Causality is not necessarily implied unless explicitly stated (e.g., in a causal inference framework).\n\nBiostatistics\n\nOften concerned with treatment effects (drug vs. placebo) or exposure effects (risk factor vs. no risk factor). The term “effect” can be used in a causal sense, especially in clinical trial contexts where randomization helps identify a cause–effect relationship.\nStill, “marginal” vs. “subject-specific” effect might differ: the “marginal (population-averaged) effect” is the difference (or ratio) in the population outcome if the entire population were exposed vs. unexposed; the “conditional (subject-specific) effect” is the difference for a given individual, possibly with random intercepts accounted for.\n\nSocial Science\n\nThe language of “effect” is often connected to causal interpretations, but many observational studies are correlational.\nResearchers often talk about “marginal effects” (especially in logistic or probit regression) as the difference in predicted probability of an event for a 1-unit change in a predictor, either at average values of other variables or averaged across the sample distribution."
  },
  {
    "objectID": "appendix/stat-topics/marginal-conditional-effects.html#how-the-definition-changes-for-non-linear-models",
    "href": "appendix/stat-topics/marginal-conditional-effects.html#how-the-definition-changes-for-non-linear-models",
    "title": "Marginal and Conditional Effects in GLM and GLMM",
    "section": "3. How the Definition Changes for Non-Linear Models",
    "text": "3. How the Definition Changes for Non-Linear Models\n\n3.1 Linear vs. Non-Linear\n\nIn a linear model \\(y = X\\beta + \\varepsilon\\):\n\nThe effect of \\(\\beta_j\\) is literally the amount \\(y\\) changes on average per 1-unit change in \\(x_j\\), holding other \\(x\\)’s constant.\nMarginal vs. conditional effects in a simple linear model can sometimes coincide if the model is fully specified (no omitted confounders, etc.). However, once you bring random effects or partialing out other covariates, the distinction can still appear (subject-specific vs. population-level interpretation).\n\nIn a non-linear model (e.g., logistic, Poisson, probit):\n\nLink Function: The coefficient \\(\\beta_j\\) is the change in the log-odds (logit), log of rate (Poisson), or another link-scale quantity, not necessarily the change in the raw outcome scale.\nMarginal vs. Conditional:\n\nMarginal (Population-Averaged) Effect: Often found by integrating out random effects or averaging across the distribution of covariates. In logistic regression, this might be the average change in predicted probability for a 1-unit change in \\(x_j\\) across the population.\nConditional (Subject-Specific) Effect: The change in the linear predictor (e.g., log-odds) for a 1-unit change in \\(x_j\\) within an individual or holding all covariates (and random effects) fixed at certain values.\n\n\n\nKey Point: For non-linear models, the difference between marginal and conditional can be much larger than in linear models, because\n\\[\n\\mathbb{E}[g^{-1}(\\eta)] \\;\\neq\\; g^{-1}(\\mathbb{E}[\\eta]),\n\\]\nand thus integrating the nonlinear link function can yield fundamentally different “average” effects compared to those conditional on specific levels.\n\n\n3.2 Example: Logistic Regression\n\nSubject-Specific (Conditional) Model\n\nA random-intercept logistic model:\n\n\n\\[\n\\text{logit}(P(Y_{i}=1 \\mid X_{i}, b_i)) = \\beta_0 + \\beta_1 X_{i} + b_i.\n\\]\n\n\\(\\beta_1\\) is the log-odds change for a 1-unit change in \\(X_i\\) within an individual, conditioning on that individual’s random effect \\(b_i\\).\n\n\nMarginal (Population-Averaged) Model\n\nA GEE approach or a marginal logistic regression:\n\n\n\\[\n\\text{logit}(P(Y_{i}=1 \\mid X_{i})) = \\alpha_0 + \\alpha_1 X_{i},\n\\]\n\n\\(\\alpha_1\\) is the log-odds change averaged over the distribution of random intercepts.\n\nBecause of the non-linearity of the logit function, \\(\\alpha_1\\) and \\(\\beta_1\\) typically differ: the population-averaged effect on the odds of \\(Y=1\\) is not the same as the subject-specific effect within an individual."
  },
  {
    "objectID": "appendix/stat-topics/marginal-conditional-effects.html#summary-highlights",
    "href": "appendix/stat-topics/marginal-conditional-effects.html#summary-highlights",
    "title": "Marginal and Conditional Effects in GLM and GLMM",
    "section": "4. Summary Highlights",
    "text": "4. Summary Highlights\n\nMarginal Effects\n\nPopulation-level interpretation.\n“Average” effect when other factors are not held fixed or are integrated out (in random-effects settings).\n\nConditional Effects\n\nSubject-specific or partial interpretation.\nEffect given specific values of other covariates (and random effects, if any).\n\nDifferent Disciplines, Different Emphases\n\nStatistics: Tends to emphasize whether you are conditioning on or marginalizing over random effects or covariates.\nBiostatistics: Clear distinction between population-averaged (GEE) vs. subject-specific (mixed-effects) interpretations, especially for repeated-measures or clustered data.\nSocial Science: Focus on marginal effects in logistic or probit models (at means or averaged over the sample) versus conditional effects with other variables “held constant.” Also, “effect” language often implies causal interpretation if the design justifies it.\n\nNon-Linear Models\n\nThe difference between marginal and conditional effects can be large because of the link function.\nOne cannot simply interpret a coefficient on the link scale (e.g., log-odds) as the change in probability; instead, the effect on the probability scale usually requires partial derivatives or predicted probability calculations—either holding variables at certain values (conditional) or averaging across their distribution (marginal).\n\n\n\nFinal Takeaway\n\nMarginal Effect = Effect of a predictor without conditioning on other variables, or after averaging over them (population-level).\nConditional Effect = Effect of a predictor at specific values of other variables (subject-specific or partial effect).\n“Effect” in each field may be purely associational or may imply causation if the study design (e.g., randomized trial) and assumptions permit.\nIn non-linear models (logistic, Poisson, etc.), marginal vs. conditional effects differ more dramatically because averaging or integrating out variables/parameters on the link scale does not equate to averaging on the outcome scale."
  },
  {
    "objectID": "appendix/stat-topics/marginal-conditional-effects.html#references-further-reading",
    "href": "appendix/stat-topics/marginal-conditional-effects.html#references-further-reading",
    "title": "Marginal and Conditional Effects in GLM and GLMM",
    "section": "References / Further Reading",
    "text": "References / Further Reading\n\nSkrondal, A., & Rabe-Hesketh, S. (2004). Generalized Latent Variable Modeling: Multilevel, Longitudinal, and Structural Equation Models.\nLong, J. S., & Freese, J. (2014). Regression Models for Categorical Dependent Variables Using Stata.\nHardin, J. W., & Hilbe, J. M. (2003). Generalized Estimating Equations.\nMcCullagh, P., & Nelder, J. A. (1989). Generalized Linear Models."
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html",
    "href": "appendix/r-topics/r-subsetting-basics.html",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Subsetting data in R is a fundamental operation for data manipulation and analysis. It involves selecting specific rows, columns, or elements of a data structure based on conditions or indices. This tutorial explores subsetting techniques in R using Base R and tidyverse packages. It will provide comprehensive examples, tips, and nuances to master this essential skill.\nPackages and Functions Covered:\n\nBase R Functions: [], subset()\nTidyverse Functions: dplyr::filter(), dplyr::select(), dplyr::slice(), dplyr::pull()\n\nUse Cases:\n\nExtracting subsets of data for focused analysis.\nFiltering data for visualization or modeling.\nReducing dataset size for computation efficiency.\nHandling missing or specific data points.\n\nBenefits:\n\nBase R: Lightweight and does not require additional installations.\nTidyverse: Provides clean, readable syntax and integrates well with pipelines (|&gt;)."
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#introduction",
    "href": "appendix/r-topics/r-subsetting-basics.html#introduction",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Subsetting data in R is a fundamental operation for data manipulation and analysis. It involves selecting specific rows, columns, or elements of a data structure based on conditions or indices. This tutorial explores subsetting techniques in R using Base R and tidyverse packages. It will provide comprehensive examples, tips, and nuances to master this essential skill.\nPackages and Functions Covered:\n\nBase R Functions: [], subset()\nTidyverse Functions: dplyr::filter(), dplyr::select(), dplyr::slice(), dplyr::pull()\n\nUse Cases:\n\nExtracting subsets of data for focused analysis.\nFiltering data for visualization or modeling.\nReducing dataset size for computation efficiency.\nHandling missing or specific data points.\n\nBenefits:\n\nBase R: Lightweight and does not require additional installations.\nTidyverse: Provides clean, readable syntax and integrates well with pipelines (|&gt;)."
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#getting-started",
    "href": "appendix/r-topics/r-subsetting-basics.html#getting-started",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the Package:\n\npacman::p_load(dplyr)  # Loads or installs the dplyr package\n\nBasic Usage: Select rows where mpg &gt; 20 using both approaches:\nBase R:\n\ndata(mtcars)\nmtcars[mtcars$mpg &gt; 20, ]\n\n\n  \n\n\n\nTidyverse:\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; 20)"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#key-functions-and-features",
    "href": "appendix/r-topics/r-subsetting-basics.html#key-functions-and-features",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Key Functions and Features",
    "text": "Key Functions and Features\n\nBase R: Subsetting with []\nDescription: The simplest and most versatile method for subsetting in R. Use [] to extract rows, columns, or individual elements from data frames or matrices.\nSyntax:\ndata[rows, columns]\nExamples:\n\nSelect rows where mpg &gt; 20:\n\n\nmtcars[mtcars$mpg &gt; 20, ]\n\n\n  \n\n\n\n\nSelect specific columns:\n\n\nmtcars[, c(\"mpg\", \"cyl\")]\n\n\n  \n\n\n\n\nCombine row and column conditions:\n\n\nmtcars[mtcars$mpg &gt; 20, c(\"mpg\", \"cyl\")]\n\n\n  \n\n\n\nNuances:\n\nOmitting rows or columns selects all:\n\n\nmtcars[mtcars$mpg &gt; 20, ]  # All columns selected\n\n\n  \n\n\nmtcars[, c(\"mpg\", \"cyl\")]  # All rows selected\n\n\n  \n\n\n\n\nNegative indexing excludes rows or columns:\n\n\nmtcars[-c(1, 2), ]  # Exclude first two rows\n\n\n  \n\n\n\n\n\nBase R: subset()\nDescription: Provides a more human-readable syntax for subsetting.\nSyntax:\nsubset(data, subset, select)\nExamples:\n\nSubset rows where mpg &gt; 20 and select mpg and cyl columns:\n\n\nsubset(mtcars, mpg &gt; 20, select = c(mpg, cyl))\n\n\n  \n\n\n\n\nSubset using multiple conditions:\n\n\nsubset(mtcars, mpg &gt; 20 & cyl == 6, select = c(mpg, cyl))\n\n\n  \n\n\n\nNuances:\n\nNon-standard evaluation allows simpler syntax but can lead to ambiguities. Use column names explicitly when necessary.\n\n\n\nTidyverse: dplyr::filter()\nDescription: Filters rows based on logical conditions.\nSyntax:\ndplyr::filter(data, condition)\nExamples:\n\nFilter rows with mpg &gt; 20:\n\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; 20)\n\n\n  \n\n\n\n\nFilter with multiple conditions:\n\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; 20 & cyl == 6)\n\n\n  \n\n\n\n\nUse dynamic variables in filtering:\n\n\nthreshold &lt;- 20\nmtcars |&gt;\n    dplyr::filter(mpg &gt; threshold)\n\n\n  \n\n\n\nNuances:\n\nUse | for “or” conditions:\n\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; 20 | cyl == 6)\n\n\n  \n\n\n\n\nFilter with is.na() to handle missing values:\n\n\nmtcars |&gt;\n    dplyr::filter(!is.na(mpg))\n\n\n  \n\n\n\n\n\nTidyverse: dplyr::select()\nDescription: Selects specific columns from a data frame.\nSyntax:\ndplyr::select(data, columns)\nExamples:\n\nSelect specific columns:\n\n\nmtcars |&gt;\n    dplyr::select(mpg, cyl)\n\n\n  \n\n\n\n\nUse helpers for dynamic selection:\n\n\nmtcars |&gt;\n    dplyr::select(starts_with(\"d\"))\n\n\n  \n\n\n\nNuances:\n\nCombine helpers for advanced selection:\n\n\nmtcars |&gt;\n    dplyr::select(starts_with(\"d\"), ends_with(\"t\"))\n\n\n  \n\n\n\n\n\nTidyverse: dplyr::slice()\nDescription: Selects rows by position.\nSyntax:\ndplyr::slice(data, row_numbers)\nExamples:\n\nSelect the first five rows:\n\n\nmtcars |&gt;\n    dplyr::slice(1:5)\n\n\n  \n\n\n\n\nRandom sampling:\n\n\nmtcars |&gt;\n    dplyr::slice_sample(n = 5)\n\n\n  \n\n\n\n\nSelect rows by percentages:\n\n\nmtcars |&gt;\n    dplyr::slice_sample(prop = .1)  # 10% of rows\n\n\n  \n\n\n\n\n\nTidyverse: dplyr::pull()\nDescription: Extracts a single column as a vector.\nSyntax:\ndplyr::pull(data, column)\nExamples:\n\nPull the mpg column:\n\n\nmtcars |&gt;\n    dplyr::pull(mpg)\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\n\nCombine with filtering:\n\n\nmtcars |&gt;\n    dplyr::filter(cyl == 6) |&gt;\n    dplyr::pull(mpg)\n\n[1] 21.0 21.0 21.4 18.1 19.2 17.8 19.7\n\n\nNuances:\n\nUseful for extracting data for external functions:\n\n\nmean(mtcars |&gt; dplyr::pull(mpg))\n\n[1] 20.09062"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#in-depth-examples",
    "href": "appendix/r-topics/r-subsetting-basics.html#in-depth-examples",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "In-Depth Examples",
    "text": "In-Depth Examples\n\nExample 1: Dynamic Filtering with External Variables\nObjective: Filter rows based on user-defined thresholds for mpg and hp.\nSteps:\n\nDefine thresholds:\n\n\nmpg_threshold &lt;- 20\nhp_threshold &lt;- 100\n\n\nBase R:\n\n\nmtcars[mtcars$mpg &gt; mpg_threshold & mtcars$hp &gt; hp_threshold, ]\n\n\n  \n\n\n\n\nTidyverse:\n\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; mpg_threshold & hp &gt; hp_threshold)\n\n\n  \n\n\n\n\n\nExample 2: Visualizing Subsets\nObjective: Visualize cars with mpg &gt; 20.\nSteps:\n\nSubset data:\n\n\nfiltered_data &lt;- mtcars |&gt;\n    dplyr::filter(mpg &gt; 20)\n\n\nPlot:\n\n\nlibrary(ggplot2)\nggplot(filtered_data, aes(x = wt, y = mpg)) +\n    geom_point()"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#advanced-features",
    "href": "appendix/r-topics/r-subsetting-basics.html#advanced-features",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Advanced Features",
    "text": "Advanced Features\nCustomization:\n\nCreate reusable functions for subsetting:\n\n\nfilter_cars &lt;- function(data, mpg_threshold, cyl_threshold) {\n    data |&gt;\n        dplyr::filter(mpg &gt; mpg_threshold, cyl == cyl_threshold)\n}\n\nfilter_cars(mtcars, 20, 6)\n\n\n  \n\n\n\nIntegration:\n\nCombine tidyr for reshaping with subsetting:\n\n\nlibrary(tidyr)\nmtcars |&gt;\n    pivot_longer(cols = everything()) |&gt;\n    dplyr::filter(value &gt; 20)"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-subsetting-basics.html#troubleshooting-and-faqs",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Troubleshooting and FAQs",
    "text": "Troubleshooting and FAQs\n\nIssue: “undefined columns selected” Fix: Double-check column names.\nIssue: Subsetting returns no rows. Fix: Ensure logical conditions are met.\n\nFAQs:\n\nHow to subset with regex? Use grepl() in Base R or matches() in dplyr::select():\n\n\nmtcars |&gt;\n    dplyr::select(matches(\"^m\"))"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#best-practices",
    "href": "appendix/r-topics/r-subsetting-basics.html#best-practices",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Best Practices",
    "text": "Best Practices\n\nPreview data before subsetting using head() or glimpse().\nChain operations for clarity in tidyverse.\nAvoid hard-coding indices; use dynamic variables."
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#conclusion",
    "href": "appendix/r-topics/r-subsetting-basics.html#conclusion",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Conclusion",
    "text": "Conclusion\nRecap: Subsetting is a powerful tool for extracting specific data. Base R provides flexibility, while tidyverse offers an elegant syntax for modern workflows."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "",
    "text": "Quarto is a powerful scientific and technical publishing system that extends R Markdown, offering enhanced functionality for rendering figures, tables, captions, and cross-references. With Quarto, you can produce high-quality, reproducible documents that integrate both code and text seamlessly. In this tutorial, we will explore how to create and reference figures and tables, include captions, and leverage various Quarto features.\nPackages and functions we will focus on:\n\nQuarto chunk options (e.g.,fig-cap, table-cap, label, etc.)\nBase R functions for data wrangling and plotting (e.g., plot(), summary())\nggplot2::ggplot() for advanced plotting\ndplyr::mutate() and other dplyr functions for data manipulation\nknitr::kable() for basic table creation\ngt::gt() for more advanced table formatting\n\nUse Cases:\n\nCreating reproducible reports with figure and table references.\nWriting scientific articles where cross-referencing figures and tables is essential.\nProducing dynamic documents that blend analysis, commentary, and visuals.\n\nBenefits:\n\nAutomatically numbered and labeled figures and tables.\nEasily referenced in text using Quarto’s cross-reference syntax.\nFlexible customization of captions and layout.\nConsistent appearance across multiple output formats (HTML, PDF, Word)."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#loading-the-package",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#loading-the-package",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Loading the Package",
    "text": "Loading the Package\nWhile you do not need a specific package to enable figure and table references in Quarto (beyond Quarto itself), you will typically rely on a suite of R packages for data manipulation and visualization. Below is an example of loading packages with pacman:\n\npacman::p_load(\n    dplyr,\n    ggplot2,\n    knitr,\n    gt\n)\n\nggplot2::theme_set(theme_minimal())"
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#basic-usage",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#basic-usage",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Basic Usage",
    "text": "Basic Usage\nQuarto uses specially marked code chunks to control figure and table output. A simple code chunk with a figure could look like this:\n\n```{r}\n#| label: fig-basicplot\n#| fig-cap: \"A basic scatter plot with base R\"\nplot(\n    cars$speed,\n    cars$dist,\n    main = \"Speed vs. Stopping Distance\",\n    xlab = \"Speed (mph)\",\n    ylab = \"Distance (ft)\"\n)\n```\n\n\n\n\n\n\n\nFigure 1: A basic scatter plot with base R\n\n\n\n\n\nAnnotations:\n\n#| label: fig-basicplot assigns a label to this code chunk, used for cross-referencing.\n#| fig-cap: \"A basic scatter plot with base R\" adds a figure caption.\n\nTo reference the figure in your text, you can write: “As shown in Figure 1 …”."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#quarto-chunk-options-for-figures",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#quarto-chunk-options-for-figures",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Quarto Chunk Options for Figures",
    "text": "Quarto Chunk Options for Figures\nDescription: Quarto recognizes options like fig-cap to set a figure caption, label to set a reference label, and fig-width/fig-height to control figure size.\nSyntax (not executed, just shown):\n#| label: chunk_label\n#| fig-cap: \"Figure caption here\"\n#| fig-width: 6\n#| fig-height: 4\nExplanation of arguments:\n\nlabel (character): Identifies the figure so it can be cross-referenced.\nfig-cap (character): The caption text for the figure.\nfig-width, fig-height (numeric): Dimensions (in inches) of the figure.\n\nExample:\n\n```{r}\n#| label: fig-ggplot-example\n#| fig-cap: \"Iris Data: Sepal Width vs. Sepal Length\"\n#| fig-width: 6\n#| fig-height: 4\niris |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = Sepal.Length, y = Sepal.Width)) +\n    ggplot2::geom_point(color = \"blue\") +\n    ggplot2::labs(title = \"Iris Scatter Plot\")\n```\n\n\n\n\n\n\n\nFigure 2: Iris Data: Sepal Width vs. Sepal Length"
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#quarto-chunk-options-for-tables",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#quarto-chunk-options-for-tables",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Quarto Chunk Options for Tables",
    "text": "Quarto Chunk Options for Tables\nDescription: Similar to figures, Quarto also provides table-cap for a table caption and label for referencing.\nSyntax (not executed, just shown):\n#| label: tbl-label\n#| tbl-cap: \"Table caption here\"\nExample:\n\n```{r}\n#| label: tbl-kable-example\n#| tbl-cap: \"Summary of the cars dataset\"\nsummary_table &lt;- knitr::kable(summary(cars))\nsummary_table\n```\n\n\n\n\n\n\n\n\nspeed\ndist\n\n\n\n\n\nMin. : 4.0\nMin. : 2.00\n\n\n\n1st Qu.:12.0\n1st Qu.: 26.00\n\n\n\nMedian :15.0\nMedian : 36.00\n\n\n\nMean :15.4\nMean : 42.98\n\n\n\n3rd Qu.:19.0\n3rd Qu.: 56.00\n\n\n\nMax. :25.0\nMax. :120.00\n\n\n\n\n\n\nTable 1: Summary of the cars dataset"
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#example-1-referencing-a-figure-generated-by-ggplot2",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#example-1-referencing-a-figure-generated-by-ggplot2",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Example 1: Referencing a Figure Generated by Ggplot2",
    "text": "Example 1: Referencing a Figure Generated by Ggplot2\nTitle: Iris Dataset Visualization\nObjective: Create a scatter plot and reference it in the text.\nSteps:\n\nLoad required packages.\nPrepare the data (in this case, the iris dataset is already built in).\nCreate a figure chunk with a label and caption.\nReference the figure in the text.\n\n\n```{r}\n#| label: fig-iris\n#| fig-cap: \"Sepal Length vs. Petal Length in Iris dataset\"\niris |&gt;\n    ggplot2::ggplot(ggplot2::aes(\n        x = Sepal.Length,\n        y = Petal.Length,\n        color = Species\n    )) +\n    ggplot2::geom_point() +\n    ggplot2::labs(title = \"Iris: Sepal vs. Petal Length\")\n```\n\n\n\n\n\n\n\nFigure 3: Sepal Length vs. Petal Length in Iris dataset\n\n\n\n\n\nOutput:\n\nA scatter plot labeled as “fig-iris,” automatically numbered by Quarto.\nTo reference this figure in your text, use Figure @fig-iris shows …."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#example-2-creating-and-referencing-a-table-with-gt",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#example-2-creating-and-referencing-a-table-with-gt",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Example 2: Creating and Referencing a Table with Gt",
    "text": "Example 2: Creating and Referencing a Table with Gt\n\nTitle: Summarizing mtcars with gt\nObjective: Produce a well-formatted table, then reference it in text.\n\nSteps:\n\nUse the mtcars dataset.\nSummarize or transform if needed (e.g., select columns).\nCreate a table chunk with a label and caption.\nCross-reference within text.\n\n\n```{r}\n#| label: tbl-gtexample\n#| tbl-cap: \"Average Horsepower by Number of Cylinders\"\nmtcars |&gt;\n    dplyr::group_by(cyl) |&gt;\n    dplyr::summarise(avg_hp = mean(hp)) |&gt;\n    gt::gt()\n```\n\n\n\n\n\n\n\n\n\n\ncyl\navg_hp\n\n\n\n\n4\n82.63636\n\n\n6\n122.28571\n\n\n8\n209.21429\n\n\n\n\n\n\n\n\nTable 2: Average Horsepower by Number of Cylinders\n\n\n\n\nOutput:\n\nA formatted table with “tbl-gt-example” as the label.\nReference it with Table @tbl-gt-example displays … in your text. This produces a clickable link in the output as follows: Table Table 2 displays …“."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#customization",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#customization",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Customization",
    "text": "Customization\n\nAdjust figure size with fig-width and fig-height to control appearance.\nChange plot themes in ggplot2::theme().\nUse chunk options like eval = FALSE or echo = FALSE to control whether code is run or shown.\n\nExample of advanced customization for a figure:\n\n```{r}\n#| label: fig-custom-plot\n#| fig-cap: \"Customized Plot with Theme\"\n#| fig-width: 7\n#| fig-height: 5\niris |&gt;\n    ggplot2::ggplot(ggplot2::aes(\n        x = Species,\n        y = Sepal.Length,\n        fill = Species\n    )) +\n    ggplot2::geom_boxplot() +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = \"Iris Boxplot\")\n```\n\n\n\n\n\n\n\nFigure 4: Customized Plot with Theme"
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#integration",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#integration",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Integration",
    "text": "Integration\n\nCombine dplyr::mutate() and ggplot2::ggplot() to produce elaborate figures.\nUse tidyr::pivot_longer() or pivot_wider() to reshape data before tabulation.\nPipe operator |&gt; ensures code readability.\n\nExample:\n\n```{r}\n#| label: tbl-integrated-example\n#| table-cap: \"Iris Species Mean Measurements\"\niris |&gt;\n    dplyr::group_by(Species) |&gt;\n    dplyr::summarise(across(\n        .cols = c(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width),\n        .fns = ~ mean(.x, na.rm = TRUE)\n    )) |&gt;\n    gt::gt()\n```\n\n\n\n\n\n\n\nSpecies\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026"
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#optimization",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#optimization",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Optimization",
    "text": "Optimization\n\nFor large datasets, consider sampling or summarizing before creating tables or plots.\nCache expensive operations using #| cache: true in your code chunk if your workflow supports it."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#common-issues",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#common-issues",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "Common Issues",
    "text": "Common Issues\n\nFigure or table labels not found\n\nMake sure you set #| label: &lt;label_name&gt; in the correct code chunk.\nCheck that you are referencing it with @fig-&lt;label_name&gt; or @tbl-&lt;label_name&gt; in the text.\n\nCaption not displayed\n\nConfirm you used fig-cap for figures or tbl-cap for tables.\nEnsure no conflicting chunk options are overriding your settings. å\n\nFigures and tables appear in the wrong order\n\nQuarto processes code chunks in the order they appear. Check the chunk’s position in the document."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure-extra.html#faqs",
    "href": "appendix/r-topics/r-quarto-table-figure-extra.html#faqs",
    "title": "Figures and Tables in Quarto: Advanced Features",
    "section": "FAQs",
    "text": "FAQs\n\nHow do I hide the code but show the figure/table?\n\nUse echo = FALSE in your chunk options.\n\nCan I reference a table as a figure or vice versa?\n\nTechnically yes, but it’s best to use fig- for figures and tbl- for tables for clarity.\n\nWhy is my code block not producing a figure?\n\nMake sure you have a plotting function in the chunk and that eval = TRUE is not set to FALSE."
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html",
    "href": "appendix/r-topics/r-factors-basics.html",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Factors in R are data structures used to handle categorical data effectively. Factors allow you to store, manipulate, and analyze data with distinct levels, making them essential for statistical modeling and visualization. This tutorial covers functions from the base R package and the forcats package.\n\n\n\n\nBase R Functions: factor(), levels(), nlevels(), as.factor(), is.factor(), relevel()\nForcats Functions: forcats::fct_relevel(), forcats::fct_infreq(), forcats::fct_collapse()\n\n\n\n\n\nRepresenting categorical variables (e.g., gender, regions).\nEncoding ordinal variables (e.g., education levels, survey responses).\nSimplifying group-wise statistical analysis.\nReordering levels for meaningful visualization.\n\n\n\n\n\nEfficient storage of categorical data.\nSeamless integration with statistical models in R.\nImproved clarity and readability of data.\n\n\n\n\n\n\n\nUse pacman::p_load() to load necessary packages:\n\npacman::p_load(forcats, dplyr)\n\n\n\n\nCreate a basic factor and explore its properties:\n\n# Creating a factor\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\"))\n\n# Display the factor\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red\n\n# Check the levels\nlevels(colors)\n\n[1] \"blue\"  \"green\" \"red\"  \n\n\n\n\n\n\n\n\n\n\nCreates a factor from a vector of values.\n\n\n\nfactor(x, levels = NULL, labels = NULL, ordered = FALSE)\n\nx: Input vector.\nlevels: Custom levels.\nlabels: Labels for levels.\nordered: Logical flag for ordinal factors.\n\n\n\n\n\ngrades &lt;- factor(c(\"A\", \"B\", \"C\", \"A\", \"B\"), levels = c(\"A\", \"B\", \"C\"), ordered = TRUE)\ngrades\n\n[1] A B C A B\nLevels: A &lt; B &lt; C\n\n\n\n\n\n\n\n\nReorders the levels of a factor to make a specific level the reference level.\n\n\n\nrelevel(factor_variable, ref)\n\nfactor_variable: The factor to be modified.\nref: The new reference level.\n\n\n\n\n\n# Releveling to make \"blue\" the reference level\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\"))\ncolors &lt;- relevel(colors, ref = \"blue\")\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red\n\n\n\n\n\n\n\n\nReorders factor levels.\n\n\n\nforcats::fct_relevel(factor_variable, new_order)\n\n\n\n\neducation &lt;- factor(c(\"High School\", \"Masters\", \"Bachelors\", \"PhD\"))\neducation &lt;- forcats::fct_relevel(education, \"High School\", \"Bachelors\", \"Masters\", \"PhD\")\neducation\n\n[1] High School Masters     Bachelors   PhD        \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\n\n\n\nReordering education levels for a survey dataset.\n\n\n\nRearrange education levels from lowest to highest for meaningful analysis.\n\n\n\n\neducation &lt;- factor(c(\"Masters\", \"PhD\", \"High School\", \"Bachelors\"))\neducation &lt;- forcats::fct_relevel(education, \"High School\", \"Bachelors\", \"Masters\", \"PhD\")\neducation\n\n[1] Masters     PhD         High School Bachelors  \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\nChange the reference level of a factor for a statistical model.\n\n\n\n\n# Relevel \"High School\" as the reference level\neducation &lt;- relevel(education, ref = \"High School\")\neducation\n\n[1] Masters     PhD         High School Bachelors  \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\nSimplify factor levels by collapsing related categories.\n\n\n\n\nfruit &lt;- factor(c(\"apple\", \"banana\", \"pear\", \"apple\", \"orange\", \"pear\"))\nfruit &lt;- forcats::fct_collapse(fruit, Citrus = c(\"orange\"), Other = c(\"apple\", \"banana\", \"pear\"))\nfruit\n\n[1] Other  Other  Other  Other  Citrus Other \nLevels: Other Citrus\n\n\n\n\n\n\n\n\n\nAdjust levels dynamically:\n\nlevels(colors) &lt;- c(levels(colors), \"yellow\")\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red yellow\n\n\n\n\n\nCombine factors with other R packages:\n\ndata &lt;- tibble(category = factor(c(\"A\", \"B\", \"A\")), value = c(10, 20, 30))\ndata |&gt; group_by(category) |&gt; summarize(mean_value = mean(value))\n\n\n  \n\n\n\n\n\n\n\n\n\n\nUnused Levels: Use droplevels() to remove unused levels.\nIncorrect Order: Use forcats::fct_relevel() to reorder.\n\n\n\n\n\nQ: How to convert a factor back to a character vector?\nA: Use as.character(factor_variable).\nQ: How to check if a variable is a factor?\nA: Use is.factor(variable).\n\n\n\n\n\n\nAlways specify levels for consistency.\nUse ordered factors for ordinal data.\nUse forcats functions for enhanced factor manipulation.\n\n\n\n\n\n\nFactors are powerful tools for handling categorical data in R. They are versatile and integrate well with R’s modeling and visualization ecosystem.\n\n\n\nExplore advanced functionalities in the forcats package, such as forcats::fct_infreq().\n\n\n\n\n\nR Factor Documentation\nForcats Documentation\nTidyverse Guide to Factors"
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#introduction",
    "href": "appendix/r-topics/r-factors-basics.html#introduction",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Factors in R are data structures used to handle categorical data effectively. Factors allow you to store, manipulate, and analyze data with distinct levels, making them essential for statistical modeling and visualization. This tutorial covers functions from the base R package and the forcats package.\n\n\n\n\nBase R Functions: factor(), levels(), nlevels(), as.factor(), is.factor(), relevel()\nForcats Functions: forcats::fct_relevel(), forcats::fct_infreq(), forcats::fct_collapse()\n\n\n\n\n\nRepresenting categorical variables (e.g., gender, regions).\nEncoding ordinal variables (e.g., education levels, survey responses).\nSimplifying group-wise statistical analysis.\nReordering levels for meaningful visualization.\n\n\n\n\n\nEfficient storage of categorical data.\nSeamless integration with statistical models in R.\nImproved clarity and readability of data."
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#getting-started",
    "href": "appendix/r-topics/r-factors-basics.html#getting-started",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Use pacman::p_load() to load necessary packages:\n\npacman::p_load(forcats, dplyr)\n\n\n\n\nCreate a basic factor and explore its properties:\n\n# Creating a factor\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\"))\n\n# Display the factor\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red\n\n# Check the levels\nlevels(colors)\n\n[1] \"blue\"  \"green\" \"red\""
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#key-functions-and-features",
    "href": "appendix/r-topics/r-factors-basics.html#key-functions-and-features",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Creates a factor from a vector of values.\n\n\n\nfactor(x, levels = NULL, labels = NULL, ordered = FALSE)\n\nx: Input vector.\nlevels: Custom levels.\nlabels: Labels for levels.\nordered: Logical flag for ordinal factors.\n\n\n\n\n\ngrades &lt;- factor(c(\"A\", \"B\", \"C\", \"A\", \"B\"), levels = c(\"A\", \"B\", \"C\"), ordered = TRUE)\ngrades\n\n[1] A B C A B\nLevels: A &lt; B &lt; C\n\n\n\n\n\n\n\n\nReorders the levels of a factor to make a specific level the reference level.\n\n\n\nrelevel(factor_variable, ref)\n\nfactor_variable: The factor to be modified.\nref: The new reference level.\n\n\n\n\n\n# Releveling to make \"blue\" the reference level\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\"))\ncolors &lt;- relevel(colors, ref = \"blue\")\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red\n\n\n\n\n\n\n\n\nReorders factor levels.\n\n\n\nforcats::fct_relevel(factor_variable, new_order)\n\n\n\n\neducation &lt;- factor(c(\"High School\", \"Masters\", \"Bachelors\", \"PhD\"))\neducation &lt;- forcats::fct_relevel(education, \"High School\", \"Bachelors\", \"Masters\", \"PhD\")\neducation\n\n[1] High School Masters     Bachelors   PhD        \nLevels: High School Bachelors Masters PhD"
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#in-depth-examples",
    "href": "appendix/r-topics/r-factors-basics.html#in-depth-examples",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Reordering education levels for a survey dataset.\n\n\n\nRearrange education levels from lowest to highest for meaningful analysis.\n\n\n\n\neducation &lt;- factor(c(\"Masters\", \"PhD\", \"High School\", \"Bachelors\"))\neducation &lt;- forcats::fct_relevel(education, \"High School\", \"Bachelors\", \"Masters\", \"PhD\")\neducation\n\n[1] Masters     PhD         High School Bachelors  \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\nChange the reference level of a factor for a statistical model.\n\n\n\n\n# Relevel \"High School\" as the reference level\neducation &lt;- relevel(education, ref = \"High School\")\neducation\n\n[1] Masters     PhD         High School Bachelors  \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\nSimplify factor levels by collapsing related categories.\n\n\n\n\nfruit &lt;- factor(c(\"apple\", \"banana\", \"pear\", \"apple\", \"orange\", \"pear\"))\nfruit &lt;- forcats::fct_collapse(fruit, Citrus = c(\"orange\"), Other = c(\"apple\", \"banana\", \"pear\"))\nfruit\n\n[1] Other  Other  Other  Other  Citrus Other \nLevels: Other Citrus"
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#advanced-features",
    "href": "appendix/r-topics/r-factors-basics.html#advanced-features",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Adjust levels dynamically:\n\nlevels(colors) &lt;- c(levels(colors), \"yellow\")\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red yellow\n\n\n\n\n\nCombine factors with other R packages:\n\ndata &lt;- tibble(category = factor(c(\"A\", \"B\", \"A\")), value = c(10, 20, 30))\ndata |&gt; group_by(category) |&gt; summarize(mean_value = mean(value))"
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-factors-basics.html#troubleshooting-and-faqs",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Unused Levels: Use droplevels() to remove unused levels.\nIncorrect Order: Use forcats::fct_relevel() to reorder.\n\n\n\n\n\nQ: How to convert a factor back to a character vector?\nA: Use as.character(factor_variable).\nQ: How to check if a variable is a factor?\nA: Use is.factor(variable)."
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#best-practices",
    "href": "appendix/r-topics/r-factors-basics.html#best-practices",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Always specify levels for consistency.\nUse ordered factors for ordinal data.\nUse forcats functions for enhanced factor manipulation."
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#conclusion",
    "href": "appendix/r-topics/r-factors-basics.html#conclusion",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Factors are powerful tools for handling categorical data in R. They are versatile and integrate well with R’s modeling and visualization ecosystem.\n\n\n\nExplore advanced functionalities in the forcats package, such as forcats::fct_infreq()."
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#references-and-resources",
    "href": "appendix/r-topics/r-factors-basics.html#references-and-resources",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "R Factor Documentation\nForcats Documentation\nTidyverse Guide to Factors"
  },
  {
    "objectID": "appendix/r-topics/r-data-import-basics.html",
    "href": "appendix/r-topics/r-data-import-basics.html",
    "title": "Importing Various Data Set Forms in R",
    "section": "",
    "text": "R’s strength lies in its data manipulation and analysis capabilities. Effectively importing data is the crucial first step. Various functions and packages handle diverse data formats, ensuring a smooth transition from your data source to R."
  },
  {
    "objectID": "appendix/r-topics/r-data-import-basics.html#introduction",
    "href": "appendix/r-topics/r-data-import-basics.html#introduction",
    "title": "Importing Various Data Set Forms in R",
    "section": "",
    "text": "R’s strength lies in its data manipulation and analysis capabilities. Effectively importing data is the crucial first step. Various functions and packages handle diverse data formats, ensuring a smooth transition from your data source to R."
  },
  {
    "objectID": "appendix/r-topics/r-data-import-basics.html#common-data-formats-and-import-functions",
    "href": "appendix/r-topics/r-data-import-basics.html#common-data-formats-and-import-functions",
    "title": "Importing Various Data Set Forms in R",
    "section": "Common Data Formats and Import Functions",
    "text": "Common Data Formats and Import Functions\n\n1. CSV (Comma-Separated Values)\nCSV files are among the most widely used for storing tabular data. R’s read.csv() function is highly efficient for this format.\n# Basic usage\nmy_data &lt;- read.csv(\"my_data.csv\")\n\n# Specifying the separator (if not a comma)\nmy_data &lt;- read.csv(\"my_data.csv\", sep = \";\") # For semicolon-separated values\n\n# Handling header rows\nmy_data &lt;- read.csv(\"my_data.csv\", header = TRUE) # If the first row contains column names (default)\nmy_data &lt;- read.csv(\"my_data.csv\", header = FALSE) # If the first row is data\n\n# Setting the decimal point\nmy_data &lt;- read.csv(\"my_data.csv\", dec = \",\") # If using a comma as the decimal separator\n\n\n2. TXT (Text Files)\nPlain text files often require more careful handling due to variations in formatting. read.table() is the core function, offering extensive control.\n# Basic usage (assumes header and tab separation)\nmy_data &lt;- read.table(\"my_data.txt\", header = TRUE, sep = \"\\t\")\n\n# Specifying separators and other parameters\nmy_data &lt;- read.table(\"my_data.txt\", header = FALSE, sep = \",\", dec = \".\", fill = TRUE, na.strings = c(\"NA\", \".\", \"\"))\n#fill=TRUE: handles rows with different number of elements\n#na.strings: defines what to treat as missing values.\n\n\n3. Excel Files (.xls, .xlsx)\nThe readxl package provides efficient tools for importing Excel files.\n# Install if necessary: install.packages(\"readxl\")\nlibrary(readxl)\n\n# Importing a specific sheet\nmy_data &lt;- read_excel(\"my_excel_file.xlsx\", sheet = \"Sheet1\")\n\n# Importing multiple sheets (returns a list)\nmy_data &lt;- lapply(excel_sheets(\"my_excel_file.xlsx\"), read_excel, path = \"my_excel_file.xlsx\")\n\n#Specifying the column types\nmy_data &lt;- read_excel(\"my_excel_file.xlsx\", sheet = \"Sheet1\", col_types = c(\"numeric\", \"text\", \"date\"))\n\n\n4. SPSS Files (.save)\nThe haven package simplifies importing SPSS data files.\n# Install if necessary: install.packages(\"haven\")\nlibrary(haven)\n\nmy_data &lt;- read_sav(\"my_spss_file.sav\")\n\n\n5. SAS Files (.sas7bdat)\nhaven also handles SAS data sets.\n# Install haven if you haven't already\nlibrary(haven)\n\nmy_data &lt;- read_sas(\"my_sas_file.sas7bdat\")\n\n\n6. Stata Files (.dta)\nSimilarly, haven supports Stata files.\n# Install haven if you haven't already\nlibrary(haven)\n\nmy_data &lt;- read_dta(\"my_stata_file.dta\")\n\n\n7. JSON (JavaScript Object Notation)\nJSON is widely used for web-based data exchange. The jsonlite package is excellent for this task.\n# Install if necessary: install.packages(\"jsonlite\")\nlibrary(jsonlite)\n\nmy_data &lt;- fromJSON(\"my_json_file.json\")\n\n\n8. RData Files\nThese are R-specific binary files preserving data structures and objects.\nload(\"my_rdata_file.RData\") #loads all objects in the RData file into the current R environment"
  },
  {
    "objectID": "appendix/r-topics/r-data-import-basics.html#handling-data-issues",
    "href": "appendix/r-topics/r-data-import-basics.html#handling-data-issues",
    "title": "Importing Various Data Set Forms in R",
    "section": "Handling Data Issues",
    "text": "Handling Data Issues\n\nMissing Values\nMany functions have na.strings arguments to specify how missing values are represented in the source file (e.g., “NA”, ““,”.”, ” “).\n\n\nData Types\nIncorrect data type inferences can cause problems. Explicitly specify column types using functions like col_types in readxl or using colClasses in read.table and read.csv to avoid such issues.\n\n\nLarge Files\nFor very large files, consider using packages designed for memory-efficient data handling, like data.table or readr. readr::read_csv() is typically faster and more memory-efficient than read.csv().\n# Install readr if necessary: install.packages(\"readr\")\nlibrary(readr)\nmy_data &lt;- read_csv(\"large_file.csv\")   #Handles large CSV files more efficiently"
  },
  {
    "objectID": "appendix/r-topics/r-data-import-basics.html#best-practices",
    "href": "appendix/r-topics/r-data-import-basics.html#best-practices",
    "title": "Importing Various Data Set Forms in R",
    "section": "Best Practices",
    "text": "Best Practices\n\nInspect your data: Preview your files before importing (e.g., in a text editor or spreadsheet program).\nSpecify options explicitly: Don’t rely on defaults – customize arguments for optimal results.\nCheck the data structure: After import, always inspect your data frame using str(my_data) or summary(my_data) to verify correct formatting.\nHandle missing values: Carefully address missing data using appropriate R functions (e.g., na.omit(), imputation methods).\n\nThis tutorial provides a foundation for importing diverse data formats into R. Remember to consult the documentation for each specific package and function for comprehensive options and detailed parameter descriptions."
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html",
    "href": "appendix/r-topics/r-base-graphics-basics.html",
    "title": "Hypothesis Testing in R",
    "section": "",
    "text": "Hypothesis testing is a cornerstone of statistical analysis, used to evaluate assumptions about population parameters based on sample data. In R, several functions and packages provide extensive tools for hypothesis testing, ranging from basic tests in base R to more specialized tests available in external packages.\n\n\n\nPackages/Functions:\n\nBase R: t.test(), chisq.test(), var.test(), wilcox.test()\nAdditional Packages: stats::ks.test(), MASS::leveneTest()\n\nPurpose:\n\nTo evaluate assumptions or hypotheses about data distributions and relationships.\n\nCommon Functions:\n\nt.test() for t-tests.\nchisq.test() for Chi-square tests.\nks.test() for Kolmogorov-Smirnov tests.\nwilcox.test() for non-parametric Wilcoxon tests.\n\n\n\n\n\n\nComparing means between two groups.\nAssessing the goodness-of-fit for categorical data.\nTesting for equal variances across groups.\nNon-parametric comparisons for skewed or ordinal data.\n\n\n\n\n\nEasy to implement with a range of built-in functions.\nHighly extensible for complex scenarios.\nSeamless integration with visualization and other analytical tools."
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html#introduction",
    "href": "appendix/r-topics/r-base-graphics-basics.html#introduction",
    "title": "Hypothesis Testing in R",
    "section": "",
    "text": "Hypothesis testing is a cornerstone of statistical analysis, used to evaluate assumptions about population parameters based on sample data. In R, several functions and packages provide extensive tools for hypothesis testing, ranging from basic tests in base R to more specialized tests available in external packages.\n\n\n\nPackages/Functions:\n\nBase R: t.test(), chisq.test(), var.test(), wilcox.test()\nAdditional Packages: stats::ks.test(), MASS::leveneTest()\n\nPurpose:\n\nTo evaluate assumptions or hypotheses about data distributions and relationships.\n\nCommon Functions:\n\nt.test() for t-tests.\nchisq.test() for Chi-square tests.\nks.test() for Kolmogorov-Smirnov tests.\nwilcox.test() for non-parametric Wilcoxon tests.\n\n\n\n\n\n\nComparing means between two groups.\nAssessing the goodness-of-fit for categorical data.\nTesting for equal variances across groups.\nNon-parametric comparisons for skewed or ordinal data.\n\n\n\n\n\nEasy to implement with a range of built-in functions.\nHighly extensible for complex scenarios.\nSeamless integration with visualization and other analytical tools."
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html#getting-started",
    "href": "appendix/r-topics/r-base-graphics-basics.html#getting-started",
    "title": "Hypothesis Testing in R",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading the Package\nBase R provides most hypothesis testing tools. Load the MASS package for additional tests.\n\nlibrary(MASS)\n\n\n\nBasic Usage\n\nExample: One-Sample T-test\nTesting whether the mean of a sample is equal to a specific value.\n\nset.seed(42)\nsample_data &lt;- rnorm(30, mean = 5, sd = 2)\n\nt.test(sample_data, mu = 5)\n\n\n    One Sample t-test\n\ndata:  sample_data\nt = 0.29933, df = 29, p-value = 0.7668\nalternative hypothesis: true mean is not equal to 5\n95 percent confidence interval:\n 4.199903 6.074444\nsample estimates:\nmean of x \n 5.137174"
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html#key-functions-and-features",
    "href": "appendix/r-topics/r-base-graphics-basics.html#key-functions-and-features",
    "title": "Hypothesis Testing in R",
    "section": "Key Functions and Features",
    "text": "Key Functions and Features\n\nt.test()\n\nDescription\nPerforms one-sample, two-sample, and paired t-tests to compare means.\n\n\nSyntax\nt.test(x, y = NULL, alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95)\n\nx, y: Numeric vectors of data values.\nalternative: Type of hypothesis (“two.sided”, “greater”, “less”).\nmu: Hypothesized mean (default is 0).\npaired: Whether the test is paired (default is FALSE).\nvar.equal: Assume equal variances (default is FALSE).\n\n\n\nExample: Two-Sample T-test\n\ngroup1 &lt;- rnorm(20, mean = 5)\ngroup2 &lt;- rnorm(20, mean = 6)\n\nt.test(group1, group2, alternative = \"two.sided\", var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  group1 and group2\nt = -4.6894, df = 38, p-value = 3.486e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.1057327 -0.8358574\nsample estimates:\nmean of x mean of y \n 4.807940  6.278735 \n\n\n\n\n\nchisq.test()\n\nDescription\nTests for independence or goodness-of-fit in categorical data.\n\n\nSyntax\nchisq.test(x, y = NULL, correct = TRUE, simulate.p.value = FALSE, rescale.p = FALSE)\n\n\nExample: Independence Test\n\nobserved &lt;- matrix(c(30, 20, 50, 40), ncol = 2)\n\nchisq.test(observed)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  observed\nX-squared = 0.10954, df = 1, p-value = 0.7407\n\n\n\n\n\nwilcox.test()\n\nDescription\nPerforms the Wilcoxon rank-sum test (non-parametric alternative to the t-test).\n\n\nSyntax\nwilcox.test(x, y = NULL, alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, exact = NULL)\n\n\nExample: Wilcoxon Test\n\nx &lt;- rnorm(30)\ny &lt;- rnorm(30, mean = 1)\n\nwilcox.test(x, y, alternative = \"less\")\n\n\n    Wilcoxon rank sum exact test\n\ndata:  x and y\nW = 271, p-value = 0.003824\nalternative hypothesis: true location shift is less than 0"
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html#in-depth-examples",
    "href": "appendix/r-topics/r-base-graphics-basics.html#in-depth-examples",
    "title": "Hypothesis Testing in R",
    "section": "In-Depth Examples",
    "text": "In-Depth Examples\n\nComparing Means Across Groups\n\nTitle: Paired T-test for Preand Post-Treatment Data\n\n\nObjective\nEvaluate the effect of a treatment by comparing preand post-treatment scores.\n\n\nSteps\n\nPrepare the data.\nApply the paired t-test.\nVisualize the differences.\n\n\npre &lt;- c(12, 14, 11, 10, 13, 15)\npost &lt;- c(14, 16, 12, 11, 15, 17)\n\nt.test(pre, post, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pre and post\nt = -7.9057, df = 5, p-value = 0.0005211\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.208593 -1.124740\nsample estimates:\nmean difference \n      -1.666667 \n\n\n\n\nOutput\nInterpret the p-value to determine if there is a significant difference between preand post-treatment scores.\n\n\n\nGoodness-of-Fit Test\n\nTitle: Testing Dice Fairness\n\n\nObjective\nCheck if a six-sided die is fair.\n\n\nSteps\n\nCollect observed frequencies of each side.\nSpecify expected frequencies.\nApply the Chi-square goodness-of-fit test.\n\n\nobserved &lt;- c(12, 15, 14, 13, 16, 10)\nexpected &lt;- rep(1 / 6, 6) * sum(observed)\n\nchisq.test(observed, p = expected / sum(expected))\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 1.75, df = 5, p-value = 0.8825\n\n\n\n\nOutput\nEvaluate the p-value to determine if the die is fair."
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html#advanced-features",
    "href": "appendix/r-topics/r-base-graphics-basics.html#advanced-features",
    "title": "Hypothesis Testing in R",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nCustomization\n\nAdjusting Confidence Levels\nSet custom confidence levels in tests:\nt.test(group1, group2, conf.level = 0.99)\n\n\n\nIntegration\n\nUsing Hypothesis Testing with dplyr\n\nlibrary(dplyr)\n\nmtcars |&gt;\n    group_by(cyl) |&gt;\n    summarise(t_result = list(t.test(mpg ~ as.factor(am))$p.value))\n\n\n  \n\n\n\n\n\n\nOptimization\n\nManaging Large Datasets\nUse vectorized operations or summary statistics for efficiency:\n\nlarge_data &lt;- rnorm(1e6)\nt.test(large_data, mu = 0)\n\n\n    One Sample t-test\n\ndata:  large_data\nt = 0.59109, df = 1e+06, p-value = 0.5545\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.001370263  0.002553645\nsample estimates:\n   mean of x \n0.0005916909"
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-base-graphics-basics.html#troubleshooting-and-faqs",
    "title": "Hypothesis Testing in R",
    "section": "Troubleshooting and FAQs",
    "text": "Troubleshooting and FAQs\n\nCommon Issues\n\nError: Data must be numeric\n\nEnsure data is numeric by using as.numeric().\n\nWarning: p-value approximation\n\nIncrease the sample size or use simulate.p.value = TRUE.\n\n\n\n\nFAQs\n\nWhat is the minimum sample size for a t-test?\n\nAt least 5 observations per group are recommended.\n\nCan I test for normality before t-tests?\n\nUse shapiro.test() to check normality."
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html#best-practices",
    "href": "appendix/r-topics/r-base-graphics-basics.html#best-practices",
    "title": "Hypothesis Testing in R",
    "section": "Best Practices",
    "text": "Best Practices\n\nAlways visualize your data before performing tests.\nCheck assumptions (e.g., normality for t-tests, independence for Chi-square tests).\nUse appropriate alternatives (e.g., Wilcoxon test for non-normal data)."
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html#conclusion",
    "href": "appendix/r-topics/r-base-graphics-basics.html#conclusion",
    "title": "Hypothesis Testing in R",
    "section": "Conclusion",
    "text": "Conclusion\nHypothesis testing in R is a versatile and powerful tool for statistical analysis. From basic t-tests to advanced non-parametric methods, R provides comprehensive support for validating assumptions and making data-driven decisions."
  },
  {
    "objectID": "appendix/r-topics/r-base-graphics-basics.html#references-and-resources",
    "href": "appendix/r-topics/r-base-graphics-basics.html#references-and-resources",
    "title": "Hypothesis Testing in R",
    "section": "References and Resources",
    "text": "References and Resources\n\nR Documentation\nR Graph Gallery"
  },
  {
    "objectID": "appendix/r-packages/tidyr.html",
    "href": "appendix/r-packages/tidyr.html",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "The tidyr package, part of the tidyverse, is designed for data tidying—reshaping data into a tidy format where each variable is a column, each observation is a row, and each value is a single cell.\nKey functions include: - tidyr::pivot_longer(): Reshape wide data into long format. - tidyr::pivot_wider(): Reshape long data into wide format. - tidyr::separate(): Split a column into multiple columns. - tidyr::unite(): Combine multiple columns into one. - tidyr::drop_na(): Remove rows with missing values. - tidyr::replace_na(): Replace missing values.\n\n\n\n\nReshaping data for analysis or visualization.\nCleaning and preparing raw datasets.\nCombining or splitting columns for better data structure.\n\n\n\n\n\nSimplifies handling of messy or untidy data.\nEnsures compatibility with other tidyverse tools.\nOffers a consistent and intuitive syntax.\n\n\n\n\n\n\n\nLoad tidyr using pacman::p_load():\n\npacman::p_load(tidyr)\n\n\n\n\nReshape a simple dataset:\n\ndata &lt;- data.frame(\n    ID = 1:3,\n    Gender = c(\"M\", \"F\", \"M\"),\n    Age_2022 = c(25, 30, 35),\n    Age_2023 = c(26, 31, 36)\n)\n\n# Pivot longer\ndata |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nConverts wide data into long format, making columns into rows.\n\n\n\ntidyr::pivot_longer(data, cols, names_to, values_to, ...)\n\ndata: A data frame.\ncols: Columns to pivot.\nnames_to: Name of the new column for column names.\nvalues_to: Name of the new column for values.\n\n\n\n\n\ndata |&gt; tidyr::pivot_longer(cols = c(Age_2022, Age_2023), names_to = \"Year\", values_to = \"Age\")\n\n\n  \n\n\n\n\n\n\n\n\n\nConverts long data into wide format, making rows into columns.\n\n\n\ntidyr::pivot_wider(data, names_from, values_from, ...)\n\ndata: A data frame.\nnames_from: Column containing names for new columns.\nvalues_from: Column containing values for new columns.\n\n\n\n\n\nlong_data &lt;- data |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\nlong_data |&gt; tidyr::pivot_wider(names_from = Year, values_from = Age)\n\n\n  \n\n\n\n\n\n\n\n\n\nSplits a single column into multiple columns.\n\n\n\ntidyr::separate(data, col, into, sep = \" \")\n\ndata: A data frame.\ncol: Column to split.\ninto: Vector of new column names.\nsep: Separator to split on.\n\n\n\n\n\ndata &lt;- data.frame(Name = c(\"Alice_2022\", \"Bob_2023\"))\ndata |&gt; tidyr::separate(Name, into = c(\"Name\", \"Year\"), sep = \"_\")\n\n\n  \n\n\n\n\n\n\n\n\n\nCombines multiple columns into one.\n\n\n\ntidyr::unite(data, col, ..., sep = \"_\")\n\ndata: A data frame.\ncol: Name of the new column.\n...: Columns to combine.\nsep: Separator between combined values.\n\n\n\n\n\ndata &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Year = c(2022, 2023))\ndata |&gt; tidyr::unite(\"Name_Year\", Name, Year, sep = \"_\")\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nSummarize the mean age by gender and year.\n\n\n\n\ndata &lt;- data.frame(\n    ID = 1:4,\n    Gender = c(\"M\", \"F\", \"F\", \"M\"),\n    Age_2022 = c(25, 30, 27, 35),\n    Age_2023 = c(26, 31, 28, 36)\n)\n\ndata_long &lt;- data |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\n\nsummary &lt;- data_long |&gt;\n    dplyr::group_by(Gender, Year) |&gt;\n    dplyr::summarize(MeanAge = mean(Age), .groups = \"drop\")\nsummary\n\n\n  \n\n\n\n\n\n\nA summary table of mean ages by gender and year.\n\n\n\n\n\n\nCombine year and ID into a unique identifier.\n\n\n\n\ndata |&gt;\n    tidyr::unite(\"UniqueID\", ID, starts_with(\"Age\"), sep = \"-\")\n\n\n  \n\n\n\n\n\n\nA new column UniqueID combines values.\n\n\n\n\n\n\n\n\n\n\ndata_long |&gt; dplyr::mutate(Year = paste0(\"Year_\", Year))\n\n\n  \n\n\n\n\n\n\n\nCombine with ggplot2 for visualization:\n\nlibrary(ggplot2)\n\ndata_long |&gt;\n    ggplot(aes(x = Year, y = Age, fill = Gender)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    labs(title = \"Age Distribution by Year\", x = \"Year\", y = \"Age\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nUse select() before pivoting to limit unnecessary columns.\nAvoid excessive pivoting by designing data collection appropriately.\n\n\n\n\n\n\n\n\n\nEnsure column names are unique before pivoting.\n\n\n\nCheck the data for inconsistencies or missing values.\n\n\n\n\n\nCan tidyr handle large datasets? Yes, but consider chunking with data.table for very large data.\nHow do I preserve column types? Use tidyr::pivot_longer() or pivot_wider() with values_transform.\n\n\n\n\n\n\nStart with well-structured raw data.\nUse descriptive column names and standard formats.\nAvoid overusing pivot_longer() or pivot_wider() for reversible tasks.\n\n\n\n\n\n\ntidyr simplifies reshaping and tidying data with a cohesive set of functions.\n\n\n\nExplore advanced tidying tasks with tidyr or integrate with other tidyverse tools like ggplot2 and dplyr.\n\n\n\n\n\ntidyr Documentation\nR for Data Science\nTidyverse GitHub"
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#introduction",
    "href": "appendix/r-packages/tidyr.html#introduction",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "The tidyr package, part of the tidyverse, is designed for data tidying—reshaping data into a tidy format where each variable is a column, each observation is a row, and each value is a single cell.\nKey functions include: - tidyr::pivot_longer(): Reshape wide data into long format. - tidyr::pivot_wider(): Reshape long data into wide format. - tidyr::separate(): Split a column into multiple columns. - tidyr::unite(): Combine multiple columns into one. - tidyr::drop_na(): Remove rows with missing values. - tidyr::replace_na(): Replace missing values.\n\n\n\n\nReshaping data for analysis or visualization.\nCleaning and preparing raw datasets.\nCombining or splitting columns for better data structure.\n\n\n\n\n\nSimplifies handling of messy or untidy data.\nEnsures compatibility with other tidyverse tools.\nOffers a consistent and intuitive syntax."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#getting-started",
    "href": "appendix/r-packages/tidyr.html#getting-started",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Load tidyr using pacman::p_load():\n\npacman::p_load(tidyr)\n\n\n\n\nReshape a simple dataset:\n\ndata &lt;- data.frame(\n    ID = 1:3,\n    Gender = c(\"M\", \"F\", \"M\"),\n    Age_2022 = c(25, 30, 35),\n    Age_2023 = c(26, 31, 36)\n)\n\n# Pivot longer\ndata |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")"
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#key-functions-and-features",
    "href": "appendix/r-packages/tidyr.html#key-functions-and-features",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Converts wide data into long format, making columns into rows.\n\n\n\ntidyr::pivot_longer(data, cols, names_to, values_to, ...)\n\ndata: A data frame.\ncols: Columns to pivot.\nnames_to: Name of the new column for column names.\nvalues_to: Name of the new column for values.\n\n\n\n\n\ndata |&gt; tidyr::pivot_longer(cols = c(Age_2022, Age_2023), names_to = \"Year\", values_to = \"Age\")\n\n\n  \n\n\n\n\n\n\n\n\n\nConverts long data into wide format, making rows into columns.\n\n\n\ntidyr::pivot_wider(data, names_from, values_from, ...)\n\ndata: A data frame.\nnames_from: Column containing names for new columns.\nvalues_from: Column containing values for new columns.\n\n\n\n\n\nlong_data &lt;- data |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\nlong_data |&gt; tidyr::pivot_wider(names_from = Year, values_from = Age)\n\n\n  \n\n\n\n\n\n\n\n\n\nSplits a single column into multiple columns.\n\n\n\ntidyr::separate(data, col, into, sep = \" \")\n\ndata: A data frame.\ncol: Column to split.\ninto: Vector of new column names.\nsep: Separator to split on.\n\n\n\n\n\ndata &lt;- data.frame(Name = c(\"Alice_2022\", \"Bob_2023\"))\ndata |&gt; tidyr::separate(Name, into = c(\"Name\", \"Year\"), sep = \"_\")\n\n\n  \n\n\n\n\n\n\n\n\n\nCombines multiple columns into one.\n\n\n\ntidyr::unite(data, col, ..., sep = \"_\")\n\ndata: A data frame.\ncol: Name of the new column.\n...: Columns to combine.\nsep: Separator between combined values.\n\n\n\n\n\ndata &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Year = c(2022, 2023))\ndata |&gt; tidyr::unite(\"Name_Year\", Name, Year, sep = \"_\")"
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#in-depth-examples",
    "href": "appendix/r-packages/tidyr.html#in-depth-examples",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Summarize the mean age by gender and year.\n\n\n\n\ndata &lt;- data.frame(\n    ID = 1:4,\n    Gender = c(\"M\", \"F\", \"F\", \"M\"),\n    Age_2022 = c(25, 30, 27, 35),\n    Age_2023 = c(26, 31, 28, 36)\n)\n\ndata_long &lt;- data |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\n\nsummary &lt;- data_long |&gt;\n    dplyr::group_by(Gender, Year) |&gt;\n    dplyr::summarize(MeanAge = mean(Age), .groups = \"drop\")\nsummary\n\n\n  \n\n\n\n\n\n\nA summary table of mean ages by gender and year.\n\n\n\n\n\n\nCombine year and ID into a unique identifier.\n\n\n\n\ndata |&gt;\n    tidyr::unite(\"UniqueID\", ID, starts_with(\"Age\"), sep = \"-\")\n\n\n  \n\n\n\n\n\n\nA new column UniqueID combines values."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#advanced-features",
    "href": "appendix/r-packages/tidyr.html#advanced-features",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "data_long |&gt; dplyr::mutate(Year = paste0(\"Year_\", Year))\n\n\n  \n\n\n\n\n\n\n\nCombine with ggplot2 for visualization:\n\nlibrary(ggplot2)\n\ndata_long |&gt;\n    ggplot(aes(x = Year, y = Age, fill = Gender)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    labs(title = \"Age Distribution by Year\", x = \"Year\", y = \"Age\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nUse select() before pivoting to limit unnecessary columns.\nAvoid excessive pivoting by designing data collection appropriately."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/tidyr.html#troubleshooting-and-faqs",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Ensure column names are unique before pivoting.\n\n\n\nCheck the data for inconsistencies or missing values.\n\n\n\n\n\nCan tidyr handle large datasets? Yes, but consider chunking with data.table for very large data.\nHow do I preserve column types? Use tidyr::pivot_longer() or pivot_wider() with values_transform."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#best-practices",
    "href": "appendix/r-packages/tidyr.html#best-practices",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Start with well-structured raw data.\nUse descriptive column names and standard formats.\nAvoid overusing pivot_longer() or pivot_wider() for reversible tasks."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#conclusion",
    "href": "appendix/r-packages/tidyr.html#conclusion",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "tidyr simplifies reshaping and tidying data with a cohesive set of functions.\n\n\n\nExplore advanced tidying tasks with tidyr or integrate with other tidyverse tools like ggplot2 and dplyr."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#references-and-resources",
    "href": "appendix/r-packages/tidyr.html#references-and-resources",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "tidyr Documentation\nR for Data Science\nTidyverse GitHub"
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html",
    "href": "appendix/r-packages/sjPlot.html",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "",
    "text": "The sjPlot package provides tools for creating publication-ready visualizations and tables for statistical models, survey data, and cross-tabulations. Key functions include:\n\nsjPlot::plot_model(): Visualizes regression model effects\nsjPlot::tab_model(): Creates HTML/Markdown tables for model summaries\nsjPlot::tab_xtab(): Generates contingency tables with statistics\n\n\n\n\n\nVisualizing interactions in mixed-effects models\nReporting odds ratios from logistic regression\nCreating APA-style frequency tables\n\n\n\n\n\nUnified syntax for diverse model types (lm, glm, lme4)\nAutomatic effect size calculations\nSeamless integration with ggplot2 for customization"
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html#introduction",
    "href": "appendix/r-packages/sjPlot.html#introduction",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "",
    "text": "The sjPlot package provides tools for creating publication-ready visualizations and tables for statistical models, survey data, and cross-tabulations. Key functions include:\n\nsjPlot::plot_model(): Visualizes regression model effects\nsjPlot::tab_model(): Creates HTML/Markdown tables for model summaries\nsjPlot::tab_xtab(): Generates contingency tables with statistics\n\n\n\n\n\nVisualizing interactions in mixed-effects models\nReporting odds ratios from logistic regression\nCreating APA-style frequency tables\n\n\n\n\n\nUnified syntax for diverse model types (lm, glm, lme4)\nAutomatic effect size calculations\nSeamless integration with ggplot2 for customization"
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html#getting-started",
    "href": "appendix/r-packages/sjPlot.html#getting-started",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading the Package\n\npacman::p_load(sjPlot, lme4, ggplot2, ggpubr)\ntheme_set(theme_pubr())\n\n\n\nBasic Usage\n\n# Fit a linear model\nmodel &lt;- lm(mpg ~ wt + cyl + gear, data = mtcars)\n\n# Create effect plot\nplot_model(model, type = \"eff\", terms = \"wt\") |&gt;\n    ggplot2::ggtitle(\"Effect of Weight on MPG\")\n\n$title\n\n\n\n\n\n\n\n\n\n\n$subtitle\n[1] \"Effect of Weight on MPG\"\n\nattr(,\"class\")\n[1] \"labels\""
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html#key-functions-and-features",
    "href": "appendix/r-packages/sjPlot.html#key-functions-and-features",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "Key Functions and Features",
    "text": "Key Functions and Features\n\nplot_model()\nDescription: Visualizes regression coefficients or marginal effects Syntax:\nsjPlot::plot_model(\n  model, \n  type = c(\"est\", \"eff\", \"pred\"), \n  terms = NULL,\n  show.values = TRUE\n)\nExample:\n\n# Fit mixed-effects model\nmixed_model &lt;- lme4::lmer(Reaction ~ Days + (1 | Subject), data = sleepstudy)\n\n# Plot coefficients\nplot_model(mixed_model,\n    type = \"est\",\n    title = \"Sleep Study Coefficients\"\n) +\n    ggplot2::theme_minimal()\n\n\n\n\n\n\n\n\n\n\ntab_model()\nDescription: Creates formatted model summary tables Syntax:\nsjPlot::tab_model(\n  ...,\n  show.ci = TRUE,\n  show.aic = TRUE,\n  string.pred = \"Predictor\"\n)\nExample:\n\nglm_model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = binomial)\n\ntab_model(glm_model,\n    show.r2 = TRUE,\n    pred.labels = c(\"Intercept\", \"Weight\", \"MPG\")\n)\n\n\n\n\n \nvs\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\nIntercept\n0.00\n0.00 – 18.56\n0.139\n\n\nWeight\n1.79\n0.15 – 18.86\n0.623\n\n\nMPG\n1.69\n1.10 – 3.21\n0.044\n\n\nObservations\n32\n\n\nR2 Tjur\n0.478"
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html#in-depth-examples",
    "href": "appendix/r-packages/sjPlot.html#in-depth-examples",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "In-Depth Examples",
    "text": "In-Depth Examples\n\nExample 1: Prediction plot for Interaction Effects Visualization\nObjective: Demonstrate temperature × insulation interaction in housing data\n\ndata(housing, package = \"MASS\")\nmodel_interaction &lt;- glm(\n    Freq ~ Type * Cont + Sat + Infl,\n    family = poisson,\n    data = housing\n)\n\nanova(model_interaction)\n\n\n\n\n\n\nDf\nDeviance\nResid. Df\nResid. Dev\nPr(&gt;Chi)\n\n\n\n\nNULL\nNA\nNA\n71\n833.6570\nNA\n\n\nType\n3\n376.30001\n68\n457.3570\n0\n\n\nCont\n1\n38.83207\n67\n418.5249\n0\n\n\nSat\n2\n44.65689\n65\n373.8680\n0\n\n\nInfl\n2\n78.51627\n63\n295.3518\n0\n\n\nType:Cont\n3\n39.05780\n60\n256.2940\n0\n\n\n\n\n\n\n\nplot_model(\n    model_interaction,\n    type = \"pred\",\n    terms = c(\"Type\", \"Cont\"),\n    axis.title = \"Type of rental accommodation\"\n) +\n    ggplot2::scale_color_viridis_d()\n\n\n\n\n\n\n\nFigure 1: Prediction Plot of Interaction Effects"
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html#advanced-features",
    "href": "appendix/r-packages/sjPlot.html#advanced-features",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nCustomization\nModify table appearances using built-in themes:\n\ntab_model(glm_model,\n    CSS = list(css.table = \"color: blue;\"),\n    title = \"Customized Model Table\"\n)\n\n\n\n\n\n \nvs\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 18.56\n0.139\n\n\nwt\n1.79\n0.15 – 18.86\n0.623\n\n\nmpg\n1.69\n1.10 – 3.21\n0.044\n\n\nObservations\n32\n\n\nR2 Tjur\n0.478\n\n\n\n\n\nCustomized Model Table\n\n\n\nIntegration with Ggplot2\n\nplot_model(mixed_model, type = \"re\") +\n    ggplot2::scale_y_discrete(labels = c(\"Subject 1\", \"Subject 2\")) +\n    ggplot2::labs(caption = \"Random Effects Plot\")"
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/sjPlot.html#troubleshooting-and-faqs",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "Troubleshooting and FAQs",
    "text": "Troubleshooting and FAQs\n\nCommon Issues\nProblem: “Model class not supported” error Solution: Convert models using sjPlot::convert_to_sjmodel()\nProblem: Missing confidence intervals Solution: Specify ci.lvl in plot_model:\n\nplot_model(model, ci.lvl = 0.99)\n\n\n\n\n\n\n\n\n\n\nFAQs\nQ: How to export tables to Word? A: Use flextable::save_as_docx():\n\ntab_model(model) |&gt; flextable::save_as_docx(path = \"table.docx\")"
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html#best-practices",
    "href": "appendix/r-packages/sjPlot.html#best-practices",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse model wrappers for complex objects:\n\nsjPlot::tab_model(lme4::lmer(...))\n\nMaintain consistent theming:\n\n\nsjPlot::set_theme(\n    theme = \"539\",\n    axis.title.size = .9,\n    geom.label.size = 3\n)"
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html#conclusion",
    "href": "appendix/r-packages/sjPlot.html#conclusion",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummary\n\nsjPlot streamlines statistical reporting through unified visualization syntax\nKey functions support &gt;30 model types from common R packages\n\n\n\nNext Steps\n\nExplore plot_stackfrq() for Likert scale visualization\nExperiment with tab_mixed() for mixed-model comparisons"
  },
  {
    "objectID": "appendix/r-packages/sjPlot.html#references-and-resources",
    "href": "appendix/r-packages/sjPlot.html#references-and-resources",
    "title": "sjPlot: Visualizing Regression Models in R",
    "section": "References and Resources",
    "text": "References and Resources\n\nOfficial Documentation\nCRAN Vignettes\nGitHub Issues"
  },
  {
    "objectID": "appendix/r-packages/pwr.html",
    "href": "appendix/r-packages/pwr.html",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "The pwr package in R is a powerful tool for conducting power analysis. Power analysis is essential in determining the sample size needed to detect an effect of a given size with a certain level of confidence. It is widely used in experimental design and statistical hypothesis testing.\n\n\n\n\nDetermining the required sample size for an experiment.\nCalculating the power of a test given sample size and effect size.\nComparing the sensitivity of different statistical tests.\n\n\n\n\n\nSimplifies power analysis for various statistical tests.\nProvides built-in functions for common tests such as t-tests, ANOVA, and correlation.\nEasy integration with R workflows for reproducibility.\n\n\n\n\n\n\n\n\npacman::p_load(pwr)\n\n\n\n\nHere is a simple example of using pwr.t.test to calculate the sample size needed for a one-sample t-test:\n\n# Calculate required sample size for a one-sample t-test\nresult &lt;- pwr.t.test(d = 0.5, power = 0.8, sig.level = 0.05, type = \"one.sample\")\nprint(result)\n\n\n     One-sample t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nThis calculates the required sample size to achieve a power of 0.8 for detecting an effect size of 0.5 at a significance level of 0.05.\n\n\n\n\n\n\n\n\nCalculates power, effect size, or sample size for t-tests.\n\n\n\npwr.t.test(n = NULL, d = NULL, sig.level = NULL, power = NULL, type = \"two.sample\", alternative = \"two.sided\")\n\nn: Sample size.\nd: Effect size.\nsig.level: Significance level (default is 0.05).\npower: Desired power (1 - Type II error probability).\ntype: Type of t-test (\"one.sample\", \"two.sample\", or \"paired\").\nalternative: Specifies whether the test is \"two.sided\" or \"greater\"/\"less\".\n\n\n\n\n\n# Power analysis for a two-sample t-test\npwr.t.test(n = 30, d = 0.5, sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.5\n      sig.level = 0.05\n          power = 0.4778965\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\n\n\n\nPerforms power analysis for balanced one-way ANOVA.\n\n\n\npwr.anova.test(k = NULL, n = NULL, f = NULL, sig.level = NULL, power = NULL)\n\nk: Number of groups.\nn: Sample size per group.\nf: Effect size.\nsig.level: Significance level.\npower: Desired power.\n\n\n\n\n\n# Power analysis for a one-way ANOVA\npwr.anova.test(k = 4, f = 0.25, sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\n\n\n\n\n\n\nCalculates power for correlation tests.\n\n\n\npwr.r.test(n = NULL, r = NULL, sig.level = NULL, power = NULL, alternative = \"two.sided\")\n\nr: Correlation coefficient.\n\n\n\n\n\n# Power analysis for correlation\npwr.r.test(r = 0.3, sig.level = 0.05, power = 0.8)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 84.07364\n              r = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\n\n\n\n\n\nObjective: Determine the power of a study with a sample size of 40 per group, an effect size of 0.5, and a significance level of 0.05.\nCode:\n\nresult &lt;- pwr.t.test(n = 40, d = 0.5, sig.level = 0.05, type = \"two.sample\")\nprint(result)\n\n\n     Two-sample t test power calculation \n\n              n = 40\n              d = 0.5\n      sig.level = 0.05\n          power = 0.5981469\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nOutput Interpretation: The output provides the calculated power for the test.\n\n\n\nObjective: Calculate the required sample size to detect a correlation of 0.4 with 80% power at a 5% significance level.\nCode:\n\nresult &lt;- pwr.r.test(r = 0.4, power = 0.8, sig.level = 0.05)\nprint(result)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 45.91614\n              r = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nOutput Interpretation: The result specifies the required sample size.\n\n\n\n\n\n\nThe pwr package allows you to create custom plots of power curves:\n\nlibrary(ggplot2)\n\neffect_sizes &lt;- seq(0.1, 0.5, 0.01)\npowers &lt;- sapply(effect_sizes, function(d) {\n    pwr.t.test(d = d, n = 30, sig.level = 0.05, type = \"two.sample\")$power\n})\n\ndata &lt;- data.frame(effect_sizes, powers)\n\nggplot(data, aes(x = effect_sizes, y = powers)) +\n    geom_line() +\n    labs(title = \"Power Curve\", x = \"Effect Size\", y = \"Power\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nThe pwr package can work seamlessly with simulation packages to validate power calculations.\n\n\n\n\n\nCommon Error: “Error in pwr.t.test(): Not enough parameters specified.” Fix: Ensure you provide at least three of the four parameters: n, d, power, or sig.level.\nHow to choose effect size? Use guidelines such as Cohen’s benchmarks: small (0.2), medium (0.5), and large (0.8).\n\n\n\n\n\nAlways use realistic estimates for effect size based on prior studies or pilot data.\nValidate your power calculations through simulations if possible.\n\n\n\n\n\n\nThe pwr package is a versatile and user-friendly tool for power analysis in R, covering various statistical tests and scenarios.\n\n\n\nExplore the official documentation or combine pwr with simulation-based methods for complex experimental designs.\n\n\n\nStart applying power analysis in your projects to optimize experimental designs and improve the validity of your results.\n\n\n\n\n\npwr Package Documentation\nR Power Analysis Tutorial\nComprehensive R Archive Network (CRAN)"
  },
  {
    "objectID": "appendix/r-packages/pwr.html#introduction",
    "href": "appendix/r-packages/pwr.html#introduction",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "The pwr package in R is a powerful tool for conducting power analysis. Power analysis is essential in determining the sample size needed to detect an effect of a given size with a certain level of confidence. It is widely used in experimental design and statistical hypothesis testing.\n\n\n\n\nDetermining the required sample size for an experiment.\nCalculating the power of a test given sample size and effect size.\nComparing the sensitivity of different statistical tests.\n\n\n\n\n\nSimplifies power analysis for various statistical tests.\nProvides built-in functions for common tests such as t-tests, ANOVA, and correlation.\nEasy integration with R workflows for reproducibility."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#getting-started",
    "href": "appendix/r-packages/pwr.html#getting-started",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "pacman::p_load(pwr)\n\n\n\n\nHere is a simple example of using pwr.t.test to calculate the sample size needed for a one-sample t-test:\n\n# Calculate required sample size for a one-sample t-test\nresult &lt;- pwr.t.test(d = 0.5, power = 0.8, sig.level = 0.05, type = \"one.sample\")\nprint(result)\n\n\n     One-sample t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nThis calculates the required sample size to achieve a power of 0.8 for detecting an effect size of 0.5 at a significance level of 0.05."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#key-functions-and-features",
    "href": "appendix/r-packages/pwr.html#key-functions-and-features",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "Calculates power, effect size, or sample size for t-tests.\n\n\n\npwr.t.test(n = NULL, d = NULL, sig.level = NULL, power = NULL, type = \"two.sample\", alternative = \"two.sided\")\n\nn: Sample size.\nd: Effect size.\nsig.level: Significance level (default is 0.05).\npower: Desired power (1 - Type II error probability).\ntype: Type of t-test (\"one.sample\", \"two.sample\", or \"paired\").\nalternative: Specifies whether the test is \"two.sided\" or \"greater\"/\"less\".\n\n\n\n\n\n# Power analysis for a two-sample t-test\npwr.t.test(n = 30, d = 0.5, sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.5\n      sig.level = 0.05\n          power = 0.4778965\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\n\n\n\nPerforms power analysis for balanced one-way ANOVA.\n\n\n\npwr.anova.test(k = NULL, n = NULL, f = NULL, sig.level = NULL, power = NULL)\n\nk: Number of groups.\nn: Sample size per group.\nf: Effect size.\nsig.level: Significance level.\npower: Desired power.\n\n\n\n\n\n# Power analysis for a one-way ANOVA\npwr.anova.test(k = 4, f = 0.25, sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\n\n\n\n\n\n\nCalculates power for correlation tests.\n\n\n\npwr.r.test(n = NULL, r = NULL, sig.level = NULL, power = NULL, alternative = \"two.sided\")\n\nr: Correlation coefficient.\n\n\n\n\n\n# Power analysis for correlation\npwr.r.test(r = 0.3, sig.level = 0.05, power = 0.8)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 84.07364\n              r = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided"
  },
  {
    "objectID": "appendix/r-packages/pwr.html#in-depth-examples",
    "href": "appendix/r-packages/pwr.html#in-depth-examples",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "Objective: Determine the power of a study with a sample size of 40 per group, an effect size of 0.5, and a significance level of 0.05.\nCode:\n\nresult &lt;- pwr.t.test(n = 40, d = 0.5, sig.level = 0.05, type = \"two.sample\")\nprint(result)\n\n\n     Two-sample t test power calculation \n\n              n = 40\n              d = 0.5\n      sig.level = 0.05\n          power = 0.5981469\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nOutput Interpretation: The output provides the calculated power for the test.\n\n\n\nObjective: Calculate the required sample size to detect a correlation of 0.4 with 80% power at a 5% significance level.\nCode:\n\nresult &lt;- pwr.r.test(r = 0.4, power = 0.8, sig.level = 0.05)\nprint(result)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 45.91614\n              r = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nOutput Interpretation: The result specifies the required sample size."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#advanced-features",
    "href": "appendix/r-packages/pwr.html#advanced-features",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "The pwr package allows you to create custom plots of power curves:\n\nlibrary(ggplot2)\n\neffect_sizes &lt;- seq(0.1, 0.5, 0.01)\npowers &lt;- sapply(effect_sizes, function(d) {\n    pwr.t.test(d = d, n = 30, sig.level = 0.05, type = \"two.sample\")$power\n})\n\ndata &lt;- data.frame(effect_sizes, powers)\n\nggplot(data, aes(x = effect_sizes, y = powers)) +\n    geom_line() +\n    labs(title = \"Power Curve\", x = \"Effect Size\", y = \"Power\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nThe pwr package can work seamlessly with simulation packages to validate power calculations."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/pwr.html#troubleshooting-and-faqs",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "Common Error: “Error in pwr.t.test(): Not enough parameters specified.” Fix: Ensure you provide at least three of the four parameters: n, d, power, or sig.level.\nHow to choose effect size? Use guidelines such as Cohen’s benchmarks: small (0.2), medium (0.5), and large (0.8)."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#best-practices",
    "href": "appendix/r-packages/pwr.html#best-practices",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "Always use realistic estimates for effect size based on prior studies or pilot data.\nValidate your power calculations through simulations if possible."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#conclusion",
    "href": "appendix/r-packages/pwr.html#conclusion",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "The pwr package is a versatile and user-friendly tool for power analysis in R, covering various statistical tests and scenarios.\n\n\n\nExplore the official documentation or combine pwr with simulation-based methods for complex experimental designs.\n\n\n\nStart applying power analysis in your projects to optimize experimental designs and improve the validity of your results."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#references-and-resources",
    "href": "appendix/r-packages/pwr.html#references-and-resources",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "pwr Package Documentation\nR Power Analysis Tutorial\nComprehensive R Archive Network (CRAN)"
  },
  {
    "objectID": "appendix/r-packages/multcomp.html",
    "href": "appendix/r-packages/multcomp.html",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "",
    "text": "The multcomp package in R is designed for simultaneous inference in linear models, specifically focused on multiple comparisons. It provides tools to test hypotheses about multiple parameters in a model while controlling for family-wise error rates.\n\n\n\n\nComparing group means in ANOVA.\nSimultaneous confidence intervals.\nAdjusting p-values for multiple testing.\n\n\n\n\n\nHandles a variety of multiple comparison tests.\nProvides flexibility with user-defined contrasts.\nSupports many types of linear models, including generalized linear models and mixed-effects models."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#introduction",
    "href": "appendix/r-packages/multcomp.html#introduction",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "",
    "text": "The multcomp package in R is designed for simultaneous inference in linear models, specifically focused on multiple comparisons. It provides tools to test hypotheses about multiple parameters in a model while controlling for family-wise error rates.\n\n\n\n\nComparing group means in ANOVA.\nSimultaneous confidence intervals.\nAdjusting p-values for multiple testing.\n\n\n\n\n\nHandles a variety of multiple comparison tests.\nProvides flexibility with user-defined contrasts.\nSupports many types of linear models, including generalized linear models and mixed-effects models."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#getting-started",
    "href": "appendix/r-packages/multcomp.html#getting-started",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\nLoading the Package\n\n# Load the multcomp package\npacman::p_load(multcomp)\n\n\n\nBasic Usage\nLet’s start with a simple example:\n\n# Example dataset\ndata(warpbreaks)\n\n# Fit a linear model\nmodel &lt;- lm(breaks ~ wool + tension, data = warpbreaks)\n\n# Perform multiple comparisons\ncomparison &lt;- glht(model, linfct = mcp(tension = \"Tukey\"))\n\n# Summarize the results\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = breaks ~ wool + tension, data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error t value Pr(&gt;|t|)   \nM - L == 0  -10.000      3.872  -2.582   0.0336 * \nH - L == 0  -14.722      3.872  -3.802   0.0011 **\nH - M == 0   -4.722      3.872  -1.219   0.4475   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\nExplanation\n\nmcp: Specifies the type of multiple comparisons (e.g., Tukey’s test).\nglht: General linear hypothesis testing function.\nsummary: Displays adjusted p-values and confidence intervals."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#key-functions-and-features",
    "href": "appendix/r-packages/multcomp.html#key-functions-and-features",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "3. Key Functions and Features",
    "text": "3. Key Functions and Features\n\nglht()\nDescription: Performs multiple comparisons or general linear hypothesis tests.\nSyntax:\nmultcomp::glht(model, linfct = mcp(\"factor\"=\"method\"))\nArguments:\n\nmodel: A fitted linear model object.\nlinfct: Defines the hypotheses to test (e.g., Tukey or user-defined contrasts).\n\nExample:\n\ncomparison &lt;- glht(model, linfct = mcp(tension = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = breaks ~ wool + tension, data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error t value Pr(&gt;|t|)   \nM - L == 0  -10.000      3.872  -2.582  0.03353 * \nH - L == 0  -14.722      3.872  -3.802  0.00109 **\nH - M == 0   -4.722      3.872  -1.219  0.44746   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\n\nmcp()\nDescription: Creates predefined contrasts for a factor.\nSyntax:\nmcp(factor = \"method\")\nExample:\nlinfct = mcp(tension = \"Tukey\")"
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#in-depth-examples",
    "href": "appendix/r-packages/multcomp.html#in-depth-examples",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "4. In-Depth Examples",
    "text": "4. In-Depth Examples\n\nExample 1: ANOVA Multiple Comparisons\nObjective: Compare tension levels in warpbreaks dataset.\nSteps:\n\nFit the model:\n\n\nmodel &lt;- aov(breaks ~ tension, data = warpbreaks)\n\n\nPerform Tukey’s test:\n\n\ncomparison &lt;- glht(model, linfct = mcp(tension = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = breaks ~ tension, data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error t value Pr(&gt;|t|)   \nM - L == 0  -10.000      3.960  -2.525  0.03853 * \nH - L == 0  -14.722      3.960  -3.718  0.00142 **\nH - M == 0   -4.722      3.960  -1.192  0.46306   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\nVisualize the results:\n\n\nplot(comparison)\n\n\n\n\n\n\n\n\nOutput: Provides adjusted p-values and confidence intervals."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#advanced-features",
    "href": "appendix/r-packages/multcomp.html#advanced-features",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "5. Advanced Features",
    "text": "5. Advanced Features\n\nCustom Contrasts\nCreate user-defined contrasts:\n\ncontrast_matrix &lt;- rbind(\n    \"M - L\" = c(-1, 1, 0),\n    \"H - L\" = c(-1, 0, 1),\n    \"H - M\" = c(0, -1, 1)\n)\ncomparison &lt;- glht(model, linfct = contrast_matrix)\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: aov(formula = breaks ~ tension, data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error t value Pr(&gt;|t|)    \nM - L == 0  -46.389      6.262  -7.408   &lt;1e-04 ***\nH - L == 0  -51.111      6.262  -8.163   &lt;1e-04 ***\nH - M == 0   -4.722      3.960  -1.192    0.456    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\n\nIntegration with Mixed-Effects Models\nSupports models from lme4:\n\nlibrary(lme4)\nmodel &lt;- lmer(breaks ~ tension + (1 | wool), data = warpbreaks)\ncomparison &lt;- glht(model, linfct = mcp(tension = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = breaks ~ tension + (1 | wool), data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error z value Pr(&gt;|z|)    \nM - L == 0  -10.000      3.872  -2.582   0.0266 *  \nH - L == 0  -14.722      3.872  -3.802   &lt;0.001 ***\nH - M == 0   -4.722      3.872  -1.219   0.4416    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)"
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/multcomp.html#troubleshooting-and-faqs",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "6. Troubleshooting and FAQs",
    "text": "6. Troubleshooting and FAQs\n\nCommon Issues\n\nError: Non-conformable arguments\n\nEnsure the linfct matches the model’s factor levels.\n\nConfusion with model types\n\nUse glht with models that support linear hypotheses (e.g., lm, lmer).\n\n\n\n\nFAQs\n\nCan multcomp handle unbalanced data? Yes, but ensure your model accounts for this."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#best-practices",
    "href": "appendix/r-packages/multcomp.html#best-practices",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "7. Best Practices",
    "text": "7. Best Practices\n\nAlways check model diagnostics before multiple comparisons.\nUse visualizations like plot() to complement numerical results.\nFor large datasets, consider summarizing key results to avoid overloading the output."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#conclusion",
    "href": "appendix/r-packages/multcomp.html#conclusion",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "8. Conclusion",
    "text": "8. Conclusion\n\nSummary\nThe multcomp package is a robust tool for conducting multiple comparisons in R, providing flexibility and accuracy.\n\n\nNext Steps\nExplore interactions in models and extend functionality with packages like emmeans.\n\n\nEncouragement\nExperiment with different datasets to build confidence in interpreting outputs!"
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#references-and-resources",
    "href": "appendix/r-packages/multcomp.html#references-and-resources",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "9. References and Resources",
    "text": "9. References and Resources\n\nPackage Documentation\nExamples on CRAN"
  },
  {
    "objectID": "appendix/r-packages/ggplot2.html",
    "href": "appendix/r-packages/ggplot2.html",
    "title": "ggplot2: A Comprehensive Guide",
    "section": "",
    "text": "Introduction\nOverview The ggplot2 package (part of the Tidyverse) provides a powerful system for creating graphics in R by implementing the Grammar of Graphics. It uses layers to gradually build complex visualizations from simple components. Packages and Key Functions:\n\nggplot2 (key functions include ggplot(), aes(), geom_point(), geom_bar(), geom_line(), geom_histogram(), facet_wrap(), theme(), etc.)\n\nUse Cases\n\nExploratory data analysis: Quick visualization of relationships in data.\nPublication-ready plots: Highly customizable appearance and layout.\nInteractive or iterative workflows: Build complex visuals step-by-step.\n\nBenefits\n\nLayered approach allows modular building of plots.\nConsistent grammar for different plot types.\nPowerful customizations (labels, scales, themes, etc.).\n\nggplot2::ggplot(data, aes(x, y)) + geom_*()\n(The skeleton structure above demonstrates how layers are added; it is not meant to run by itself.)\nBelow is a schematic (using Mermaid) illustrating the layered approach in ggplot2:\n\ngraph LR\n    A[\"Begin with ggplot()\"] --&gt; B[\"Add aes() layer\"]\n    B --&gt; C[\"Add geom_*() layer\"]\n    C --&gt; D[\"Optional: Add facet or theme layers\"]\n\n\n\n\ngraph LR\n    A[\"Begin with ggplot()\"] --&gt; B[\"Add aes() layer\"]\n    B --&gt; C[\"Add geom_*() layer\"]\n    C --&gt; D[\"Optional: Add facet or theme layers\"]\n\n\n\n\n\n\n\n\nGetting Started\nLoading the Package You can load ggplot2 with pacman::p_load():\n\npacman::p_load(ggplot2)\n\nBasic Usage Below is a simple example to illustrate the core functionality of ggplot2. We will use the built-in mtcars dataset to create a scatter plot of mpg (miles per gallon) vs. hp (horsepower).\n\n# Scatter plot of mpg vs. hp\nmtcars |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = hp, y = mpg)) +\n    ggplot2::geom_point() +\n    ggplot2::labs(\n        title = \"Basic Scatter Plot\",\n        x = \"Horsepower\",\n        y = \"Miles Per Gallon\"\n    )\n\n\n\n\n\n\n\n\nIn this code:\n\nmtcars is piped into ggplot2::ggplot().\nggplot2::aes() maps hp to the x-axis and mpg to the y-axis.\nggplot2::geom_point() adds the point geometry to create a scatter plot.\nggplot2::labs() customizes labels.\n\n\n\nKey Functions and Features\nggplot2::ggplot() Description\n\nThe main function to initialize a ggplot object, where you specify the dataset and general aesthetic mappings.\n\nSyntax\nggplot2::ggplot(data, ggplot2::aes(...))\n\ndata: A data frame.\nggplot2::aes(…): Aesthetic mappings for x, y, color, size, etc.\n\nExample\n\n# Basic template (with mpg dataset from ggplot2)\nggplot2::ggplot(data = ggplot2::mpg, ggplot2::aes(x = displ, y = hwy)) +\n    ggplot2::geom_point()\n\n\n\n\n\n\n\n\nggplot2::aes() Description\n\nDefines the mapping of variables to visual properties (aesthetics) such as x, y, color, fill, size, shape, and more.\n\nSyntax\nggplot2::aes(x, y, color = ..., fill = ..., shape = ...)\nExample\n\nggplot2::ggplot(ggplot2::mpg, ggplot2::aes(x = cyl, y = hwy, color = drv)) +\n    ggplot2::geom_point()\n\n\n\n\n\n\n\n\n\nHere, drv is mapped to color, creating different point colors by drive type.\n\nggplot2::geom_*() Description\n\nA family of geometric functions to specify how data points are represented (points, lines, bars, etc.).\n\nSyntax\nggplot2::geom_point(...)\nggplot2::geom_line(...)\nggplot2::geom_bar(...)\nggplot2::geom_histogram(...)\nArguments often include statistical transformations, bin widths, position adjustments, etc.\nExample: Bar Plot\n\nggplot2::ggplot(ggplot2::mpg, ggplot2::aes(x = class)) +\n    ggplot2::geom_bar()\n\n\n\n\n\n\n\n\n\nDisplays the count of each vehicle class in the mpg dataset.\n\nggplot2::facet_wrap() Description\n\nSplits data into subplots (facets) based on one or more categorical variables.\n\nSyntax\nggplot2::facet_wrap(~ variable, ncol = ...)\nExample\n\nggplot2::ggplot(ggplot2::mpg, ggplot2::aes(x = displ, y = hwy)) +\n    ggplot2::geom_point() +\n    ggplot2::facet_wrap(~drv)\n\n\n\n\n\n\n\n\n\nCreates separate panels for each drive type (drv).\n\nggplot2::theme() Description\n\nAdjusts non-data components (text size, legend placement, background, etc.).\n\nSyntax\nggplot2::theme(\n  panel.background = element_rect(...),\n  axis.text.x = element_text(...),\n  ...\n)\nExample\n\nggplot2::ggplot(ggplot2::mpg, ggplot2::aes(x = displ, y = hwy)) +\n    ggplot2::geom_point() +\n    ggplot2::theme_minimal()\n\n\n\n\n\n\n\n\n\nApplies a minimal theme to reduce chart ink and highlight data.\n\n\n\nIn-Depth Examples\nTitle: Comparing Engine Displacement and Fuel Efficiency Objective\n\nVisualize relationships between engine displacement (displ) and highway fuel efficiency (hwy) for different car classes.\n\nSteps\n\nData Preparation We will use the built-in mpg dataset and select relevant columns: displ, hwy, class.\nFunction Application\n\nInitialize the plot with ggplot2::ggplot().\nMap displ and hwy using ggplot2::aes().\nAdd geom_point().\nFacet the plot by car class using ggplot2::facet_wrap().\nAdd a custom theme.\n\nVisualization\n\n\n# Step-by-step example\nfiltered_mpg &lt;- ggplot2::mpg\n\nfiltered_mpg |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = displ, y = hwy, color = class)) +\n    ggplot2::geom_point(size = 3) +\n    ggplot2::facet_wrap(~class) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        title = \"Engine Displacement vs. Highway MPG by Class\",\n        subtitle = \"Faceted by Vehicle Class\",\n        x = \"Engine Displacement (Liters)\",\n        y = \"Highway Miles per Gallon\",\n        color = \"Vehicle Class\"\n    )\n\n\n\n\n\n\n\n\nOutput\n\nYou will see a facetted scatter plot showing how different classes compare in the displ-hwy relationship. The color indicates each class, and each facet displays data for one class.\n\n\n\nAdvanced Features\nCustomization\n\nModify legends, labels, and scales with functions like ggplot2::labs(), ggplot2::scale_color_manual(), or ggplot2::theme().\nAdjust axis transformations (e.g., log scale) with ggplot2::scale_x_log10() or ggplot2::scale_y_log10().\n\nIntegration\n\nCombine ggplot2 with dplyr for data wrangling:\n\n\nlibrary(dplyr)\n\nggplot2::mpg |&gt;\n    dplyr::filter(displ &lt; 5) |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = displ, y = cty)) +\n    ggplot2::geom_point() +\n    ggplot2::geom_smooth(method = \"lm\") +\n    ggplot2::theme_minimal()\n\n\n\n\n\n\n\n\n\nThis uses dplyr::filter() to remove rows with displacement &gt;= 5, then plots with ggplot2.\n\nOptimization\n\nFor large datasets, consider sampling or binning (geom_bin2d, geom_hex from other packages).\nUse efficient data handling (data.table or arrow) before feeding data to ggplot2 for plotting.\nLimit layering of overly detailed geoms when performance is a concern.\n\n\n\nTroubleshooting and FAQs\nCommon Issues\n\n“Error in ggplot(…): object not found”: Ensure the dataset or column name matches correctly and is in scope.\n“Discrete value supplied to continuous scale”: A variable mapped to x or y might be character instead of numeric; convert if necessary.\nMissing + sign between layers: Each layer is added with the + operator; forgetting it can break the chain of commands.\n\nFAQs\n\nHow do I rotate x-axis labels?\n\nUse ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 90, hjust = 1)).\n\nCan I save my plots?\n\nYes, with ggplot2::ggsave(“plot.png”, width = …, height = …).\n\nHow do I create interactive plots?\n\nUse packages like plotly or ggiraph that build on ggplot2.\n\n\n\n\nBest Practices\n\nKeep code modular: Prepare and clean data first, then pipe into ggplot2 functions.\nUse meaningful variable and aesthetic mappings for readability.\nWrite reproducible scripts by incorporating set.seed() for sampling or random processes.\nWhen presenting results (including power analysis in broader data workflows), ensure the data, code, and figure output are clearly documented.\n\n\n\nConclusion\nSummary We have explored the ggplot2 package’s layered approach to building plots, introduced key functions, and provided examples for customizing, faceting, and integrating with other data pipelines.\nNext Steps\n\nExperiment with specialized geoms (e.g., boxplots, violin plots).\nExplore advanced theming options (e.g., ggthemes package).\nInvestigate extension packages like gganimate for animated graphics or patchwork for arranging multiple ggplots.\n\n\n\nReferences and Resources\nDocumentation\n\nggplot2 Official Documentation\n\nLearning Materials\n\nR for Data Science by Hadley Wickham and Garrett Grolemund\nOnline tutorials on the RStudio Community\n\nSupport Channels\n\nGitHub Issues for ggplot2\nStack Overflow (r + ggplot2 tag)\nLocal R User Groups or R-Ladies meetups for community support"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html",
    "href": "appendix/r-packages/emmeans-v1.html",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "emmeans (Estimated Marginal Means) is an R package designed for the analysis of linear models. It provides tools for obtaining and visualizing adjusted means for factors in statistical models, allowing users to interpret complex model outputs more easily.\n\n\n\n\nPost-hoc Comparisons: Evaluating differences between group means after fitting a model.\nVisualization: Creating clear and informative plots of marginal means.\nModel Checking: Assessing the fit and assumptions of linear models.\n\n\n\n\n\nSimplifies the interpretation of model outputs.\nFacilitates the comparison of means across different factors.\nIntegrates seamlessly with various types of models in R.\n\n\n\n\n\n\n\nFirst, load the emmeans package into your R session:\n\nlibrary(emmeans)\n\n\n\n\nHere’s a simple example using a linear model:\n\n# Sample data\ndata(mtcars)\n\n# Fit a linear model\nmodel &lt;- lm(mpg ~ factor(cyl), data = mtcars)\n\n# Obtain estimated marginal means\nemm &lt;- emmeans(model, ~cyl)\nsummary(emm)\n\n\n  \n\n\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\n       4        6        8 \n26.66364 19.74286 15.10000 \n\n\nAnnotation: This will provide the estimated marginal means of miles per gallon (mpg) adjusted for the number of cylinders (cyl).\n\n\n\n\n\n\n\nDescription: Computes estimated marginal means for specified factors in a model.\nSyntax:\n\nemmeans(object, specs, ...)\n\nobject: A fitted model object.\nspecs: Terms for which to compute marginal means.\nExample:\n\n\n# Example of emmeans\nemm_cyl &lt;- emmeans(model, ~cyl)\nprint(emm_cyl)\n\n cyl emmean    SE df lower.CL upper.CL\n   4   26.7 0.972 29     24.7     28.7\n   6   19.7 1.220 29     17.3     22.2\n   8   15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n\nThis will display the estimated marginal means for each cylinder category.\n\n\n\n\nDescription: Computes pairwise comparisons among estimated marginal means.\nSyntax:\npairs(x, ...)\n\nx: An object of class emmeans.\n\nExample:\n\n\n# Pairwise comparisons\npairs(emm_cyl)\n\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0003\n cyl4 - cyl8    11.56 1.30 29   8.905  &lt;.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThis gives pairwise comparisons between the cylinder levels.\n\n\n\n\n\n\n\n\nTo evaluate how the number of cylinders affects the miles per gallon (mpg) in cars.\n\n\n\n\nData Preparation:\n\n\ndata(mtcars)\nmodel &lt;- lm(mpg ~ factor(cyl), data = mtcars)\n\n\nFunction Application:\n\n\nemm_cyl &lt;- emmeans(model, ~cyl)\n\n\nVisualization:\n\n\nplot(emm_cyl)\n\n\n\n\n\n\n\n\n\nOutput: The plot will show the estimated marginal means of mpg for each cylinder group, allowing for quick visual comparisons.\n\n\n\n\n\n\n\n\nYou can customize the output of emmeans using additional arguments:\n\nemm_cyl &lt;- emmeans(model, ~cyl, at = list(cyl = c(4, 6, 8)))\n\n\n\n\nemmeans can be used with packages like ggplot2 for enhanced visualizations:\n\nlibrary(ggplot2)\nemm_df &lt;- as.data.frame(emm_cyl)\nggplot(emm_df, aes(x = cyl, y = emmean)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError: Object not found: Ensure the model object exists and is correctly specified.\nWarnings about convergence: Check the model fitting process for issues related to data or model complexity.\n\n\n\n\n\nQ: Can emmeans be used with non-linear models?\n\nA: Yes, emmeans supports various model types, including generalized linear models.\n\n\n\n\n\n\n\nUse clear and descriptive variable names when fitting models.\nRegularly check model diagnostics to ensure assumptions are met.\nDocument your analysis steps for reproducibility.\n\n\n\n\n\n\nIn this tutorial, we explored the emmeans package, covering its key functions, in-depth examples, and advanced features.\n\n\n\nTry applying emmeans to your own datasets or explore more advanced statistical models.\n\n\n\nDive into your data analysis projects with confidence, utilizing the power of emmeans!\n\n\n\n\n\nDocumentation: emmeans Documentation\nLearning Materials: R for Data Science\nSupport Channels: GitHub Issues, Stack Overflow\n\n\ngraph TD;\n    A[Start] --&gt; B[Load Package];\n    B --&gt; C[Fit Model];\n    C --&gt; D[Compute EMM];\n    D --&gt; E[Visualize Results];\n    E --&gt; F[Interpret Output];\n    F --&gt; G[Explore Further];\n    G --&gt; H[End];\n\n\n\n\ngraph TD;\n    A[Start] --&gt; B[Load Package];\n    B --&gt; C[Fit Model];\n    C --&gt; D[Compute EMM];\n    D --&gt; E[Visualize Results];\n    E --&gt; F[Interpret Output];\n    F --&gt; G[Explore Further];\n    G --&gt; H[End];"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#introduction",
    "href": "appendix/r-packages/emmeans-v1.html#introduction",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "emmeans (Estimated Marginal Means) is an R package designed for the analysis of linear models. It provides tools for obtaining and visualizing adjusted means for factors in statistical models, allowing users to interpret complex model outputs more easily.\n\n\n\n\nPost-hoc Comparisons: Evaluating differences between group means after fitting a model.\nVisualization: Creating clear and informative plots of marginal means.\nModel Checking: Assessing the fit and assumptions of linear models.\n\n\n\n\n\nSimplifies the interpretation of model outputs.\nFacilitates the comparison of means across different factors.\nIntegrates seamlessly with various types of models in R."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#getting-started",
    "href": "appendix/r-packages/emmeans-v1.html#getting-started",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "First, load the emmeans package into your R session:\n\nlibrary(emmeans)\n\n\n\n\nHere’s a simple example using a linear model:\n\n# Sample data\ndata(mtcars)\n\n# Fit a linear model\nmodel &lt;- lm(mpg ~ factor(cyl), data = mtcars)\n\n# Obtain estimated marginal means\nemm &lt;- emmeans(model, ~cyl)\nsummary(emm)\n\n\n  \n\n\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\n       4        6        8 \n26.66364 19.74286 15.10000 \n\n\nAnnotation: This will provide the estimated marginal means of miles per gallon (mpg) adjusted for the number of cylinders (cyl)."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#key-functions-and-features",
    "href": "appendix/r-packages/emmeans-v1.html#key-functions-and-features",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "Description: Computes estimated marginal means for specified factors in a model.\nSyntax:\n\nemmeans(object, specs, ...)\n\nobject: A fitted model object.\nspecs: Terms for which to compute marginal means.\nExample:\n\n\n# Example of emmeans\nemm_cyl &lt;- emmeans(model, ~cyl)\nprint(emm_cyl)\n\n cyl emmean    SE df lower.CL upper.CL\n   4   26.7 0.972 29     24.7     28.7\n   6   19.7 1.220 29     17.3     22.2\n   8   15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n\nThis will display the estimated marginal means for each cylinder category.\n\n\n\n\nDescription: Computes pairwise comparisons among estimated marginal means.\nSyntax:\npairs(x, ...)\n\nx: An object of class emmeans.\n\nExample:\n\n\n# Pairwise comparisons\npairs(emm_cyl)\n\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0003\n cyl4 - cyl8    11.56 1.30 29   8.905  &lt;.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThis gives pairwise comparisons between the cylinder levels."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#in-depth-examples",
    "href": "appendix/r-packages/emmeans-v1.html#in-depth-examples",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "To evaluate how the number of cylinders affects the miles per gallon (mpg) in cars.\n\n\n\n\nData Preparation:\n\n\ndata(mtcars)\nmodel &lt;- lm(mpg ~ factor(cyl), data = mtcars)\n\n\nFunction Application:\n\n\nemm_cyl &lt;- emmeans(model, ~cyl)\n\n\nVisualization:\n\n\nplot(emm_cyl)\n\n\n\n\n\n\n\n\n\nOutput: The plot will show the estimated marginal means of mpg for each cylinder group, allowing for quick visual comparisons."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#advanced-features",
    "href": "appendix/r-packages/emmeans-v1.html#advanced-features",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "You can customize the output of emmeans using additional arguments:\n\nemm_cyl &lt;- emmeans(model, ~cyl, at = list(cyl = c(4, 6, 8)))\n\n\n\n\nemmeans can be used with packages like ggplot2 for enhanced visualizations:\n\nlibrary(ggplot2)\nemm_df &lt;- as.data.frame(emm_cyl)\nggplot(emm_df, aes(x = cyl, y = emmean)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL))"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/emmeans-v1.html#troubleshooting-and-faqs",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "Error: Object not found: Ensure the model object exists and is correctly specified.\nWarnings about convergence: Check the model fitting process for issues related to data or model complexity.\n\n\n\n\n\nQ: Can emmeans be used with non-linear models?\n\nA: Yes, emmeans supports various model types, including generalized linear models."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#best-practices",
    "href": "appendix/r-packages/emmeans-v1.html#best-practices",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "Use clear and descriptive variable names when fitting models.\nRegularly check model diagnostics to ensure assumptions are met.\nDocument your analysis steps for reproducibility."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#conclusion",
    "href": "appendix/r-packages/emmeans-v1.html#conclusion",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "In this tutorial, we explored the emmeans package, covering its key functions, in-depth examples, and advanced features.\n\n\n\nTry applying emmeans to your own datasets or explore more advanced statistical models.\n\n\n\nDive into your data analysis projects with confidence, utilizing the power of emmeans!"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#references-and-resources",
    "href": "appendix/r-packages/emmeans-v1.html#references-and-resources",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "Documentation: emmeans Documentation\nLearning Materials: R for Data Science\nSupport Channels: GitHub Issues, Stack Overflow\n\n\ngraph TD;\n    A[Start] --&gt; B[Load Package];\n    B --&gt; C[Fit Model];\n    C --&gt; D[Compute EMM];\n    D --&gt; E[Visualize Results];\n    E --&gt; F[Interpret Output];\n    F --&gt; G[Explore Further];\n    G --&gt; H[End];\n\n\n\n\ngraph TD;\n    A[Start] --&gt; B[Load Package];\n    B --&gt; C[Fit Model];\n    C --&gt; D[Compute EMM];\n    D --&gt; E[Visualize Results];\n    E --&gt; F[Interpret Output];\n    F --&gt; G[Explore Further];\n    G --&gt; H[End];"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#understanding-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#understanding-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "1. Understanding Interactions",
    "text": "1. Understanding Interactions\nInteractions occur when the effect of one predictor variable on the response variable depends on the level of another predictor. For example, the relationship between treatment and outcome may vary depending on gender."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "2. Fitting a Model with Interactions",
    "text": "2. Fitting a Model with Interactions\n\nExample\nLet’s create a model with an interaction between two factors, treatment and gender, on the response variable outcome.\n\n# Sample data\ndata(mtcars)\n\n# Create a categorical variable for the example\nmtcars$am &lt;- factor(mtcars$am, labels = c(\"Automatic\", \"Manual\"))\n\n# Fit a linear model with interaction\nmodel &lt;- lm(mpg ~ am * cyl, data = mtcars)"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3. Using emmeans for Interactions",
    "text": "3. Using emmeans for Interactions\n\nCompute Estimated Marginal Means\nTo compute estimated marginal means for the interaction, use the emmeans() function with the interaction term specified.\n\n# Obtain estimated marginal means for the interaction\nemm_interaction &lt;- emmeans(model, ~ am * cyl)\nsummary(emm_interaction)\n\n\n  \n\n\n\n\n\nInterpret the Output\nThe output will give you the estimated marginal means for each combination of the levels of am and cyl. This allows for examining how the means differ based on the interaction."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#visualizing-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#visualizing-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4. Visualizing Interactions",
    "text": "4. Visualizing Interactions\nVisualizing interactions can help in understanding how the effects vary. Use the plot() function to create interaction plots.\n\n# Interaction plot\nplot(emm_interaction)\n\n\n\n\n\n\n\n\n\nCustomizing the Plot\nYou can further customize the plot using ggplot2 for enhanced visual representation:\n\nlibrary(ggplot2)\n\n# Convert emmeans object to a data frame\nemm_df &lt;- as.data.frame(emm_interaction)\n\n# Create a ggplot\nggplot(emm_df, aes(x = cyl, y = emmean, color = am)) +\n    geom_point() +\n    geom_line() +\n    labs(\n        title = \"Interaction between Treatment and Cylinder\",\n        x = \"Number of Cylinders\",\n        y = \"Estimated Marginal Mean MPG\"\n    ) +\n    theme_minimal()"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons",
    "href": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "5. Pairwise Comparisons",
    "text": "5. Pairwise Comparisons\nYou can also perform pairwise comparisons for the interaction effects:\n\n# Pairwise comparisons for the interaction\npairs(emm_interaction)\n\n contrast                               estimate   SE df t.ratio p.value\n Automatic cyl6.1875 - Manual cyl6.1875     -2.1 1.27 28  -1.658  0.1085\n\n\nThis will give you insights into which combinations of am and cyl are significantly different from one another."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#conclusion-1",
    "href": "appendix/r-packages/emmeans-v1.html#conclusion-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Conclusion",
    "text": "Conclusion\nHandling interactions in emmeans is straightforward and provides valuable insights into how different factors influence the response variable. By fitting models that include interactions, computing estimated marginal means, and visualizing the results, you can effectively interpret complex relationships in your data."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#understanding-multi-factor-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#understanding-multi-factor-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "1. Understanding Multi-Factor Interactions",
    "text": "1. Understanding Multi-Factor Interactions\nMulti-factor interactions occur when the effect of one factor on the response variable depends on the levels of two or more other factors. For example, you may want to investigate how the effect of treatment varies across different levels of gender and age group."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-multiple-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-multiple-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "2. Fitting a Model with Multiple Interactions",
    "text": "2. Fitting a Model with Multiple Interactions\n\nExample\nLet’s create a hypothetical dataset and fit a model that includes interactions among three factors: treatment, gender, and age_group.\n\n# Load necessary library\nlibrary(dplyr)\n\n# Simulated dataset\nset.seed(123)\ndata &lt;- data.frame(\n    treatment = factor(rep(c(\"A\", \"B\"), each = 30)),\n    gender = factor(rep(c(\"Male\", \"Female\"), times = 30)),\n    age_group = factor(rep(c(\"Young\", \"Old\"), each = 15, times = 2)),\n    outcome = rnorm(60, 50, 10)\n)\n\n# Fit a linear model with interactions\nmodel &lt;- lm(outcome ~ treatment * gender * age_group, data = data)"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-multi-factor-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-multi-factor-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3. Using emmeans for Multi-Factor Interactions",
    "text": "3. Using emmeans for Multi-Factor Interactions\n\nCompute Estimated Marginal Means\nTo compute estimated marginal means for the interaction among the three factors, specify the interaction in the emmeans() function.\n\n# Obtain estimated marginal means for the interaction\nemm_multi &lt;- emmeans(model, ~ treatment * gender * age_group)\nsummary(emm_multi)\n\n\n  \n\n\n\n\n\nInterpret the Output\nThe output will display the estimated marginal means for each combination of the levels of treatment, gender, and age_group. This helps you understand how the outcome varies across these groups."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#visualizing-multi-factor-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#visualizing-multi-factor-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4. Visualizing Multi-Factor Interactions",
    "text": "4. Visualizing Multi-Factor Interactions\nVisualizing interactions with multiple factors can be complex, but it can be done effectively using ggplot2. Here’s how to create a plot:\n\nlibrary(ggplot2)\n\n# Convert emmeans object to a data frame\nemm_df &lt;- as.data.frame(emm_multi)\n\n# Create a ggplot\nggplot(emm_df, aes(x = treatment, y = emmean, group = interaction(gender, age_group), color = interaction(gender, age_group))) +\n    geom_point(position = position_dodge(width = 0.5)) +\n    geom_line(position = position_dodge(width = 0.5)) +\n    labs(\n        title = \"Multi-Factor Interaction\",\n        x = \"Treatment\",\n        y = \"Estimated Marginal Mean Outcome\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCustomizing the Plot\nYou can adjust the aesthetics of the plot to improve clarity, such as adding facet_wrap() to separate plots by one of the factors:\n\nggplot(emm_df, aes(x = treatment, y = emmean, color = gender)) +\n    geom_point(position = position_dodge(width = 0.5)) +\n    geom_line(position = position_dodge(width = 0.5), aes(group = age_group)) +\n    facet_wrap(~age_group) +\n    labs(\n        title = \"Multi-Factor Interaction by Age Group\",\n        x = \"Treatment\",\n        y = \"Estimated Marginal Mean Outcome\"\n    ) +\n    theme_minimal()"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons-1",
    "href": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "5. Pairwise Comparisons",
    "text": "5. Pairwise Comparisons\nYou can perform pairwise comparisons for the multi-factor interactions as well:\n\n# Pairwise comparisons for the multi-factor interaction\npairs(emm_multi)\n\n contrast                        estimate   SE df t.ratio p.value\n A Female Old - B Female Old      -4.8285 4.68 52  -1.031  0.9674\n A Female Old - A Male Old         0.2514 4.85 52   0.052  1.0000\n A Female Old - B Male Old        -0.8298 4.85 52  -0.171  1.0000\n A Female Old - A Female Young    -2.7988 4.85 52  -0.577  0.9990\n A Female Old - B Female Young    -6.3348 4.85 52  -1.306  0.8923\n A Female Old - A Male Young      -4.8118 4.68 52  -1.027  0.9681\n A Female Old - B Male Young      -4.3973 4.68 52  -0.939  0.9806\n B Female Old - A Male Old         5.0800 4.85 52   1.048  0.9645\n B Female Old - B Male Old         3.9987 4.85 52   0.825  0.9909\n B Female Old - A Female Young     2.0297 4.85 52   0.419  0.9999\n B Female Old - B Female Young    -1.5063 4.85 52  -0.311  1.0000\n B Female Old - A Male Young       0.0167 4.68 52   0.004  1.0000\n B Female Old - B Male Young       0.4312 4.68 52   0.092  1.0000\n A Male Old - B Male Old          -1.0812 5.01 52  -0.216  1.0000\n A Male Old - A Female Young      -3.0503 5.01 52  -0.609  0.9986\n A Male Old - B Female Young      -6.5862 5.01 52  -1.315  0.8889\n A Male Old - A Male Young        -5.0633 4.85 52  -1.044  0.9651\n A Male Old - B Male Young        -4.6487 4.85 52  -0.959  0.9782\n B Male Old - A Female Young      -1.9690 5.01 52  -0.393  0.9999\n B Male Old - B Female Young      -5.5050 5.01 52  -1.099  0.9541\n B Male Old - A Male Young        -3.9820 4.85 52  -0.821  0.9911\n B Male Old - B Male Young        -3.5675 4.85 52  -0.736  0.9954\n A Female Young - B Female Young  -3.5360 5.01 52  -0.706  0.9965\n A Female Young - A Male Young    -2.0130 4.85 52  -0.415  0.9999\n A Female Young - B Male Young    -1.5985 4.85 52  -0.330  1.0000\n B Female Young - A Male Young     1.5230 4.85 52   0.314  1.0000\n B Female Young - B Male Young     1.9375 4.85 52   0.400  0.9999\n A Male Young - B Male Young       0.4145 4.68 52   0.088  1.0000\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\nThis will provide insights into significant differences among the combinations of the three factors."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#conclusion-2",
    "href": "appendix/r-packages/emmeans-v1.html#conclusion-2",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Conclusion",
    "text": "Conclusion\nHandling interactions with more than two factors in emmeans allows you to explore complex relationships in your data effectively. By fitting models that include multiple interactions, computing estimated marginal means, and visualizing the results, you can gain a deeper understanding of how different factors influence the response variable."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#understanding-multi-factor-interactions-1",
    "href": "appendix/r-packages/emmeans-v1.html#understanding-multi-factor-interactions-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "1. Understanding Multi-Factor Interactions",
    "text": "1. Understanding Multi-Factor Interactions\nMulti-factor interactions occur when the effect of one factor on the response variable depends on the levels of two or more other factors. For example, you may want to investigate how the effect of treatment varies across different levels of gender and age group."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-multiple-interactions-1",
    "href": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-multiple-interactions-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "2. Fitting a Model with Multiple Interactions",
    "text": "2. Fitting a Model with Multiple Interactions\n\nExample\nLet’s create a hypothetical dataset and fit a model that includes interactions among three factors: treatment, gender, and age_group.\n\n# Load necessary library\nlibrary(dplyr)\n\n# Simulated dataset\nset.seed(123)\ndata &lt;- data.frame(\n    treatment = factor(rep(c(\"A\", \"B\"), each = 30)),\n    gender = factor(rep(c(\"Male\", \"Female\"), times = 30)),\n    age_group = factor(rep(c(\"Young\", \"Old\"), each = 15, times = 2)),\n    outcome = rnorm(60, 50, 10)\n)\n\n# Fit a linear model with interactions\nmodel &lt;- lm(outcome ~ treatment * gender * age_group, data = data)"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-multi-factor-interactions-1",
    "href": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-multi-factor-interactions-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3. Using emmeans for Multi-Factor Interactions",
    "text": "3. Using emmeans for Multi-Factor Interactions\n\nCompute Estimated Marginal Means\nTo compute estimated marginal means for the interaction among the three factors, specify the interaction in the emmeans() function.\n\n# Obtain estimated marginal means for the interaction\nemm_multi &lt;- emmeans(model, ~ treatment * gender * age_group)\nsummary(emm_multi)\n\n\n  \n\n\n\n\n\nInterpret the Output\nThe output will display the estimated marginal means for each combination of the levels of treatment, gender, and age_group. This helps you understand how the outcome varies across these groups."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#visualizing-multi-factor-interactions-1",
    "href": "appendix/r-packages/emmeans-v1.html#visualizing-multi-factor-interactions-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4. Visualizing Multi-Factor Interactions",
    "text": "4. Visualizing Multi-Factor Interactions\nVisualizing interactions with multiple factors can be complex, but it can be done effectively using ggplot2. Here’s how to create a plot:\n\nlibrary(ggplot2)\n\n# Convert emmeans object to a data frame\nemm_df &lt;- as.data.frame(emm_multi)\n\n# Create a ggplot\nggplot(emm_df, aes(x = treatment, y = emmean, group = interaction(gender, age_group), color = interaction(gender, age_group))) +\n    geom_point(position = position_dodge(width = 0.5)) +\n    geom_line(position = position_dodge(width = 0.5)) +\n    labs(\n        title = \"Multi-Factor Interaction\",\n        x = \"Treatment\",\n        y = \"Estimated Marginal Mean Outcome\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCustomizing the Plot\nYou can adjust the aesthetics of the plot to improve clarity, such as adding facet_wrap() to separate plots by one of the factors:\n\nggplot(emm_df, aes(x = treatment, y = emmean, color = gender)) +\n    geom_point(position = position_dodge(width = 0.5)) +\n    geom_line(position = position_dodge(width = 0.5), aes(group = age_group)) +\n    facet_wrap(~age_group) +\n    labs(\n        title = \"Multi-Factor Interaction by Age Group\",\n        x = \"Treatment\",\n        y = \"Estimated Marginal Mean Outcome\"\n    ) +\n    theme_minimal()"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons-2",
    "href": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons-2",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "5. Pairwise Comparisons",
    "text": "5. Pairwise Comparisons\nYou can perform pairwise comparisons for the multi-factor interactions as well:\n\n# Pairwise comparisons for the multi-factor interaction\npairs(emm_multi)\n\n contrast                        estimate   SE df t.ratio p.value\n A Female Old - B Female Old      -4.8285 4.68 52  -1.031  0.9674\n A Female Old - A Male Old         0.2514 4.85 52   0.052  1.0000\n A Female Old - B Male Old        -0.8298 4.85 52  -0.171  1.0000\n A Female Old - A Female Young    -2.7988 4.85 52  -0.577  0.9990\n A Female Old - B Female Young    -6.3348 4.85 52  -1.306  0.8923\n A Female Old - A Male Young      -4.8118 4.68 52  -1.027  0.9681\n A Female Old - B Male Young      -4.3973 4.68 52  -0.939  0.9806\n B Female Old - A Male Old         5.0800 4.85 52   1.048  0.9645\n B Female Old - B Male Old         3.9987 4.85 52   0.825  0.9909\n B Female Old - A Female Young     2.0297 4.85 52   0.419  0.9999\n B Female Old - B Female Young    -1.5063 4.85 52  -0.311  1.0000\n B Female Old - A Male Young       0.0167 4.68 52   0.004  1.0000\n B Female Old - B Male Young       0.4312 4.68 52   0.092  1.0000\n A Male Old - B Male Old          -1.0812 5.01 52  -0.216  1.0000\n A Male Old - A Female Young      -3.0503 5.01 52  -0.609  0.9986\n A Male Old - B Female Young      -6.5862 5.01 52  -1.315  0.8889\n A Male Old - A Male Young        -5.0633 4.85 52  -1.044  0.9651\n A Male Old - B Male Young        -4.6487 4.85 52  -0.959  0.9782\n B Male Old - A Female Young      -1.9690 5.01 52  -0.393  0.9999\n B Male Old - B Female Young      -5.5050 5.01 52  -1.099  0.9541\n B Male Old - A Male Young        -3.9820 4.85 52  -0.821  0.9911\n B Male Old - B Male Young        -3.5675 4.85 52  -0.736  0.9954\n A Female Young - B Female Young  -3.5360 5.01 52  -0.706  0.9965\n A Female Young - A Male Young    -2.0130 4.85 52  -0.415  0.9999\n A Female Young - B Male Young    -1.5985 4.85 52  -0.330  1.0000\n B Female Young - A Male Young     1.5230 4.85 52   0.314  1.0000\n B Female Young - B Male Young     1.9375 4.85 52   0.400  0.9999\n A Male Young - B Male Young       0.4145 4.68 52   0.088  1.0000\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\nThis will provide insights into significant differences among the combinations of the three factors."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#conclusion-3",
    "href": "appendix/r-packages/emmeans-v1.html#conclusion-3",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Conclusion",
    "text": "Conclusion\nHandling interactions with more than two factors in emmeans allows you to explore complex relationships in your data effectively. By fitting models that include multiple interactions, computing estimated marginal means, and visualizing the results, you can gain a deeper understanding of how different factors influence the response variable."
  },
  {
    "objectID": "appendix/r-package-functions/r-design.bin-agricolae.html",
    "href": "appendix/r-package-functions/r-design.bin-agricolae.html",
    "title": "Tutorial for design.bib in the agricolae Package",
    "section": "",
    "text": "The agricolae package in R is a comprehensive tool designed for statistical procedures in agricultural research. It provides a variety of functions for experimental design, analysis of variance, clustering, and more. Among its offerings, the agricolae::design.bib() function stands out for creating Randomized Balanced Incomplete Block Designs (BIBD). A BIBD is an experimental design where the number of treatments exceeds the block size, yet each pair of treatments appears together in the same number of blocks, ensuring balanced comparisons. This tutorial focuses on agricolae::design.bib(), with mentions of supporting packages like AlgDesign, which is required for its operation.\n\nPackage: agricolae\nKey Function: agricolae::design.bib()\nSupporting Package: AlgDesign\nPurpose: Generates a Randomized Balanced Incomplete Block Design for experiments with incomplete blocks.\n\n\n\n\nThe agricolae::design.bib() function is particularly useful in scenarios such as:\n\nAgricultural field trials where plots (blocks) have limited space to test all treatments (e.g., crop varieties or fertilizers).\nExperiments requiring balanced comparisons despite constraints on block size, such as greenhouse studies with limited bench space.\nResearch settings where resource efficiency is critical, and not all treatments can be applied simultaneously in each block.\n\n\n\n\nUsing agricolae::design.bib() offers several advantages:\n\nResource Efficiency: Optimizes the use of limited experimental units.\nBalanced Comparisons: Ensures each treatment pair is compared an equal number of times, enhancing statistical reliability.\nFlexibility: Handles incomplete block designs, making it adaptable to real-world constraints.\n\nWhen referencing functions from non-base packages, we will use the package::function() format, such as agricolae::design.bib(), to clearly indicate their origin.\n\n\n\n\n\n\nTo begin, load the agricolae package using pacman::p_load(), which simplifies package installation and loading:\n\npacman::p_load(agricolae)\n\n\n\n\nHere’s a simple example to create a BIBD with 4 treatments and a block size of 3:\n\n# Define treatments\ntrt &lt;- c(\"A\", \"B\", \"C\", \"D\")\n\n# Define block size\nk &lt;- 3\n\n# Generate BIBD\noutdesign &lt;- agricolae::design.bib(\n    trt,\n    k,\n    serie = 2,\n    seed = 41,\n    kinds = \"Super-Duper\"\n)\n\n\nParameters BIB\n==============\nLambda     : 2\ntreatmeans : 4\nBlock size : 3\nBlocks     : 4\nReplication: 3 \n\nEfficiency factor 0.8888889 \n\n&lt;&lt;&lt; Book &gt;&gt;&gt;\n\n# Print design parameters\noutdesign$parameters |&gt; print()\n\n$design\n[1] \"bib\"\n\n$trt\n[1] \"A\" \"B\" \"C\" \"D\"\n\n$k\n[1] 3\n\n$serie\n[1] 2\n\n$seed\n[1] 41\n\n$kinds\n[1] \"Super-Duper\"\n\n# View the field book\nbook &lt;- outdesign$book\nbook |&gt; print()\n\n   plots block trt\n1    101     1   D\n2    102     1   B\n3    103     1   A\n4    201     2   B\n5    202     2   D\n6    203     2   C\n7    301     3   A\n8    302     3   C\n9    303     3   D\n10   401     4   B\n11   402     4   A\n12   403     4   C\n\n# View the sketch of the design\noutdesign$sketch |&gt; print()\n\n     [,1] [,2] [,3]\n[1,] \"D\"  \"B\"  \"A\" \n[2,] \"B\"  \"D\"  \"C\" \n[3,] \"A\"  \"C\"  \"D\" \n[4,] \"B\"  \"A\"  \"C\" \n\n\nAnnotations: - trt: A vector of treatment labels (here, “A” to “D”). - k: The number of treatments per block (3). - serie = 2: Sets plot numbering to a two-digit format (e.g., 101, 102). - seed = 41: Ensures randomization is reproducible. - kinds = \"Super-Duper\": Specifies the random number generation method. - The output includes parameters (design details), book (field layout), and sketch (block-treatment matrix).\n\n\n\n\n\n\nThe agricolae::design.bib() function generates a Randomized Balanced Incomplete Block Design. It ensures each treatment appears in a specified number of blocks and each pair of treatments appears together in a consistent number of blocks (denoted as lambda, λ), maintaining balance despite incomplete blocks.\n\n\n\nHere’s the function structure without execution:\nagricolae::design.bib(trt, k, r = NULL, serie = 2, seed = 0, kinds = \"Super-Duper\", maxRep = 20, randomization = TRUE)\nArguments: - trt: Vector of treatment names or an integer for the number of treatments. - k: Integer specifying the block size (treatments per block). - r: Optional; number of replications per treatment. If NULL, the smallest feasible r is calculated. - serie: Integer (1, 2, or 3) for plot numbering format (e.g., 1: 11, 12; 2: 101, 102). - seed: Integer for randomization reproducibility. - kinds: Character string for the random number generation method (e.g., “Super-Duper”, “Mersenne-Twister”). - maxRep: Maximum iterations to find a design. - randomization: Logical; if TRUE, randomizes the design.\n\n\n\nLet’s create a BIBD with 5 treatments and a block size of 3:\n\n# Define treatments\ntrt &lt;- c(\"T1\", \"T2\", \"T3\", \"T4\", \"T5\")\n\n# Define block size\nk &lt;- 3\n\n# Generate BIBD with a seed for reproducibility\noutdesign &lt;- agricolae::design.bib(trt, k, seed = 123, serie = 2)\n\n\nParameters BIB\n==============\nLambda     : 3\ntreatmeans : 5\nBlock size : 3\nBlocks     : 10\nReplication: 6 \n\nEfficiency factor 0.8333333 \n\n&lt;&lt;&lt; Book &gt;&gt;&gt;\n\n# Print design parameters\noutdesign$parameters |&gt; print()\n\n$design\n[1] \"bib\"\n\n$trt\n[1] \"T1\" \"T2\" \"T3\" \"T4\" \"T5\"\n\n$k\n[1] 3\n\n$serie\n[1] 2\n\n$seed\n[1] 123\n\n$kinds\n[1] \"Super-Duper\"\n\n# Extract and print the field book\nbook &lt;- outdesign$book\nbook |&gt; print()\n\n   plots block trt\n1    101     1  T2\n2    102     1  T1\n3    103     1  T4\n4    201     2  T4\n5    202     2  T3\n6    203     2  T2\n7    301     3  T4\n8    302     3  T2\n9    303     3  T5\n10   401     4  T1\n11   402     4  T4\n12   403     4  T5\n13   501     5  T2\n14   502     5  T5\n15   503     5  T1\n16   601     6  T3\n17   602     6  T5\n18   603     6  T2\n19   701     7  T3\n20   702     7  T5\n21   703     7  T1\n22   801     8  T2\n23   802     8  T3\n24   803     8  T1\n25   901     9  T4\n26   902     9  T5\n27   903     9  T3\n28  1001    10  T4\n29  1002    10  T1\n30  1003    10  T3\n\n# Print the design sketch\noutdesign$sketch |&gt; print()\n\n      [,1] [,2] [,3]\n [1,] \"T2\" \"T1\" \"T4\"\n [2,] \"T4\" \"T3\" \"T2\"\n [3,] \"T4\" \"T2\" \"T5\"\n [4,] \"T1\" \"T4\" \"T5\"\n [5,] \"T2\" \"T5\" \"T1\"\n [6,] \"T3\" \"T5\" \"T2\"\n [7,] \"T3\" \"T5\" \"T1\"\n [8,] \"T2\" \"T3\" \"T1\"\n [9,] \"T4\" \"T5\" \"T3\"\n[10,] \"T4\" \"T1\" \"T3\"\n\n\nStep-by-Step Comments: - trt: Defines 5 treatments as a character vector. - k = 3: Sets each block to contain 3 treatments. - seed = 123: Fixes the randomization for consistency. - outdesign$parameters: Shows design specifics like number of blocks and lambda. - outdesign$book: Lists plot assignments with block and treatment details. - outdesign$sketch: Displays a matrix of blocks (rows) and treatment positions (columns).\n\n\n\n\n\n\n\n\nCreate a BIBD for 6 treatments, with each block containing 4 treatments, to plan a field experiment with limited plot sizes.\n\n\n\n\nData Preparation: Define the treatments.\nFunction Application: Apply agricolae::design.bib() with specified parameters.\nVisualization: Examine the design structure.\n\n\n# Define treatments\ntrt &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\")\n\n# Define block size\nk &lt;- 4\n\n# Generate BIBD\noutdesign &lt;- agricolae::design.bib(trt, k, seed = 456, serie = 2)\n\n# Print design parameters\noutdesign$parameters |&gt; print()\n\n# Print the field book\nbook &lt;- outdesign$book\nbook |&gt; print()\n\n# Print the sketch\noutdesign$sketch |&gt; print()\n\n\n\n\n\nParameters: Lists the number of treatments (6), block size (4), replications (r), blocks (b), and lambda (λ).\nField Book: Shows plot numbers, block assignments, and treatments (e.g., plot 101 in block 1 with treatment “A”).\nSketch: A matrix where each row is a block, and columns show treatment assignments (e.g., row 1 might be “A”, “B”, “C”, “D”).\n\n\n\n\nThe design ensures each treatment appears in r blocks, and each pair appears together λ times, balancing comparisons across the experiment."
  },
  {
    "objectID": "appendix/r-package-functions/r-design.bin-agricolae.html#introduction",
    "href": "appendix/r-package-functions/r-design.bin-agricolae.html#introduction",
    "title": "Tutorial for design.bib in the agricolae Package",
    "section": "",
    "text": "The agricolae package in R is a comprehensive tool designed for statistical procedures in agricultural research. It provides a variety of functions for experimental design, analysis of variance, clustering, and more. Among its offerings, the agricolae::design.bib() function stands out for creating Randomized Balanced Incomplete Block Designs (BIBD). A BIBD is an experimental design where the number of treatments exceeds the block size, yet each pair of treatments appears together in the same number of blocks, ensuring balanced comparisons. This tutorial focuses on agricolae::design.bib(), with mentions of supporting packages like AlgDesign, which is required for its operation.\n\nPackage: agricolae\nKey Function: agricolae::design.bib()\nSupporting Package: AlgDesign\nPurpose: Generates a Randomized Balanced Incomplete Block Design for experiments with incomplete blocks.\n\n\n\n\nThe agricolae::design.bib() function is particularly useful in scenarios such as:\n\nAgricultural field trials where plots (blocks) have limited space to test all treatments (e.g., crop varieties or fertilizers).\nExperiments requiring balanced comparisons despite constraints on block size, such as greenhouse studies with limited bench space.\nResearch settings where resource efficiency is critical, and not all treatments can be applied simultaneously in each block.\n\n\n\n\nUsing agricolae::design.bib() offers several advantages:\n\nResource Efficiency: Optimizes the use of limited experimental units.\nBalanced Comparisons: Ensures each treatment pair is compared an equal number of times, enhancing statistical reliability.\nFlexibility: Handles incomplete block designs, making it adaptable to real-world constraints.\n\nWhen referencing functions from non-base packages, we will use the package::function() format, such as agricolae::design.bib(), to clearly indicate their origin."
  },
  {
    "objectID": "appendix/r-package-functions/r-design.bin-agricolae.html#getting-started",
    "href": "appendix/r-package-functions/r-design.bin-agricolae.html#getting-started",
    "title": "Tutorial for design.bib in the agricolae Package",
    "section": "",
    "text": "To begin, load the agricolae package using pacman::p_load(), which simplifies package installation and loading:\n\npacman::p_load(agricolae)\n\n\n\n\nHere’s a simple example to create a BIBD with 4 treatments and a block size of 3:\n\n# Define treatments\ntrt &lt;- c(\"A\", \"B\", \"C\", \"D\")\n\n# Define block size\nk &lt;- 3\n\n# Generate BIBD\noutdesign &lt;- agricolae::design.bib(\n    trt,\n    k,\n    serie = 2,\n    seed = 41,\n    kinds = \"Super-Duper\"\n)\n\n\nParameters BIB\n==============\nLambda     : 2\ntreatmeans : 4\nBlock size : 3\nBlocks     : 4\nReplication: 3 \n\nEfficiency factor 0.8888889 \n\n&lt;&lt;&lt; Book &gt;&gt;&gt;\n\n# Print design parameters\noutdesign$parameters |&gt; print()\n\n$design\n[1] \"bib\"\n\n$trt\n[1] \"A\" \"B\" \"C\" \"D\"\n\n$k\n[1] 3\n\n$serie\n[1] 2\n\n$seed\n[1] 41\n\n$kinds\n[1] \"Super-Duper\"\n\n# View the field book\nbook &lt;- outdesign$book\nbook |&gt; print()\n\n   plots block trt\n1    101     1   D\n2    102     1   B\n3    103     1   A\n4    201     2   B\n5    202     2   D\n6    203     2   C\n7    301     3   A\n8    302     3   C\n9    303     3   D\n10   401     4   B\n11   402     4   A\n12   403     4   C\n\n# View the sketch of the design\noutdesign$sketch |&gt; print()\n\n     [,1] [,2] [,3]\n[1,] \"D\"  \"B\"  \"A\" \n[2,] \"B\"  \"D\"  \"C\" \n[3,] \"A\"  \"C\"  \"D\" \n[4,] \"B\"  \"A\"  \"C\" \n\n\nAnnotations: - trt: A vector of treatment labels (here, “A” to “D”). - k: The number of treatments per block (3). - serie = 2: Sets plot numbering to a two-digit format (e.g., 101, 102). - seed = 41: Ensures randomization is reproducible. - kinds = \"Super-Duper\": Specifies the random number generation method. - The output includes parameters (design details), book (field layout), and sketch (block-treatment matrix)."
  },
  {
    "objectID": "appendix/r-package-functions/r-design.bin-agricolae.html#key-functions-and-features",
    "href": "appendix/r-package-functions/r-design.bin-agricolae.html#key-functions-and-features",
    "title": "Tutorial for design.bib in the agricolae Package",
    "section": "",
    "text": "The agricolae::design.bib() function generates a Randomized Balanced Incomplete Block Design. It ensures each treatment appears in a specified number of blocks and each pair of treatments appears together in a consistent number of blocks (denoted as lambda, λ), maintaining balance despite incomplete blocks.\n\n\n\nHere’s the function structure without execution:\nagricolae::design.bib(trt, k, r = NULL, serie = 2, seed = 0, kinds = \"Super-Duper\", maxRep = 20, randomization = TRUE)\nArguments: - trt: Vector of treatment names or an integer for the number of treatments. - k: Integer specifying the block size (treatments per block). - r: Optional; number of replications per treatment. If NULL, the smallest feasible r is calculated. - serie: Integer (1, 2, or 3) for plot numbering format (e.g., 1: 11, 12; 2: 101, 102). - seed: Integer for randomization reproducibility. - kinds: Character string for the random number generation method (e.g., “Super-Duper”, “Mersenne-Twister”). - maxRep: Maximum iterations to find a design. - randomization: Logical; if TRUE, randomizes the design.\n\n\n\nLet’s create a BIBD with 5 treatments and a block size of 3:\n\n# Define treatments\ntrt &lt;- c(\"T1\", \"T2\", \"T3\", \"T4\", \"T5\")\n\n# Define block size\nk &lt;- 3\n\n# Generate BIBD with a seed for reproducibility\noutdesign &lt;- agricolae::design.bib(trt, k, seed = 123, serie = 2)\n\n\nParameters BIB\n==============\nLambda     : 3\ntreatmeans : 5\nBlock size : 3\nBlocks     : 10\nReplication: 6 \n\nEfficiency factor 0.8333333 \n\n&lt;&lt;&lt; Book &gt;&gt;&gt;\n\n# Print design parameters\noutdesign$parameters |&gt; print()\n\n$design\n[1] \"bib\"\n\n$trt\n[1] \"T1\" \"T2\" \"T3\" \"T4\" \"T5\"\n\n$k\n[1] 3\n\n$serie\n[1] 2\n\n$seed\n[1] 123\n\n$kinds\n[1] \"Super-Duper\"\n\n# Extract and print the field book\nbook &lt;- outdesign$book\nbook |&gt; print()\n\n   plots block trt\n1    101     1  T2\n2    102     1  T1\n3    103     1  T4\n4    201     2  T4\n5    202     2  T3\n6    203     2  T2\n7    301     3  T4\n8    302     3  T2\n9    303     3  T5\n10   401     4  T1\n11   402     4  T4\n12   403     4  T5\n13   501     5  T2\n14   502     5  T5\n15   503     5  T1\n16   601     6  T3\n17   602     6  T5\n18   603     6  T2\n19   701     7  T3\n20   702     7  T5\n21   703     7  T1\n22   801     8  T2\n23   802     8  T3\n24   803     8  T1\n25   901     9  T4\n26   902     9  T5\n27   903     9  T3\n28  1001    10  T4\n29  1002    10  T1\n30  1003    10  T3\n\n# Print the design sketch\noutdesign$sketch |&gt; print()\n\n      [,1] [,2] [,3]\n [1,] \"T2\" \"T1\" \"T4\"\n [2,] \"T4\" \"T3\" \"T2\"\n [3,] \"T4\" \"T2\" \"T5\"\n [4,] \"T1\" \"T4\" \"T5\"\n [5,] \"T2\" \"T5\" \"T1\"\n [6,] \"T3\" \"T5\" \"T2\"\n [7,] \"T3\" \"T5\" \"T1\"\n [8,] \"T2\" \"T3\" \"T1\"\n [9,] \"T4\" \"T5\" \"T3\"\n[10,] \"T4\" \"T1\" \"T3\"\n\n\nStep-by-Step Comments: - trt: Defines 5 treatments as a character vector. - k = 3: Sets each block to contain 3 treatments. - seed = 123: Fixes the randomization for consistency. - outdesign$parameters: Shows design specifics like number of blocks and lambda. - outdesign$book: Lists plot assignments with block and treatment details. - outdesign$sketch: Displays a matrix of blocks (rows) and treatment positions (columns)."
  },
  {
    "objectID": "appendix/r-package-functions/r-design.bin-agricolae.html#in-depth-examples",
    "href": "appendix/r-package-functions/r-design.bin-agricolae.html#in-depth-examples",
    "title": "Tutorial for design.bib in the agricolae Package",
    "section": "",
    "text": "Create a BIBD for 6 treatments, with each block containing 4 treatments, to plan a field experiment with limited plot sizes.\n\n\n\n\nData Preparation: Define the treatments.\nFunction Application: Apply agricolae::design.bib() with specified parameters.\nVisualization: Examine the design structure.\n\n\n# Define treatments\ntrt &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\")\n\n# Define block size\nk &lt;- 4\n\n# Generate BIBD\noutdesign &lt;- agricolae::design.bib(trt, k, seed = 456, serie = 2)\n\n# Print design parameters\noutdesign$parameters |&gt; print()\n\n# Print the field book\nbook &lt;- outdesign$book\nbook |&gt; print()\n\n# Print the sketch\noutdesign$sketch |&gt; print()\n\n\n\n\n\nParameters: Lists the number of treatments (6), block size (4), replications (r), blocks (b), and lambda (λ).\nField Book: Shows plot numbers, block assignments, and treatments (e.g., plot 101 in block 1 with treatment “A”).\nSketch: A matrix where each row is a block, and columns show treatment assignments (e.g., row 1 might be “A”, “B”, “C”, “D”).\n\n\n\n\nThe design ensures each treatment appears in r blocks, and each pair appears together λ times, balancing comparisons across the experiment."
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html",
    "href": "appendix/r-base/r-replications-basics.html",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "The replications() function in the stats package is used to determine the number of replicates for each term in a formula. It helps analyze the replication structure of experimental designs, making it a key tool for assessing balance in data and verifying the adequacy of designs.\nKey components:\n\nPackage: stats\nFunction: replications()\n\n\n\n\n\nEvaluating the number of replicates for factors in experimental designs.\nVerifying the balance of experimental data.\nChecking data structure before conducting ANOVA or other statistical analyses.\n\n\n\n\n\nProvides a concise summary of replicates for each term.\nIdentifies balanced and unbalanced designs.\nFlexible input handling: supports formulas, terms, or data frames.\n\n\n\n\n\n\n\nHere is a simple example using the built-in npk dataset, which contains data from an agricultural experiment:\n\ndata(npk)\nreplications(~ N + P + K, data = npk)\n\n N  P  K \n12 12 12 \n\n\nThis output provides the number of replicates for each factor (N, P, and K).\n\n\n\n\n\n\nThe replications() function computes the number of replicates for each level of factors or terms in a given formula. It can also determine if the data are balanced.\n\n\n\nreplications(formula, data = NULL, na.action)\n\n\n\nformula: A formula or terms object specifying the variables.\ndata: A data frame containing the variables in the formula.\nna.action: Function to handle missing values (default: na.fail).\n\n\n\n\n\nCompute the replication structure of factors:\n\nreplications(~ block + N + P, data = npk)\n\nblock     N     P \n    4    12    12 \n\n\nThis calculates the number of replicates for each level of block, N, and P.\n\n\n\n\n\n\n\n\nDetermine if the dataset npk has a balanced design.\n\n\n\n\nLoad the dataset:\n\n\ndata(npk)\n\n\nApply replications():\n\n\nreplications(~ N + P + K, data = npk)\n\n N  P  K \n12 12 12 \n\n\n\nInterpret the output:\n\n\nIf the output is a vector, the design is balanced.\nIf the output is a list, the design is unbalanced.\n\n\n\n\nIn this case, the function returns a vector, indicating that npk is a balanced design.\n\n\n\n\n\n\nAnalyze the replication structure of npk, including a response variable.\n\n\n\n\nDefine the formula:\n\n\nformula &lt;- ~ block + N + P + K\n\n\nApply the function:\n\n\nreplications(formula, data = npk)\n\nblock     N     P     K \n    4    12    12    12 \n\n\n\nOutput explanation: The output indicates how many times each combination of factor levels is replicated.\n\n\n\n\n\n\n\n\nCustomize the formula to include or exclude terms:\n\nreplications(~ . - yield, data = npk)\n\nblock     N     P     K \n    4    12    12    12 \n\n\nThis excludes the response variable yield from the computation.\n\n\n\nCombine replications() with other tools to analyze designs:\n\nlibrary(dplyr)\nnpk |&gt;\n    filter(block != 6) |&gt;\n    replications(~ N + P + K, data = _)\n\n N  P  K \n10 10 10 \n\n\n\n\n\n\n\n\n\nError: “object not found”: Ensure all variables in the formula are present in the data.\nNA values: Use na.action = na.omit to handle missing values.\n\n\n\n\n\nHow do I check if the design is balanced? Use !is.list(replications(…)). A TRUE result indicates a balanced design.\nCan it handle nested factors? Yes, include nested terms in the formula.\n\n\n\n\n\n\nVerify dataset integrity before applying replications().\nUse concise formulas to focus on relevant factors.\nCheck for balance to guide the choice of statistical methods.\n\n\n\n\n\n\nThe replications() function is essential for assessing the replication structure of experimental designs. It simplifies the evaluation of factor levels and design balance, aiding in accurate and efficient analysis.\n\n\n\nExplore:\n\nUsing model.tables() for detailed summaries of designs.\nCombining replications() with custom datasets for tailored analyses.\n\n\n\n\n\n\nR Documentation for replications()\nANOVA Tutorials in R\nStatistical Models in S"
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html#introduction",
    "href": "appendix/r-base/r-replications-basics.html#introduction",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "The replications() function in the stats package is used to determine the number of replicates for each term in a formula. It helps analyze the replication structure of experimental designs, making it a key tool for assessing balance in data and verifying the adequacy of designs.\nKey components:\n\nPackage: stats\nFunction: replications()\n\n\n\n\n\nEvaluating the number of replicates for factors in experimental designs.\nVerifying the balance of experimental data.\nChecking data structure before conducting ANOVA or other statistical analyses.\n\n\n\n\n\nProvides a concise summary of replicates for each term.\nIdentifies balanced and unbalanced designs.\nFlexible input handling: supports formulas, terms, or data frames."
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html#getting-started",
    "href": "appendix/r-base/r-replications-basics.html#getting-started",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "Here is a simple example using the built-in npk dataset, which contains data from an agricultural experiment:\n\ndata(npk)\nreplications(~ N + P + K, data = npk)\n\n N  P  K \n12 12 12 \n\n\nThis output provides the number of replicates for each factor (N, P, and K)."
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html#key-functions-and-features",
    "href": "appendix/r-base/r-replications-basics.html#key-functions-and-features",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "The replications() function computes the number of replicates for each level of factors or terms in a given formula. It can also determine if the data are balanced.\n\n\n\nreplications(formula, data = NULL, na.action)\n\n\n\nformula: A formula or terms object specifying the variables.\ndata: A data frame containing the variables in the formula.\nna.action: Function to handle missing values (default: na.fail).\n\n\n\n\n\nCompute the replication structure of factors:\n\nreplications(~ block + N + P, data = npk)\n\nblock     N     P \n    4    12    12 \n\n\nThis calculates the number of replicates for each level of block, N, and P."
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html#in-depth-examples",
    "href": "appendix/r-base/r-replications-basics.html#in-depth-examples",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "Determine if the dataset npk has a balanced design.\n\n\n\n\nLoad the dataset:\n\n\ndata(npk)\n\n\nApply replications():\n\n\nreplications(~ N + P + K, data = npk)\n\n N  P  K \n12 12 12 \n\n\n\nInterpret the output:\n\n\nIf the output is a vector, the design is balanced.\nIf the output is a list, the design is unbalanced.\n\n\n\n\nIn this case, the function returns a vector, indicating that npk is a balanced design.\n\n\n\n\n\n\nAnalyze the replication structure of npk, including a response variable.\n\n\n\n\nDefine the formula:\n\n\nformula &lt;- ~ block + N + P + K\n\n\nApply the function:\n\n\nreplications(formula, data = npk)\n\nblock     N     P     K \n    4    12    12    12 \n\n\n\nOutput explanation: The output indicates how many times each combination of factor levels is replicated."
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html#advanced-features",
    "href": "appendix/r-base/r-replications-basics.html#advanced-features",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "Customize the formula to include or exclude terms:\n\nreplications(~ . - yield, data = npk)\n\nblock     N     P     K \n    4    12    12    12 \n\n\nThis excludes the response variable yield from the computation.\n\n\n\nCombine replications() with other tools to analyze designs:\n\nlibrary(dplyr)\nnpk |&gt;\n    filter(block != 6) |&gt;\n    replications(~ N + P + K, data = _)\n\n N  P  K \n10 10 10"
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-base/r-replications-basics.html#troubleshooting-and-faqs",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "Error: “object not found”: Ensure all variables in the formula are present in the data.\nNA values: Use na.action = na.omit to handle missing values.\n\n\n\n\n\nHow do I check if the design is balanced? Use !is.list(replications(…)). A TRUE result indicates a balanced design.\nCan it handle nested factors? Yes, include nested terms in the formula."
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html#best-practices",
    "href": "appendix/r-base/r-replications-basics.html#best-practices",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "Verify dataset integrity before applying replications().\nUse concise formulas to focus on relevant factors.\nCheck for balance to guide the choice of statistical methods."
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html#conclusion",
    "href": "appendix/r-base/r-replications-basics.html#conclusion",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "The replications() function is essential for assessing the replication structure of experimental designs. It simplifies the evaluation of factor levels and design balance, aiding in accurate and efficient analysis.\n\n\n\nExplore:\n\nUsing model.tables() for detailed summaries of designs.\nCombining replications() with custom datasets for tailored analyses."
  },
  {
    "objectID": "appendix/r-base/r-replications-basics.html#references-and-resources",
    "href": "appendix/r-base/r-replications-basics.html#references-and-resources",
    "title": "Tutorial on replications() Function in R",
    "section": "",
    "text": "R Documentation for replications()\nANOVA Tutorials in R\nStatistical Models in S"
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html",
    "href": "appendix/r-base/r-plot.design-basics.html",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "The plot.design() function in R, part of the base graphics package, is used to visualize univariate effects of one or more factors in experimental designs. It is commonly applied in the context of aov() analyses and supports understanding of how factors affect a numeric response variable.\nKey components:\n\nPackage: graphics\nFunction: plot.design()\n\n\n\n\n\nExploratory data analysis in experimental design.\nComparing the effects of factors on response variables.\nDiagnosing potential outliers and interaction effects.\n\n\n\n\n\nFacilitates quick, visual understanding of main effects.\nHandles multiple factors simultaneously.\nCompatible with formulas and data frames for flexibility.\n\n\n\n\n\n\n\nThe simplest use involves plotting the default dataset warpbreaks:\n\nplot.design(warpbreaks)\n\n\n\n\n\n\n\n\nThis creates a series of vertical lines where each factor’s levels are plotted against a response variable.\n\n\n\n\n\n\nplot.design() visualizes the effect of factors on a numeric response variable.\n\n\n\nplot.design(x, y = NULL, fun = mean, data = NULL, ...,\n            ylim = NULL, xlab = \"Factors\", ylab = NULL,\n            main = NULL, ask = NULL, xaxt = par(\"xaxt\"),\n            axes = TRUE, xtick = FALSE)\n\n\n\nx: A data frame, formula, or terms object.\ny: The response variable (optional if included in x).\nfun: Function applied to subsets (default is mean).\ndata: Data frame for variables referenced in x if formula-like.\nylim: Range of y values.\nxlab, ylab: Axis labels.\nmain: Main plot title.\naxes, xtick: Controls for axes and tick marks.\n\n\n\n\n\nBasic plot for a formula and dataset:\n\nplot.design(breaks ~ wool + tension, data = warpbreaks)\n\n\n\n\n\n\n\n\nThis shows the effects of wool and tension on the response variable breaks.\n\n\n\n\n\n\n\n\nVisualize and compare the effects of wool and tension on breaks.\n\n\n\n\nPrepare the data:\n\n\ndata(\"warpbreaks\")\n\n\nGenerate the plot:\n\n\nplot.design(breaks ~ wool + tension, data = warpbreaks,\n            main = \"Effects of Wool and Tension on Breaks\",\n            col = c(\"red\", \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nThe plot displays vertical lines for each factor’s levels and their impact on breaks, with overall mean as a horizontal reference.\n\n\n\n\n\n\n\nAdjust aesthetics and parameters:\n\nplot.design(breaks ~ wool + tension, data = warpbreaks,\n            col = c(\"green\", \"purple\"), pch = 16, ylim = c(20, 70),\n            main = \"Customized Plot\")\n\n\n\n\n\n\n\n\n\n\n\nCombine plot.design() with dplyr for preprocessing:\n\nlibrary(dplyr)\nwarpbreaks |&gt; \n  filter(tension != \"M\") |&gt; \n  plot.design(breaks ~ wool + tension, data = _,\n              fun = median)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError: “No numeric variables”: Ensure the response variable is numeric.\n\n\nwarpbreaks$breaks &lt;- as.numeric(warpbreaks$breaks)\n\n\nPlot not displaying: Use dev.off() to reset plotting device.\n\n\n\n\n\nCan it handle interaction terms? No, use interaction.plot() for interactions.\n\n\n\n\n\n\nAlways verify numeric response variables.\nUse fun parameter to explore summary statistics like median.\nCombine with diagnostic plots for a thorough analysis.\n\n\n\n\n\n\nplot.design() is a versatile tool for visualizing factor effects in experimental data. It simplifies the analysis of variance by providing clear, interpretable plots.\n\n\n\nExplore:\n\ninteraction.plot() for interaction effects.\nAdvanced visualization packages like ggplot2.\n\n\n\n\n\n\nR Documentation for plot.design\nStatistical Models in S"
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html#introduction",
    "href": "appendix/r-base/r-plot.design-basics.html#introduction",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "The plot.design() function in R, part of the base graphics package, is used to visualize univariate effects of one or more factors in experimental designs. It is commonly applied in the context of aov() analyses and supports understanding of how factors affect a numeric response variable.\nKey components:\n\nPackage: graphics\nFunction: plot.design()\n\n\n\n\n\nExploratory data analysis in experimental design.\nComparing the effects of factors on response variables.\nDiagnosing potential outliers and interaction effects.\n\n\n\n\n\nFacilitates quick, visual understanding of main effects.\nHandles multiple factors simultaneously.\nCompatible with formulas and data frames for flexibility."
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html#getting-started",
    "href": "appendix/r-base/r-plot.design-basics.html#getting-started",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "The simplest use involves plotting the default dataset warpbreaks:\n\nplot.design(warpbreaks)\n\n\n\n\n\n\n\n\nThis creates a series of vertical lines where each factor’s levels are plotted against a response variable."
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html#key-functions-and-features",
    "href": "appendix/r-base/r-plot.design-basics.html#key-functions-and-features",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "plot.design() visualizes the effect of factors on a numeric response variable.\n\n\n\nplot.design(x, y = NULL, fun = mean, data = NULL, ...,\n            ylim = NULL, xlab = \"Factors\", ylab = NULL,\n            main = NULL, ask = NULL, xaxt = par(\"xaxt\"),\n            axes = TRUE, xtick = FALSE)\n\n\n\nx: A data frame, formula, or terms object.\ny: The response variable (optional if included in x).\nfun: Function applied to subsets (default is mean).\ndata: Data frame for variables referenced in x if formula-like.\nylim: Range of y values.\nxlab, ylab: Axis labels.\nmain: Main plot title.\naxes, xtick: Controls for axes and tick marks.\n\n\n\n\n\nBasic plot for a formula and dataset:\n\nplot.design(breaks ~ wool + tension, data = warpbreaks)\n\n\n\n\n\n\n\n\nThis shows the effects of wool and tension on the response variable breaks."
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html#in-depth-examples",
    "href": "appendix/r-base/r-plot.design-basics.html#in-depth-examples",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "Visualize and compare the effects of wool and tension on breaks.\n\n\n\n\nPrepare the data:\n\n\ndata(\"warpbreaks\")\n\n\nGenerate the plot:\n\n\nplot.design(breaks ~ wool + tension, data = warpbreaks,\n            main = \"Effects of Wool and Tension on Breaks\",\n            col = c(\"red\", \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nThe plot displays vertical lines for each factor’s levels and their impact on breaks, with overall mean as a horizontal reference."
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html#advanced-features",
    "href": "appendix/r-base/r-plot.design-basics.html#advanced-features",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "Adjust aesthetics and parameters:\n\nplot.design(breaks ~ wool + tension, data = warpbreaks,\n            col = c(\"green\", \"purple\"), pch = 16, ylim = c(20, 70),\n            main = \"Customized Plot\")\n\n\n\n\n\n\n\n\n\n\n\nCombine plot.design() with dplyr for preprocessing:\n\nlibrary(dplyr)\nwarpbreaks |&gt; \n  filter(tension != \"M\") |&gt; \n  plot.design(breaks ~ wool + tension, data = _,\n              fun = median)"
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-base/r-plot.design-basics.html#troubleshooting-and-faqs",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "Error: “No numeric variables”: Ensure the response variable is numeric.\n\n\nwarpbreaks$breaks &lt;- as.numeric(warpbreaks$breaks)\n\n\nPlot not displaying: Use dev.off() to reset plotting device.\n\n\n\n\n\nCan it handle interaction terms? No, use interaction.plot() for interactions."
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html#best-practices",
    "href": "appendix/r-base/r-plot.design-basics.html#best-practices",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "Always verify numeric response variables.\nUse fun parameter to explore summary statistics like median.\nCombine with diagnostic plots for a thorough analysis."
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html#conclusion",
    "href": "appendix/r-base/r-plot.design-basics.html#conclusion",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "plot.design() is a versatile tool for visualizing factor effects in experimental data. It simplifies the analysis of variance by providing clear, interpretable plots.\n\n\n\nExplore:\n\ninteraction.plot() for interaction effects.\nAdvanced visualization packages like ggplot2."
  },
  {
    "objectID": "appendix/r-base/r-plot.design-basics.html#references-and-resources",
    "href": "appendix/r-base/r-plot.design-basics.html#references-and-resources",
    "title": "Tutorial on plot.design() in R",
    "section": "",
    "text": "R Documentation for plot.design\nStatistical Models in S"
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html",
    "href": "appendix/r-base/r-interaction-basics.html",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "The interaction function in R is used to combine two or more factors into a single factor. It is particularly useful in ANOVA or linear modeling when you need to analyze interactions between factors.\n\n\n\n\nExploring interaction effects between multiple factors in ANOVA.\nCreating unique combinations of factor levels.\nPreparing datasets for hierarchical or nested analyses.\n\n\n\n\n\nSimplifies the analysis of interactions between factors.\nEnsures unique identification of factor level combinations.\nSeamlessly integrates with ANOVA and modeling workflows in R.\n\n\n\n\n\n\n\nThe interaction function is part of base R, so no additional packages are required.\n\n\n\n\n# Define two factors\nfactor1 &lt;- gl(2, 3, labels = c(\"Low\", \"High\"))\nfactor2 &lt;- gl(3, 1, 6, labels = c(\"A\", \"B\", \"C\"))\n\n# Combine factors using interaction\ncombined &lt;- interaction(factor1, factor2)\nprint(combined)\n\n[1] Low.A  Low.B  Low.C  High.A High.B High.C\nLevels: Low.A High.A Low.B High.B Low.C High.C\n\n\nAnnotations:\n\ngl: Generates factors with specified levels and repetitions.\ninteraction: Combines multiple factors into a single factor.\n\n\n\n\n\n\n\n\n\nCreates a single factor by combining multiple factors. Each level of the new factor corresponds to a unique combination of levels from the input factors.\n\n\n\ninteraction(..., sep = \":\", lex.order = FALSE)\n\n…: Factors to be combined.\nsep: Separator for combined levels (default is \":\").\nlex.order: Logical; determines if levels should be ordered lexicographically.\n\n\n\n\n\n# Example with custom separator\ncombined &lt;- interaction(factor1, factor2, sep = \"-\")\nprint(combined)\n\n[1] Low-A  Low-B  Low-C  High-A High-B High-C\nLevels: Low-A High-A Low-B High-B Low-C High-C\n\n\n\n\n\n\n\n\n\n\n\nAnalyze the interaction between treatment and dose in an experiment.\n\n\n\n\nCreate a dataset with two factors and a response variable.\nUse interaction to generate a combined factor.\nPerform two-way ANOVA.\n\n\n\n\n\n# Create factors and response\nset.seed(42)\ntreatment &lt;- gl(2, 6, labels = c(\"Control\", \"Treatment\"))\ndose &lt;- gl(3, 2, 12, labels = c(\"Low\", \"Medium\", \"High\"))\nresponse &lt;- c(rnorm(6, 5), rnorm(6, 6))\n\n# Combine factors\ninteraction_factor &lt;- interaction(treatment, dose)\n\n# Perform ANOVA\nanova_model &lt;- aov(response ~ treatment * dose)\nsummary(anova_model)\n\n               Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntreatment       1  9.835   9.835   9.872  0.020 *\ndose            2  0.349   0.174   0.175  0.843  \ntreatment:dose  2  1.064   0.532   0.534  0.612  \nResiduals       6  5.977   0.996                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nVisualize interaction effects using interaction and ggplot2.\n\n\n\n\nGenerate a dataset with interaction factors.\nCreate a plot of means by interaction levels.\n\n\n\n\n\nlibrary(ggplot2)\n\n# Create dataset\ndata &lt;- data.frame(\n    Treatment = treatment,\n    Dose = dose,\n    Response = response,\n    Interaction = interaction(treatment, dose)\n)\n\n# Plot\nggplot(data, aes(x = Interaction, y = Response, fill = Treatment)) +\n    geom_boxplot() +\n    theme_minimal() +\n    labs(title = \"Interaction Effects\", x = \"Interaction\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomize interaction labels using the sep argument.\n\ninteraction_factor &lt;- interaction(treatment, dose, sep = \"_\")\nprint(interaction_factor)\n\n [1] Control_Low      Control_Low      Control_Medium   Control_Medium  \n [5] Control_High     Control_High     Treatment_Low    Treatment_Low   \n [9] Treatment_Medium Treatment_Medium Treatment_High   Treatment_High  \n6 Levels: Control_Low Treatment_Low Control_Medium ... Treatment_High\n\n\n\n\n\nUse interaction directly in model formulas.\n\nanova_model &lt;- aov(response ~ interaction(treatment, dose))\nsummary(anova_model)\n\n                             Df Sum Sq Mean Sq F value Pr(&gt;F)\ninteraction(treatment, dose)  5 11.248  2.2495   2.258  0.175\nResiduals                     6  5.977  0.9962               \n\n\n\n\n\nFor large datasets, ensure factors are properly ordered to reduce computational overhead.\n\n\n\n\n\n\n\nMismatch in factor lengths:\n\nEnsure all input factors have the same length.\n\nUnexpected level combinations:\n\nUse lex.order = TRUE to enforce lexicographic ordering.\n\n\n\n\n\n\n\nUse descriptive factor labels for clarity.\nApply interaction within model formulas for cleaner workflows.\nVisualize interaction effects to better understand relationships between factors.\n\n\n\n\n\n\n\nThe interaction function is essential for analyzing and visualizing factor interactions.\nIt integrates seamlessly with ANOVA and modeling workflows.\n\n\n\n\n\nExplore interactions with more than two factors.\nCombine interaction with advanced visualization tools like ggplot2.\n\n\n\n\n\n\nR Documentation on interaction\nANOVA in R Tutorial"
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#introduction",
    "href": "appendix/r-base/r-interaction-basics.html#introduction",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "The interaction function in R is used to combine two or more factors into a single factor. It is particularly useful in ANOVA or linear modeling when you need to analyze interactions between factors.\n\n\n\n\nExploring interaction effects between multiple factors in ANOVA.\nCreating unique combinations of factor levels.\nPreparing datasets for hierarchical or nested analyses.\n\n\n\n\n\nSimplifies the analysis of interactions between factors.\nEnsures unique identification of factor level combinations.\nSeamlessly integrates with ANOVA and modeling workflows in R."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#getting-started",
    "href": "appendix/r-base/r-interaction-basics.html#getting-started",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "The interaction function is part of base R, so no additional packages are required.\n\n\n\n\n# Define two factors\nfactor1 &lt;- gl(2, 3, labels = c(\"Low\", \"High\"))\nfactor2 &lt;- gl(3, 1, 6, labels = c(\"A\", \"B\", \"C\"))\n\n# Combine factors using interaction\ncombined &lt;- interaction(factor1, factor2)\nprint(combined)\n\n[1] Low.A  Low.B  Low.C  High.A High.B High.C\nLevels: Low.A High.A Low.B High.B Low.C High.C\n\n\nAnnotations:\n\ngl: Generates factors with specified levels and repetitions.\ninteraction: Combines multiple factors into a single factor."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#key-functions-and-features",
    "href": "appendix/r-base/r-interaction-basics.html#key-functions-and-features",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Creates a single factor by combining multiple factors. Each level of the new factor corresponds to a unique combination of levels from the input factors.\n\n\n\ninteraction(..., sep = \":\", lex.order = FALSE)\n\n…: Factors to be combined.\nsep: Separator for combined levels (default is \":\").\nlex.order: Logical; determines if levels should be ordered lexicographically.\n\n\n\n\n\n# Example with custom separator\ncombined &lt;- interaction(factor1, factor2, sep = \"-\")\nprint(combined)\n\n[1] Low-A  Low-B  Low-C  High-A High-B High-C\nLevels: Low-A High-A Low-B High-B Low-C High-C"
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#in-depth-examples",
    "href": "appendix/r-base/r-interaction-basics.html#in-depth-examples",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Analyze the interaction between treatment and dose in an experiment.\n\n\n\n\nCreate a dataset with two factors and a response variable.\nUse interaction to generate a combined factor.\nPerform two-way ANOVA.\n\n\n\n\n\n# Create factors and response\nset.seed(42)\ntreatment &lt;- gl(2, 6, labels = c(\"Control\", \"Treatment\"))\ndose &lt;- gl(3, 2, 12, labels = c(\"Low\", \"Medium\", \"High\"))\nresponse &lt;- c(rnorm(6, 5), rnorm(6, 6))\n\n# Combine factors\ninteraction_factor &lt;- interaction(treatment, dose)\n\n# Perform ANOVA\nanova_model &lt;- aov(response ~ treatment * dose)\nsummary(anova_model)\n\n               Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntreatment       1  9.835   9.835   9.872  0.020 *\ndose            2  0.349   0.174   0.175  0.843  \ntreatment:dose  2  1.064   0.532   0.534  0.612  \nResiduals       6  5.977   0.996                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nVisualize interaction effects using interaction and ggplot2.\n\n\n\n\nGenerate a dataset with interaction factors.\nCreate a plot of means by interaction levels.\n\n\n\n\n\nlibrary(ggplot2)\n\n# Create dataset\ndata &lt;- data.frame(\n    Treatment = treatment,\n    Dose = dose,\n    Response = response,\n    Interaction = interaction(treatment, dose)\n)\n\n# Plot\nggplot(data, aes(x = Interaction, y = Response, fill = Treatment)) +\n    geom_boxplot() +\n    theme_minimal() +\n    labs(title = \"Interaction Effects\", x = \"Interaction\", y = \"Response\")"
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#advanced-features",
    "href": "appendix/r-base/r-interaction-basics.html#advanced-features",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Customize interaction labels using the sep argument.\n\ninteraction_factor &lt;- interaction(treatment, dose, sep = \"_\")\nprint(interaction_factor)\n\n [1] Control_Low      Control_Low      Control_Medium   Control_Medium  \n [5] Control_High     Control_High     Treatment_Low    Treatment_Low   \n [9] Treatment_Medium Treatment_Medium Treatment_High   Treatment_High  \n6 Levels: Control_Low Treatment_Low Control_Medium ... Treatment_High\n\n\n\n\n\nUse interaction directly in model formulas.\n\nanova_model &lt;- aov(response ~ interaction(treatment, dose))\nsummary(anova_model)\n\n                             Df Sum Sq Mean Sq F value Pr(&gt;F)\ninteraction(treatment, dose)  5 11.248  2.2495   2.258  0.175\nResiduals                     6  5.977  0.9962               \n\n\n\n\n\nFor large datasets, ensure factors are properly ordered to reduce computational overhead."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-base/r-interaction-basics.html#troubleshooting-and-faqs",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Mismatch in factor lengths:\n\nEnsure all input factors have the same length.\n\nUnexpected level combinations:\n\nUse lex.order = TRUE to enforce lexicographic ordering."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#best-practices",
    "href": "appendix/r-base/r-interaction-basics.html#best-practices",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Use descriptive factor labels for clarity.\nApply interaction within model formulas for cleaner workflows.\nVisualize interaction effects to better understand relationships between factors."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#conclusion",
    "href": "appendix/r-base/r-interaction-basics.html#conclusion",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "The interaction function is essential for analyzing and visualizing factor interactions.\nIt integrates seamlessly with ANOVA and modeling workflows.\n\n\n\n\n\nExplore interactions with more than two factors.\nCombine interaction with advanced visualization tools like ggplot2."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#references-and-resources",
    "href": "appendix/r-base/r-interaction-basics.html#references-and-resources",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "R Documentation on interaction\nANOVA in R Tutorial"
  },
  {
    "objectID": "appendix/r-base/r-dummy.coef.html",
    "href": "appendix/r-base/r-dummy.coef.html",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "",
    "text": "This function is particularly useful when dealing with linear models that include factors (categorical variables). It helps you extract and understand the coefficients associated with those factors, considering the way R handles factor levels.\n1. What is dummy.coef?\nThe dummy.coef function is a generic function in R. It extracts the coefficients associated with the dummy variables that are implicitly created when you include a factor variable in a linear model (using lm). It’s an S3 method, meaning it has specific implementations for different classes of objects, most notably for lm objects (linear models).\n2. Why do we need it?\nWhen you include a factor variable in a linear model, R internally creates dummy variables (also known as indicator variables) to represent the different levels of that factor. For example, if you have a factor variable “Color” with levels “Red”, “Green”, and “Blue”, R will create two dummy variables (one less than the number of levels). One level is implicitly treated as the “baseline” or “reference” level. The coefficients of the other dummy variables represent the difference in the response variable compared to the baseline level.\ndummy.coef makes it easier to see these coefficients and their relationship to the original factor levels. It can be particularly helpful when you have interactions between factors or when you want to get a clear picture of the effect of each factor level.\n3. Basic Usage\nThe general syntax is:\ndummy.coef(object, use.na = FALSE, ...)\n\nobject: This is the fitted lm object (the result of running lm()).\nuse.na: A logical value. If TRUE, coefficients for the NA level of a factor (if present) will be included in the output. Defaults to FALSE.\n...: Other arguments passed to or from other methods.\n\n4. Example with Simulated Data\nLet’s create a simple example to illustrate how dummy.coef works:\n\n# Create a data frame\nset.seed(123)  # for reproducibility\ndata &lt;- data.frame(\n  Group = factor(rep(c(\"A\", \"B\", \"C\"), each = 20)),\n  Treatment = factor(rep(c(\"X\", \"Y\"), times = 30)),\n  Value = rnorm(60, mean = c(10, 12, 8), sd = 2) +\n          ifelse(rep(c(\"X\", \"Y\"), times = 30) == \"Y\", 3, 0) # Treatment effect\n)\n\n# Fit a linear model\nmodel &lt;- lm(Value ~ Group + Treatment, data = data)\n\n# Use dummy.coef\ndummy_coefs &lt;- dummy.coef(model)\n\n# Print the results\nprint(dummy_coefs)\n\nFull coefficients are \n                                                \n(Intercept):      10.30044                      \nGroup:                   A          B          C\n                 0.0000000 -0.5857619 -0.1702771\nTreatment:               X          Y           \n                  0.000000   3.165612           \n\n\nExplanation:\n\nData Creation:\n\nWe create a data frame with two factor variables: Group (levels A, B, C) and Treatment (levels X, Y).\nValue is a continuous variable that depends on both Group and Treatment. We introduce a treatment effect, so “Y” consistently increases the value. We also give different mean values for the groups.\n\nModel Fitting:\n\nlm(Value ~ Group + Treatment, data = data) fits a linear model where Value is predicted by Group and Treatment.\n\ndummy.coef Application:\n\ndummy.coef(model) extracts the coefficients related to the factor variables.\n\nOutput Interpretation:\nThe output of print(dummy_coefs) will be a list. It will contain elements like:\n\n(Intercept): This is the intercept of the model. It represents the predicted Value when all factor variables are at their baseline levels. R chooses one level as the baseline (usually the first alphabetically).\nGroup: This will contain coefficients for GroupB and GroupC. These coefficients represent the difference in Value between Group B and Group A (the baseline), and between Group C and Group A.\nTreatment: This will contain a coefficient for TreatmentY. This coefficient represents the difference in Value between Treatment Y and Treatment X (the baseline).\n\n\n5. Example with Interactions\ndummy.coef is even more helpful when you have interactions between factor variables. Let’s add an interaction term to the model:\n\n# Fit a linear model with interaction\nmodel_interaction &lt;- lm(Value ~ Group * Treatment, data = data)\n\n# Use dummy.coef\ndummy_coefs_interaction &lt;- dummy.coef(model_interaction)\n\n# Print the results\nprint(dummy_coefs_interaction)\n\nFull coefficients are \n                                                                        \n(Intercept):          10.63396                                          \nGroup:                       A          B          C                    \n                     0.0000000 -0.9595787 -0.7970170                    \nTreatment:                   X          Y                               \n                      0.000000   2.498574                               \nGroup:Treatment:           A:X        B:X        C:X       A:Y       B:Y\n                     0.0000000  0.0000000  0.0000000 0.0000000 0.7476336\n                             \n(Intercept):                 \nGroup:                       \n                             \nTreatment:                   \n                             \nGroup:Treatment:          C:Y\n                    1.2534796\n\n\nExplanation:\n\nValue ~ Group * Treatment includes both the main effects of Group and Treatment and their interaction.\nThe output of dummy.coef will now include coefficients for the interaction terms, such as GroupB:TreatmentY and GroupC:TreatmentY. These coefficients represent the additional effect of Treatment Y on Groups B and C, beyond the main effects of Group and Treatment.\n\n6. Changing the Baseline Level\nSometimes you want to change the baseline level of a factor. You can do this using the relevel() function:\n\n# Change the baseline level of Group to \"B\"\ndata$Group &lt;- relevel(data$Group, ref = \"B\")\n\n# Refit the model\nmodel_relevel &lt;- lm(Value ~ Group + Treatment, data = data)\n\n# Use dummy.coef\ndummy_coefs_relevel &lt;- dummy.coef(model_relevel)\n\n# Print the results\nprint(dummy_coefs_relevel)\n\nFull coefficients are \n                                             \n(Intercept):      9.71468                    \nGroup:                  B         A         C\n                0.0000000 0.5857619 0.4154848\nTreatment:              X         Y          \n                 0.000000  3.165612          \n\n\nNow, the (Intercept) will represent the predicted Value when Group is at level “B” and Treatment is at level “X”. The Group coefficients will now represent the differences between Groups A and B, and Groups C and B.\n7. Handling NA Values\nIf your factor variable contains NA values, you can use the use.na = TRUE argument to include the coefficient for the NA level in the output. However, be cautious about interpreting this coefficient, as NA values often indicate missing or problematic data.\n\n# Introduce some NAs\ndata$Group[sample(1:nrow(data), 5)] &lt;- NA\n\n# Refit the model\nmodel_na &lt;- lm(Value ~ Group + Treatment, data = data)\n\n# Use dummy.coef with use.na = TRUE\ndummy_coefs_na &lt;- dummy.coef(model_na, use.na = TRUE)\n\n# Print the results\nprint(dummy_coefs_na)\n\nFull coefficients are \n                                             \n(Intercept):     9.602415                    \nGroup:                  B         A         C\n                0.0000000 0.6096867 0.5201619\nTreatment:              X         Y          \n                 0.000000  3.180788          \n\n\n8. Key Takeaways\n\ndummy.coef is a valuable tool for understanding the coefficients associated with factor variables in linear models.\nIt helps you see the effects of each factor level relative to the baseline level.\nIt’s particularly useful when dealing with interactions between factors.\nYou can change the baseline level using relevel().\nUse use.na = TRUE to include coefficients for NA levels (but interpret with caution).\n\n9. Where to Go Next\n\n?dummy.coef: Read the official R documentation for dummy.coef.\n?lm: Review the documentation for the lm function to understand how it handles factor variables.\nBooks on Regression Analysis: Consult a good textbook on regression analysis to deepen your understanding of dummy variables and model interpretation. Examples include “Applied Regression Analysis” by Draper and Smith or “Regression Models for Categorical and Limited Dependent Variables” by Long.\nOnline Tutorials: Search for tutorials on “dummy variables in R” or “factor variables in linear models R”.\n\nThis tutorial should give you a solid foundation for using the dummy.coef function effectively. Remember to experiment with different models and datasets to gain a deeper understanding of its capabilities. Good luck!"
  },
  {
    "objectID": "appendix/r-base/r-base-plot-basics.html",
    "href": "appendix/r-base/r-base-plot-basics.html",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Base R graphics provides a comprehensive suite of tools for creating high-quality plots and visualizations in R. Unlike specialized plotting libraries, Base R graphics is included with R and is highly flexible, making it ideal for both exploratory data analysis and publication-quality visuals.\n\n\n\nPackages/Functions:\n\nBase R: Functions like plot(), barplot(), hist(), boxplot(), par(), and layout().\n\nPurpose:\n\nTo create a wide variety of plots, from scatterplots and line graphs to histograms and boxplots.\n\n\n\n\n\n\nVisualizing relationships between variables.\nSummarizing distributions of data.\nCustomizing graphical parameters for presentation-ready visuals.\n\n\n\n\n\nNo additional installations or dependencies.\nFull control over plotting parameters.\nExtensible for more complex use cases."
  },
  {
    "objectID": "appendix/r-base/r-base-plot-basics.html#introduction",
    "href": "appendix/r-base/r-base-plot-basics.html#introduction",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Base R graphics provides a comprehensive suite of tools for creating high-quality plots and visualizations in R. Unlike specialized plotting libraries, Base R graphics is included with R and is highly flexible, making it ideal for both exploratory data analysis and publication-quality visuals.\n\n\n\nPackages/Functions:\n\nBase R: Functions like plot(), barplot(), hist(), boxplot(), par(), and layout().\n\nPurpose:\n\nTo create a wide variety of plots, from scatterplots and line graphs to histograms and boxplots.\n\n\n\n\n\n\nVisualizing relationships between variables.\nSummarizing distributions of data.\nCustomizing graphical parameters for presentation-ready visuals.\n\n\n\n\n\nNo additional installations or dependencies.\nFull control over plotting parameters.\nExtensible for more complex use cases."
  },
  {
    "objectID": "appendix/r-base/r-base-plot-basics.html#getting-started",
    "href": "appendix/r-base/r-base-plot-basics.html#getting-started",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading the Package\nBase R graphics is built into R, so no explicit loading is required. Simply call the plotting functions directly.\n\n\nBasic Usage\n\nExample: Scatterplot\nA simple scatterplot of mpg against wt from the mtcars dataset.\n\ndata(mtcars)\nplot(mtcars$wt, mtcars$mpg,\n    main = \"Scatterplot of MPG vs. Weight\",\n    xlab = \"Weight (1000 lbs)\",\n    ylab = \"Miles per Gallon\",\n    pch = 19, col = \"blue\"\n)"
  },
  {
    "objectID": "appendix/r-base/r-base-plot-basics.html#key-functions-and-features",
    "href": "appendix/r-base/r-base-plot-basics.html#key-functions-and-features",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Key Functions and Features",
    "text": "Key Functions and Features\n\nplot()\n\nDescription\nThe plot() function is a versatile tool for creating scatterplots, line plots, and other graphs depending on input data types.\n\n\nSyntax\nplot(x, y, type = \"p\", main = \"\", xlab = \"\", ylab = \"\", col = \"black\", pch = 1)\n\nx, y: Coordinates for points.\ntype: Type of plot (\"p\" for points, \"l\" for lines, etc.).\nmain, xlab, ylab: Titles and axis labels.\ncol: Color of points/lines.\npch: Plotting symbol.\n\n\n\nExample\n\nplot(pressure$temperature, pressure$pressure,\n    type = \"b\", col = \"red\",\n    main = \"Pressure vs Temperature\",\n    xlab = \"Temperature\", ylab = \"Pressure\"\n)\n\n\n\n\n\n\n\n\n\n\n\nbarplot()\n\nDescription\nCreates bar charts for categorical data.\n\n\nSyntax\nbarplot(height, names.arg = NULL, main = \"\", xlab = \"\", ylab = \"\", col = \"white\", beside = TRUE)\n\n\nExample\n\ncounts &lt;- table(mtcars$cyl)\nbarplot(counts,\n    main = \"Cylinder Counts\",\n    xlab = \"Number of Cylinders\",\n    ylab = \"Frequency\",\n    col = c(\"red\", \"green\", \"blue\")\n)"
  },
  {
    "objectID": "appendix/r-base/r-base-plot-basics.html#in-depth-examples",
    "href": "appendix/r-base/r-base-plot-basics.html#in-depth-examples",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "In-Depth Examples",
    "text": "In-Depth Examples\n\nPlot Customization\n\nTitle: Customizing Scatterplots\n\n\nObjective: Enhance a Scatterplot with Colors, Symbols, and Labels\n\n\nSteps\n\nLoad the iris dataset.\nUse plot() to create a scatterplot.\nCustomize colors and labels.\n\n\nplot(iris$Sepal.Length, iris$Sepal.Width,\n    col = iris$Species,\n    pch = as.numeric(iris$Species),\n    main = \"Sepal Dimensions by Species\",\n    xlab = \"Sepal Length (cm)\",\n    ylab = \"Sepal Width (cm)\"\n)\nlegend(\"topright\",\n    legend = levels(iris$Species),\n    col = 1:3, pch = 1:3, title = \"Species\"\n)\n\n\n\n\n\n\n\n\n\n\nOutput\nA scatterplot with a legend differentiating species.\n\n\n\nAdvanced Features\n\nCombining Plots with layout()\n\nlayout(matrix(c(1, 2, 3, 4), 2, 2))\nplot(mtcars$wt, mtcars$mpg, main = \"Plot 1\")\nbarplot(table(mtcars$cyl), main = \"Plot 2\")\nboxplot(mtcars$mpg ~ mtcars$cyl, main = \"Plot 3\")\nhist(mtcars$mpg, main = \"Plot 4\", col = \"gray\")"
  },
  {
    "objectID": "appendix/r-base/r-base-plot-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-base/r-base-plot-basics.html#troubleshooting-and-faqs",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Troubleshooting and FAQs",
    "text": "Troubleshooting and FAQs\n\nCommon Issues\n\nError: Plot not showing\n\nEnsure the plotting window is open and active.\n\nLegend overlap\n\nAdjust the position using legend() arguments like x and y.\n\n\n\n\nFAQs\n\nHow do I save a plot? Use png() or pdf() before plotting and dev.off() to close the device."
  },
  {
    "objectID": "appendix/r-base/r-base-plot-basics.html#best-practices",
    "href": "appendix/r-base/r-base-plot-basics.html#best-practices",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse par() to set global graphical parameters.\nAlways label axes and include legends for clarity.\nCombine plots using layout() or par(mfrow) for comparisons."
  },
  {
    "objectID": "appendix/r-base/r-base-plot-basics.html#conclusion",
    "href": "appendix/r-base/r-base-plot-basics.html#conclusion",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Conclusion",
    "text": "Conclusion\nBase R graphics is a robust and flexible system for creating a wide range of plots. By mastering key functions like plot() and barplot(), you can create tailored visualizations for data analysis and presentation."
  },
  {
    "objectID": "appendix/r-base/r-base-plot-basics.html#references-and-resources",
    "href": "appendix/r-base/r-base-plot-basics.html#references-and-resources",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "References and Resources",
    "text": "References and Resources\n\nR Base Graphics Documentation\nR Graph Gallery"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html",
    "href": "appendix/r-base/r-gl-basics.html",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "The gl function in R is used to generate factor levels for balanced designs. It is particularly useful in creating factors for experimental designs, enabling users to set up data for analysis of variance (ANOVA) efficiently.\n\n\n\n\nConstructing balanced datasets for factorial experiments.\nSimplifying the creation of factor levels in simulations.\nPreparing data for linear modeling and ANOVA.\n\n\n\n\n\nStreamlines the creation of balanced datasets.\nReduces errors in data preparation.\nIntegrates seamlessly with ANOVA-related functions in R.\n\n\n\n\n\n\n\nThe gl function is part of base R, so no additional libraries are required.\n\n\n\n\n# Generate a factor with 3 levels, each repeated 5 times\nfactor_levels &lt;- gl(n = 3, k = 5, length = 15)\nprint(factor_levels)\n\n [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3\nLevels: 1 2 3\n\n\nAnnotations:\n\nn: Number of levels.\nk: Number of repetitions per level.\nlength: Total length of the factor.\n\n\n\n\n\n\n\n\n\nCreates balanced factors by specifying the number of levels, repetitions, and total length.\n\n\n\ngl(n, k, length = n*k, labels = NULL)\n\nn: Number of levels.\nk: Repetitions of each level.\nlength: Total length of the factor.\nlabels: Custom labels for the factor levels.\n\n\n\n\n\n# Example with custom labels\nfactor_levels &lt;- gl(n = 2, k = 4, labels = c(\"Control\", \"Treatment\"))\nprint(factor_levels)\n\n[1] Control   Control   Control   Control   Treatment Treatment Treatment\n[8] Treatment\nLevels: Control Treatment\n\n\n\n\n\n\n\n\n\n\n\nCreate factors for a 2x3 factorial design.\n\n\n\n\nDefine two factors using gl.\nCombine factors into a dataset.\n\n\n\n\n\n# Factor 1: Treatment with 2 levels\ntreatment &lt;- gl(n = 2, k = 3, labels = c(\"Low\", \"High\"))\n\n# Factor 2: Dose with 3 levels\ndose &lt;- gl(n = 3, k = 1, length = 6, labels = c(\"Low\", \"Medium\", \"High\"))\n\n# Combine into a data frame\ndata &lt;- data.frame(Treatment = treatment, Dose = dose)\nprint(data)\n\n  Treatment   Dose\n1       Low    Low\n2       Low Medium\n3       Low   High\n4      High    Low\n5      High Medium\n6      High   High\n\n\n\n\n\n\n\n\nConduct a one-way ANOVA on a dataset.\n\n\n\n\nSimulate data.\nFit an ANOVA model.\n\n\n\n\n\n# Simulate data\nset.seed(123)\ngroup &lt;- gl(3, 10, labels = c(\"A\", \"B\", \"C\"))\nresponse &lt;- c(rnorm(10, mean = 5), rnorm(10, mean = 6), rnorm(10, mean = 7))\n\n# Create a data frame\ndata &lt;- data.frame(Group = group, Response = response)\n\n# Perform ANOVA\nanova_model &lt;- aov(Response ~ Group, data = data)\nsummary(anova_model)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nGroup        2  12.24   6.122   6.435 0.00518 **\nResiduals   27  25.68   0.951                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\ncustom_labels &lt;- gl(n = 3, k = 5, labels = c(\"Group1\", \"Group2\", \"Group3\"))\nprint(custom_labels)\n\n [1] Group1 Group1 Group1 Group1 Group1 Group2 Group2 Group2 Group2 Group2\n[11] Group3 Group3 Group3 Group3 Group3\nLevels: Group1 Group2 Group3\n\n\n\n\n\nCombine factors to create interaction terms.\n\ninteraction_factor &lt;- interaction(treatment, dose)\nprint(interaction_factor)\n\n[1] Low.Low     Low.Medium  Low.High    High.Low    High.Medium High.High  \nLevels: Low.Low High.Low Low.Medium High.Medium Low.High High.High\n\n\n\n\n\n\n\n\n\nMismatch in lengths: Ensure length matches n * k if specified.\nIncorrect labels: Verify the number of labels matches n.\n\n\n\n\n\n\nAlways use descriptive labels for factors for clarity.\nUse gl in combination with other functions like expand.grid for complex designs.\nValidate the structure of your factors using table() or str().\n\n\n\n\n\n\n\nThe gl function is a versatile tool for creating factors in R.\nIt simplifies the setup of balanced designs and integrates seamlessly with ANOVA workflows.\n\n\n\n\n\nExplore interactions between factors.\nApply gl in complex factorial experiments.\n\n\n\n\n\n\nR Documentation on gl\nANOVA in R Tutorial"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#introduction",
    "href": "appendix/r-base/r-gl-basics.html#introduction",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "The gl function in R is used to generate factor levels for balanced designs. It is particularly useful in creating factors for experimental designs, enabling users to set up data for analysis of variance (ANOVA) efficiently.\n\n\n\n\nConstructing balanced datasets for factorial experiments.\nSimplifying the creation of factor levels in simulations.\nPreparing data for linear modeling and ANOVA.\n\n\n\n\n\nStreamlines the creation of balanced datasets.\nReduces errors in data preparation.\nIntegrates seamlessly with ANOVA-related functions in R."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#getting-started",
    "href": "appendix/r-base/r-gl-basics.html#getting-started",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "The gl function is part of base R, so no additional libraries are required.\n\n\n\n\n# Generate a factor with 3 levels, each repeated 5 times\nfactor_levels &lt;- gl(n = 3, k = 5, length = 15)\nprint(factor_levels)\n\n [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3\nLevels: 1 2 3\n\n\nAnnotations:\n\nn: Number of levels.\nk: Number of repetitions per level.\nlength: Total length of the factor."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#key-functions-and-features",
    "href": "appendix/r-base/r-gl-basics.html#key-functions-and-features",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "Creates balanced factors by specifying the number of levels, repetitions, and total length.\n\n\n\ngl(n, k, length = n*k, labels = NULL)\n\nn: Number of levels.\nk: Repetitions of each level.\nlength: Total length of the factor.\nlabels: Custom labels for the factor levels.\n\n\n\n\n\n# Example with custom labels\nfactor_levels &lt;- gl(n = 2, k = 4, labels = c(\"Control\", \"Treatment\"))\nprint(factor_levels)\n\n[1] Control   Control   Control   Control   Treatment Treatment Treatment\n[8] Treatment\nLevels: Control Treatment"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#in-depth-examples",
    "href": "appendix/r-base/r-gl-basics.html#in-depth-examples",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "Create factors for a 2x3 factorial design.\n\n\n\n\nDefine two factors using gl.\nCombine factors into a dataset.\n\n\n\n\n\n# Factor 1: Treatment with 2 levels\ntreatment &lt;- gl(n = 2, k = 3, labels = c(\"Low\", \"High\"))\n\n# Factor 2: Dose with 3 levels\ndose &lt;- gl(n = 3, k = 1, length = 6, labels = c(\"Low\", \"Medium\", \"High\"))\n\n# Combine into a data frame\ndata &lt;- data.frame(Treatment = treatment, Dose = dose)\nprint(data)\n\n  Treatment   Dose\n1       Low    Low\n2       Low Medium\n3       Low   High\n4      High    Low\n5      High Medium\n6      High   High\n\n\n\n\n\n\n\n\nConduct a one-way ANOVA on a dataset.\n\n\n\n\nSimulate data.\nFit an ANOVA model.\n\n\n\n\n\n# Simulate data\nset.seed(123)\ngroup &lt;- gl(3, 10, labels = c(\"A\", \"B\", \"C\"))\nresponse &lt;- c(rnorm(10, mean = 5), rnorm(10, mean = 6), rnorm(10, mean = 7))\n\n# Create a data frame\ndata &lt;- data.frame(Group = group, Response = response)\n\n# Perform ANOVA\nanova_model &lt;- aov(Response ~ Group, data = data)\nsummary(anova_model)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nGroup        2  12.24   6.122   6.435 0.00518 **\nResiduals   27  25.68   0.951                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#advanced-features",
    "href": "appendix/r-base/r-gl-basics.html#advanced-features",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "custom_labels &lt;- gl(n = 3, k = 5, labels = c(\"Group1\", \"Group2\", \"Group3\"))\nprint(custom_labels)\n\n [1] Group1 Group1 Group1 Group1 Group1 Group2 Group2 Group2 Group2 Group2\n[11] Group3 Group3 Group3 Group3 Group3\nLevels: Group1 Group2 Group3\n\n\n\n\n\nCombine factors to create interaction terms.\n\ninteraction_factor &lt;- interaction(treatment, dose)\nprint(interaction_factor)\n\n[1] Low.Low     Low.Medium  Low.High    High.Low    High.Medium High.High  \nLevels: Low.Low High.Low Low.Medium High.Medium Low.High High.High"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-base/r-gl-basics.html#troubleshooting-and-faqs",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "Mismatch in lengths: Ensure length matches n * k if specified.\nIncorrect labels: Verify the number of labels matches n."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#best-practices",
    "href": "appendix/r-base/r-gl-basics.html#best-practices",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "Always use descriptive labels for factors for clarity.\nUse gl in combination with other functions like expand.grid for complex designs.\nValidate the structure of your factors using table() or str()."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#conclusion",
    "href": "appendix/r-base/r-gl-basics.html#conclusion",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "The gl function is a versatile tool for creating factors in R.\nIt simplifies the setup of balanced designs and integrates seamlessly with ANOVA workflows.\n\n\n\n\n\nExplore interactions between factors.\nApply gl in complex factorial experiments."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#references-and-resources",
    "href": "appendix/r-base/r-gl-basics.html#references-and-resources",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "R Documentation on gl\nANOVA in R Tutorial"
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html",
    "href": "appendix/r-base/r-model.tables-basics.html",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "The model.tables() function in the stats package generates summary tables for models fitted using aov(). It helps extract and interpret information such as effects or means for terms in a model. This function is particularly useful in analyzing complex ANOVA designs.\nKey components:\n\nPackage: stats\nFunction: model.tables()\n\n\n\n\n\nSummarizing model terms in ANOVA fits.\nComputing and displaying effects or means of factors.\nAdding standard errors for results, when required.\n\n\n\n\n\nProvides detailed summaries of complex ANOVA models.\nFacilitates interpretation of factor effects and mean responses.\nSeamless integration with other ANOVA tools in R.\n\n\n\n\n\n\n\nHere is a basic example using the npk dataset:\n\ndata(npk)\n\n# Fit an ANOVA model\nnpk.aov &lt;- aov(yield ~ block + N * P * K, data = npk)\n\n# Generate a table of means\nmodel.tables(npk.aov, type = \"means\")\n\nTables of means\nGrand mean\n       \n54.875 \n\n block \nblock\n    1     2     3     4     5     6 \n54.03 57.45 60.77 50.12 50.52 56.35 \n\n N \nN\n    0     1 \n52.07 57.68 \n\n P \nP\n    0     1 \n55.47 54.28 \n\n K \nK\n    0     1 \n56.87 52.88 \n\n N:P \n   P\nN   0     1    \n  0 51.72 52.42\n  1 59.22 56.15\n\n N:K \n   K\nN   0     1    \n  0 52.88 51.25\n  1 60.85 54.52\n\n P:K \n   K\nP   0     1    \n  0 57.60 53.33\n  1 56.13 52.43\n\n\nThis generates a table summarizing mean responses for each term.\n\n\n\n\n\n\nThe model.tables() function computes summary tables of effects or means for terms in an aov model.\n\n\n\nmodel.tables(x, type = \"effects\", se = FALSE, cterms, ...)\n\n\n\nx: An aov model object.\ntype: Type of table to compute: \"effects\" (default) or \"means\".\nse: Logical indicating whether standard errors should be included.\ncterms: A character vector specifying terms for which tables are computed (default: all terms).\n…: Additional arguments passed to methods.\n\n\n\n\n\nCompute effects for all terms:\n\nmodel.tables(npk.aov, type = \"effects\")\n\nTables of effects\n\n block \nblock\n     1      2      3      4      5      6 \n-0.850  2.575  5.900 -4.750 -4.350  1.475 \n\n N \nN\n      0       1 \n-2.8083  2.8083 \n\n P \nP\n      0       1 \n 0.5917 -0.5917 \n\n K \nK\n      0       1 \n 1.9917 -1.9917 \n\n N:P \n   P\nN   0       1      \n  0 -0.9417  0.9417\n  1  0.9417 -0.9417\n\n N:K \n   K\nN   0      1     \n  0 -1.175  1.175\n  1  1.175 -1.175\n\n P:K \n   K\nP   0        1       \n  0  0.14167 -0.14167\n  1 -0.14167  0.14167\n\n\n\n\n\n\n\n\n\n\nSummarize the mean responses for combinations of factors in the npk dataset.\n\n\n\n\nLoad the data:\n\n\ndata(npk)\n\n\nFit the ANOVA model:\n\n\nnpk.aov &lt;- aov(yield ~ block + N * P * K, data = npk)\n\n\nGenerate mean tables:\n\n\nmodel.tables(npk.aov, type = \"means\", se = TRUE)\n\nTables of means\nGrand mean\n       \n54.875 \n\n block \nblock\n    1     2     3     4     5     6 \n54.03 57.45 60.77 50.12 50.52 56.35 \n\n N \nN\n    0     1 \n52.07 57.68 \n\n P \nP\n    0     1 \n55.47 54.28 \n\n K \nK\n    0     1 \n56.87 52.88 \n\n N:P \n   P\nN   0     1    \n  0 51.72 52.42\n  1 59.22 56.15\n\n N:K \n   K\nN   0     1    \n  0 52.88 51.25\n  1 60.85 54.52\n\n P:K \n   K\nP   0     1    \n  0 57.60 53.33\n  1 56.13 52.43\n\nStandard errors for differences of means\n        block     N     P     K   N:P   N:K   P:K\n        2.779 1.604 1.604 1.604 2.269 2.269 2.269\nreplic.     4    12    12    12     6     6     6\n\n\n\nInterpret output:\n\nEach term’s mean response is displayed.\nStandard errors are included.\n\n\n\n\n\n\n\n\nExamine factor effects in the npk dataset.\n\n\n\n\nDefine the model:\n\n\nnpk.aov &lt;- aov(yield ~ block + N + P + K, data = npk)\n\n\nCompute effects:\n\n\nmodel.tables(npk.aov, type = \"effects\", se = TRUE)\n\nTables of effects\n\n block \nblock\n     1      2      3      4      5      6 \n-0.850  2.575  5.900 -4.750 -4.350  1.475 \n\n N \nN\n      0       1 \n-2.8083  2.8083 \n\n P \nP\n      0       1 \n 0.5917 -0.5917 \n\n K \nK\n      0       1 \n 1.9917 -1.9917 \n\nStandard errors of effects\n        block     N     P     K\n        2.001 1.155 1.155 1.155\nreplic.     4    12    12    12\n\n\n\nOutput explanation:\n\nLists coefficients for each term.\nIncludes standard errors, helping gauge variability.\n\n\n\n\n\n\n\n\n\nFocus on specific terms using cterms:\n\nmodel.tables(npk.aov, type = \"means\", cterms = c(\"N\", \"P\"))\n\nTables of means\nGrand mean\n       \n54.875 \n\n N \nN\n    0     1 \n52.07 57.68 \n\n P \nP\n    0     1 \n55.47 54.28 \n\n\n\n\n\nCombine with TukeyHSD() for pairwise comparisons:\n\nTukeyHSD(npk.aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = yield ~ block + N + P + K, data = npk)\n\n$block\n       diff        lwr       upr     p adj\n2-1   3.425  -5.768011 12.618011 0.8253019\n3-1   6.750  -2.443011 15.943011 0.2218775\n4-1  -3.900 -13.093011  5.293011 0.7386705\n5-1  -3.500 -12.693011  5.693011 0.8125420\n6-1   2.325  -6.868011 11.518011 0.9588314\n3-2   3.325  -5.868011 12.518011 0.8416850\n4-2  -7.325 -16.518011  1.868011 0.1601517\n5-2  -6.925 -16.118011  2.268011 0.2013480\n6-2  -1.100 -10.293011  8.093011 0.9985985\n4-3 -10.650 -19.843011 -1.456989 0.0189601\n5-3 -10.250 -19.443011 -1.056989 0.0248128\n6-3  -4.425 -13.618011  4.768011 0.6320049\n5-4   0.400  -8.793011  9.593011 0.9999902\n6-4   6.225  -2.968011 15.418011 0.2931556\n6-5   5.825  -3.368011 15.018011 0.3572937\n\n$N\n        diff      lwr     upr     p adj\n1-0 5.616667 2.134683 9.09865 0.0036596\n\n$P\n         diff       lwr     upr     p adj\n1-0 -1.183333 -4.665317 2.29865 0.4799905\n\n$K\n         diff       lwr        upr     p adj\n1-0 -3.983333 -7.465317 -0.5013496 0.0276673\n\nmodel.tables(npk.aov, type = \"effects\")\n\nTables of effects\n\n block \nblock\n     1      2      3      4      5      6 \n-0.850  2.575  5.900 -4.750 -4.350  1.475 \n\n N \nN\n      0       1 \n-2.8083  2.8083 \n\n P \nP\n      0       1 \n 0.5917 -0.5917 \n\n K \nK\n      0       1 \n 1.9917 -1.9917 \n\n\n\n\n\n\n\n\n\nError: “Weighted aov fits are not supported”: Ensure the aov model is unweighted.\nMissing terms in output: Use cterms to specify required terms explicitly.\n\n\n\n\n\nWhat types of models are supported? Only aov objects and certain aovlist objects.\nCan it handle nested designs? Yes, with appropriate formula specification.\n\n\n\n\n\n\nUse type = \"effects\" to understand the contributions of each term.\nAlways include se = TRUE for better interpretability.\nValidate model assumptions before interpreting tables.\n\n\n\n\n\n\nThe model.tables() function is a versatile tool for summarizing ANOVA model results, offering insights into factor effects and mean responses. Its ability to compute standard errors enhances its utility for interpreting results.\n\n\n\n\nExplore TukeyHSD() for post-hoc comparisons.\nCombine with custom data preprocessing for more advanced analyses.\n\n\n\n\n\n\nR Documentation for model.tables()\nANOVA Techniques in R\nComprehensive Guide to R ANOVA Models"
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html#introduction",
    "href": "appendix/r-base/r-model.tables-basics.html#introduction",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "The model.tables() function in the stats package generates summary tables for models fitted using aov(). It helps extract and interpret information such as effects or means for terms in a model. This function is particularly useful in analyzing complex ANOVA designs.\nKey components:\n\nPackage: stats\nFunction: model.tables()\n\n\n\n\n\nSummarizing model terms in ANOVA fits.\nComputing and displaying effects or means of factors.\nAdding standard errors for results, when required.\n\n\n\n\n\nProvides detailed summaries of complex ANOVA models.\nFacilitates interpretation of factor effects and mean responses.\nSeamless integration with other ANOVA tools in R."
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html#getting-started",
    "href": "appendix/r-base/r-model.tables-basics.html#getting-started",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "Here is a basic example using the npk dataset:\n\ndata(npk)\n\n# Fit an ANOVA model\nnpk.aov &lt;- aov(yield ~ block + N * P * K, data = npk)\n\n# Generate a table of means\nmodel.tables(npk.aov, type = \"means\")\n\nTables of means\nGrand mean\n       \n54.875 \n\n block \nblock\n    1     2     3     4     5     6 \n54.03 57.45 60.77 50.12 50.52 56.35 \n\n N \nN\n    0     1 \n52.07 57.68 \n\n P \nP\n    0     1 \n55.47 54.28 \n\n K \nK\n    0     1 \n56.87 52.88 \n\n N:P \n   P\nN   0     1    \n  0 51.72 52.42\n  1 59.22 56.15\n\n N:K \n   K\nN   0     1    \n  0 52.88 51.25\n  1 60.85 54.52\n\n P:K \n   K\nP   0     1    \n  0 57.60 53.33\n  1 56.13 52.43\n\n\nThis generates a table summarizing mean responses for each term."
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html#key-functions-and-features",
    "href": "appendix/r-base/r-model.tables-basics.html#key-functions-and-features",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "The model.tables() function computes summary tables of effects or means for terms in an aov model.\n\n\n\nmodel.tables(x, type = \"effects\", se = FALSE, cterms, ...)\n\n\n\nx: An aov model object.\ntype: Type of table to compute: \"effects\" (default) or \"means\".\nse: Logical indicating whether standard errors should be included.\ncterms: A character vector specifying terms for which tables are computed (default: all terms).\n…: Additional arguments passed to methods.\n\n\n\n\n\nCompute effects for all terms:\n\nmodel.tables(npk.aov, type = \"effects\")\n\nTables of effects\n\n block \nblock\n     1      2      3      4      5      6 \n-0.850  2.575  5.900 -4.750 -4.350  1.475 \n\n N \nN\n      0       1 \n-2.8083  2.8083 \n\n P \nP\n      0       1 \n 0.5917 -0.5917 \n\n K \nK\n      0       1 \n 1.9917 -1.9917 \n\n N:P \n   P\nN   0       1      \n  0 -0.9417  0.9417\n  1  0.9417 -0.9417\n\n N:K \n   K\nN   0      1     \n  0 -1.175  1.175\n  1  1.175 -1.175\n\n P:K \n   K\nP   0        1       \n  0  0.14167 -0.14167\n  1 -0.14167  0.14167"
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html#in-depth-examples",
    "href": "appendix/r-base/r-model.tables-basics.html#in-depth-examples",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "Summarize the mean responses for combinations of factors in the npk dataset.\n\n\n\n\nLoad the data:\n\n\ndata(npk)\n\n\nFit the ANOVA model:\n\n\nnpk.aov &lt;- aov(yield ~ block + N * P * K, data = npk)\n\n\nGenerate mean tables:\n\n\nmodel.tables(npk.aov, type = \"means\", se = TRUE)\n\nTables of means\nGrand mean\n       \n54.875 \n\n block \nblock\n    1     2     3     4     5     6 \n54.03 57.45 60.77 50.12 50.52 56.35 \n\n N \nN\n    0     1 \n52.07 57.68 \n\n P \nP\n    0     1 \n55.47 54.28 \n\n K \nK\n    0     1 \n56.87 52.88 \n\n N:P \n   P\nN   0     1    \n  0 51.72 52.42\n  1 59.22 56.15\n\n N:K \n   K\nN   0     1    \n  0 52.88 51.25\n  1 60.85 54.52\n\n P:K \n   K\nP   0     1    \n  0 57.60 53.33\n  1 56.13 52.43\n\nStandard errors for differences of means\n        block     N     P     K   N:P   N:K   P:K\n        2.779 1.604 1.604 1.604 2.269 2.269 2.269\nreplic.     4    12    12    12     6     6     6\n\n\n\nInterpret output:\n\nEach term’s mean response is displayed.\nStandard errors are included.\n\n\n\n\n\n\n\n\nExamine factor effects in the npk dataset.\n\n\n\n\nDefine the model:\n\n\nnpk.aov &lt;- aov(yield ~ block + N + P + K, data = npk)\n\n\nCompute effects:\n\n\nmodel.tables(npk.aov, type = \"effects\", se = TRUE)\n\nTables of effects\n\n block \nblock\n     1      2      3      4      5      6 \n-0.850  2.575  5.900 -4.750 -4.350  1.475 \n\n N \nN\n      0       1 \n-2.8083  2.8083 \n\n P \nP\n      0       1 \n 0.5917 -0.5917 \n\n K \nK\n      0       1 \n 1.9917 -1.9917 \n\nStandard errors of effects\n        block     N     P     K\n        2.001 1.155 1.155 1.155\nreplic.     4    12    12    12\n\n\n\nOutput explanation:\n\nLists coefficients for each term.\nIncludes standard errors, helping gauge variability."
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html#advanced-features",
    "href": "appendix/r-base/r-model.tables-basics.html#advanced-features",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "Focus on specific terms using cterms:\n\nmodel.tables(npk.aov, type = \"means\", cterms = c(\"N\", \"P\"))\n\nTables of means\nGrand mean\n       \n54.875 \n\n N \nN\n    0     1 \n52.07 57.68 \n\n P \nP\n    0     1 \n55.47 54.28 \n\n\n\n\n\nCombine with TukeyHSD() for pairwise comparisons:\n\nTukeyHSD(npk.aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = yield ~ block + N + P + K, data = npk)\n\n$block\n       diff        lwr       upr     p adj\n2-1   3.425  -5.768011 12.618011 0.8253019\n3-1   6.750  -2.443011 15.943011 0.2218775\n4-1  -3.900 -13.093011  5.293011 0.7386705\n5-1  -3.500 -12.693011  5.693011 0.8125420\n6-1   2.325  -6.868011 11.518011 0.9588314\n3-2   3.325  -5.868011 12.518011 0.8416850\n4-2  -7.325 -16.518011  1.868011 0.1601517\n5-2  -6.925 -16.118011  2.268011 0.2013480\n6-2  -1.100 -10.293011  8.093011 0.9985985\n4-3 -10.650 -19.843011 -1.456989 0.0189601\n5-3 -10.250 -19.443011 -1.056989 0.0248128\n6-3  -4.425 -13.618011  4.768011 0.6320049\n5-4   0.400  -8.793011  9.593011 0.9999902\n6-4   6.225  -2.968011 15.418011 0.2931556\n6-5   5.825  -3.368011 15.018011 0.3572937\n\n$N\n        diff      lwr     upr     p adj\n1-0 5.616667 2.134683 9.09865 0.0036596\n\n$P\n         diff       lwr     upr     p adj\n1-0 -1.183333 -4.665317 2.29865 0.4799905\n\n$K\n         diff       lwr        upr     p adj\n1-0 -3.983333 -7.465317 -0.5013496 0.0276673\n\nmodel.tables(npk.aov, type = \"effects\")\n\nTables of effects\n\n block \nblock\n     1      2      3      4      5      6 \n-0.850  2.575  5.900 -4.750 -4.350  1.475 \n\n N \nN\n      0       1 \n-2.8083  2.8083 \n\n P \nP\n      0       1 \n 0.5917 -0.5917 \n\n K \nK\n      0       1 \n 1.9917 -1.9917"
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-base/r-model.tables-basics.html#troubleshooting-and-faqs",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "Error: “Weighted aov fits are not supported”: Ensure the aov model is unweighted.\nMissing terms in output: Use cterms to specify required terms explicitly.\n\n\n\n\n\nWhat types of models are supported? Only aov objects and certain aovlist objects.\nCan it handle nested designs? Yes, with appropriate formula specification."
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html#best-practices",
    "href": "appendix/r-base/r-model.tables-basics.html#best-practices",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "Use type = \"effects\" to understand the contributions of each term.\nAlways include se = TRUE for better interpretability.\nValidate model assumptions before interpreting tables."
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html#conclusion",
    "href": "appendix/r-base/r-model.tables-basics.html#conclusion",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "The model.tables() function is a versatile tool for summarizing ANOVA model results, offering insights into factor effects and mean responses. Its ability to compute standard errors enhances its utility for interpreting results.\n\n\n\n\nExplore TukeyHSD() for post-hoc comparisons.\nCombine with custom data preprocessing for more advanced analyses."
  },
  {
    "objectID": "appendix/r-base/r-model.tables-basics.html#references-and-resources",
    "href": "appendix/r-base/r-model.tables-basics.html#references-and-resources",
    "title": "Tutorial on model.tables() Function in R",
    "section": "",
    "text": "R Documentation for model.tables()\nANOVA Techniques in R\nComprehensive Guide to R ANOVA Models"
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html",
    "href": "appendix/r-base/r-rep-basics.html",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "The rep function in R replicates the elements of vectors. It is commonly used to:\n\nRepeat single values or sequences.\nExpand data sets with repetitive patterns.\nCreate predictable test cases for analysis or debugging.\n\nIt is a versatile function that supports various ways to replicate elements, making it a fundamental tool in data manipulation.\n\n\n\nThe rep function has the following arguments:\n\nx:\n\nDescription: The vector to be replicated.\nType: Numeric, character, logical, or any other R object.\nRequired: Yes.\nExamples:\n\n\n\nrep(1:3, times = 2) # Output: 1 2 3 1 2 3\n\n[1] 1 2 3 1 2 3\n\n\n\ntimes:\n\nDescription: Specifies the number of times to repeat each element.\nType: Numeric vector (must be non-negative integers).\nDefault: None; must be explicitly defined if used.\nExamples:\n\n\n\nrep(1:3, times = c(2, 3, 1)) # Output: 1 1 2 2 2 3\n\n[1] 1 1 2 2 2 3\n\n\n\neach:\n\nDescription: Specifies the number of times to repeat each element individually.\nType: Numeric scalar (non-negative integer).\nDefault: 1.\nExamples:\n\n\n\nrep(1:3, each = 2) # Output: 1 1 2 2 3 3\n\n[1] 1 1 2 2 3 3\n\n\n\nlength.out:\n\nDescription: Specifies the total length of the resulting vector.\nType: Numeric scalar.\nDefault: NULL (calculated based on other arguments).\nExamples:\n\n\n\nrep(1:3, length.out = 5) # Output: 1 2 3 1 2\n\n[1] 1 2 3 1 2\n\n\n\n\n\n\nCombining each and times:\n\nWhen both each and times are specified, each operates first, followed by times.\n\n\n\nrep(1:2, each = 2, times = 2) # Output: 1 1 2 2 1 1 2 2\n\n[1] 1 1 2 2 1 1 2 2\n\n\n\nNon-integer inputs:\n\nNon-integer times, each, or length.out values are truncated to their integer part.\n\n\n\nrep(1:2, times = 2.7) # Output: 1 1 2 2\n\n[1] 1 2 1 2\n\n\n\nHandling NA values:\n\nWorks seamlessly with vectors containing NA.\n\n\n\nrep(c(NA, 1), each = 2) # Output: NA NA 1 1\n\n[1] NA NA  1  1\n\n\n\n\n\nBasic Usage:\n\n# Repeat the sequence twice\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\n# Output: 1 2 3 1 2 3\n\nUsing each:\n\n# Repeat each element twice\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3\n\n# Output: 1 1 2 2 3 3\n\nCustom Length:\n\n# Truncate the output to a specific length\nrep(1:3, length.out = 4)\n\n[1] 1 2 3 1\n\n# Output: 1 2 3 1\n\nCombining each and times:\n\n# A complex repetition pattern\nrep(1:3, each = 2, times = 3)\n\n [1] 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3\n\n# Output: 1 1 2 2 3 3 1 1 2 2 3 3\n\n\n\n\n\nNegative values for times or each:\n\nError: Error in rep.int: invalid 'times' argument\nFix: Ensure times and each are non-negative integers.\n\n\n\n```{r}\n#| eval: false\n\nrep(1:3, times = -1) # Error\n```\n\n\nMismatched lengths for times:\n\nThe times vector length must match x or be recycled appropriately.\n\n\n\n```{r}\n#| eval: false\n\nrep(1:3, times = c(1, 2)) # Warning: longer object length is not a multiple of shorter object length\n```\n\n\nNon-numeric inputs:\n\nEnsure that all arguments like length.out, each, and times are numeric.\n\n\n\n```{r}\n#| eval: false\nrep(1:3, times = \"two\") # Error\n```\n\n\nRecycling:\n\nIf times or each is shorter than x, it generates an error.\n\n\n\n```{r}\n#| eval: false\nrep(1:3, times = c(2, 3)) # Output: 1 1 2 2 2 3\n```"
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#function-overview",
    "href": "appendix/r-base/r-rep-basics.html#function-overview",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "The rep function in R replicates the elements of vectors. It is commonly used to:\n\nRepeat single values or sequences.\nExpand data sets with repetitive patterns.\nCreate predictable test cases for analysis or debugging.\n\nIt is a versatile function that supports various ways to replicate elements, making it a fundamental tool in data manipulation."
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#arguments-explanation",
    "href": "appendix/r-base/r-rep-basics.html#arguments-explanation",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "The rep function has the following arguments:\n\nx:\n\nDescription: The vector to be replicated.\nType: Numeric, character, logical, or any other R object.\nRequired: Yes.\nExamples:\n\n\n\nrep(1:3, times = 2) # Output: 1 2 3 1 2 3\n\n[1] 1 2 3 1 2 3\n\n\n\ntimes:\n\nDescription: Specifies the number of times to repeat each element.\nType: Numeric vector (must be non-negative integers).\nDefault: None; must be explicitly defined if used.\nExamples:\n\n\n\nrep(1:3, times = c(2, 3, 1)) # Output: 1 1 2 2 2 3\n\n[1] 1 1 2 2 2 3\n\n\n\neach:\n\nDescription: Specifies the number of times to repeat each element individually.\nType: Numeric scalar (non-negative integer).\nDefault: 1.\nExamples:\n\n\n\nrep(1:3, each = 2) # Output: 1 1 2 2 3 3\n\n[1] 1 1 2 2 3 3\n\n\n\nlength.out:\n\nDescription: Specifies the total length of the resulting vector.\nType: Numeric scalar.\nDefault: NULL (calculated based on other arguments).\nExamples:\n\n\n\nrep(1:3, length.out = 5) # Output: 1 2 3 1 2\n\n[1] 1 2 3 1 2"
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#nuances-and-advanced-features",
    "href": "appendix/r-base/r-rep-basics.html#nuances-and-advanced-features",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "Combining each and times:\n\nWhen both each and times are specified, each operates first, followed by times.\n\n\n\nrep(1:2, each = 2, times = 2) # Output: 1 1 2 2 1 1 2 2\n\n[1] 1 1 2 2 1 1 2 2\n\n\n\nNon-integer inputs:\n\nNon-integer times, each, or length.out values are truncated to their integer part.\n\n\n\nrep(1:2, times = 2.7) # Output: 1 1 2 2\n\n[1] 1 2 1 2\n\n\n\nHandling NA values:\n\nWorks seamlessly with vectors containing NA.\n\n\n\nrep(c(NA, 1), each = 2) # Output: NA NA 1 1\n\n[1] NA NA  1  1"
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#examples",
    "href": "appendix/r-base/r-rep-basics.html#examples",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "Basic Usage:\n\n# Repeat the sequence twice\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\n# Output: 1 2 3 1 2 3\n\nUsing each:\n\n# Repeat each element twice\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3\n\n# Output: 1 1 2 2 3 3\n\nCustom Length:\n\n# Truncate the output to a specific length\nrep(1:3, length.out = 4)\n\n[1] 1 2 3 1\n\n# Output: 1 2 3 1\n\nCombining each and times:\n\n# A complex repetition pattern\nrep(1:3, each = 2, times = 3)\n\n [1] 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3\n\n# Output: 1 1 2 2 3 3 1 1 2 2 3 3"
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#common-errors-and-troubleshooting",
    "href": "appendix/r-base/r-rep-basics.html#common-errors-and-troubleshooting",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "Negative values for times or each:\n\nError: Error in rep.int: invalid 'times' argument\nFix: Ensure times and each are non-negative integers.\n\n\n\n```{r}\n#| eval: false\n\nrep(1:3, times = -1) # Error\n```\n\n\nMismatched lengths for times:\n\nThe times vector length must match x or be recycled appropriately.\n\n\n\n```{r}\n#| eval: false\n\nrep(1:3, times = c(1, 2)) # Warning: longer object length is not a multiple of shorter object length\n```\n\n\nNon-numeric inputs:\n\nEnsure that all arguments like length.out, each, and times are numeric.\n\n\n\n```{r}\n#| eval: false\nrep(1:3, times = \"two\") # Error\n```\n\n\nRecycling:\n\nIf times or each is shorter than x, it generates an error.\n\n\n\n```{r}\n#| eval: false\nrep(1:3, times = c(2, 3)) # Output: 1 1 2 2 2 3\n```"
  },
  {
    "objectID": "appendix/r-base/r-sample-basics.html",
    "href": "appendix/r-base/r-sample-basics.html",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "",
    "text": "The sample() function operates on vectors or integers and uses random number generation to produce randomized output.\n\n\n\nx:\n\nA vector of elements to sample from.\nIf x is a single positive integer, sample() assumes x as 1:x (i.e., the sequence from 1 to x).\n\nsize:\n\nSpecifies the number of elements to sample. Default is the length of x (i.e., it permutes the entire input if size is not specified).\n\nreplace:\n\nLogical, determines if sampling is with replacement (TRUE) or without replacement (FALSE).\nDefault: FALSE.\n\nprob:\n\nA vector of weights for sampling probabilities. Weights are applied proportionally for sampling elements.\nDefault: NULL (uniform probabilities).\n\nuseHash (in sample.int):\n\nOptimizes sampling for large datasets when replace = FALSE, prob = NULL, and size &lt;= n/2.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf replace = FALSE, size should not exceed the length of x. Otherwise, an error occurs."
  },
  {
    "objectID": "appendix/r-base/r-sample-basics.html#function-description",
    "href": "appendix/r-base/r-sample-basics.html#function-description",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "",
    "text": "The sample() function operates on vectors or integers and uses random number generation to produce randomized output.\n\n\n\nx:\n\nA vector of elements to sample from.\nIf x is a single positive integer, sample() assumes x as 1:x (i.e., the sequence from 1 to x).\n\nsize:\n\nSpecifies the number of elements to sample. Default is the length of x (i.e., it permutes the entire input if size is not specified).\n\nreplace:\n\nLogical, determines if sampling is with replacement (TRUE) or without replacement (FALSE).\nDefault: FALSE.\n\nprob:\n\nA vector of weights for sampling probabilities. Weights are applied proportionally for sampling elements.\nDefault: NULL (uniform probabilities).\n\nuseHash (in sample.int):\n\nOptimizes sampling for large datasets when replace = FALSE, prob = NULL, and size &lt;= n/2.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf replace = FALSE, size should not exceed the length of x. Otherwise, an error occurs."
  },
  {
    "objectID": "appendix/r-base/r-sample-basics.html#key-features",
    "href": "appendix/r-base/r-sample-basics.html#key-features",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "Key Features",
    "text": "Key Features\n\nRandom Permutation:\n\nIf no size is provided and replace = FALSE, the function shuffles all elements of x:\n\n\n\nset.seed(42) # For reproducibility\nsample(1:5)  # Outputs a random permutation, e.g., c(3, 5, 1, 4, 2)\n\n[1] 1 5 4 3 2\n\n\n\nWith Replacement:\n\nAllows sampling the same element multiple times:\n\n\n\nsample(1:3, size = 5, replace = TRUE)  # Outputs, e.g., c(1, 3, 2, 2, 1)\n\n[1] 2 2 1 3 3\n\n\n\nWeighted Sampling:\n\nSampling based on a specified probability vector:\n\n\n\nsample(1:3, size = 6, prob = c(0.1, 0.7, 0.2), replace = TRUE )  # Likely favors \"2\"\n\n[1] 2 2 1 1 2 2\n\n\n\nDefault Behavior with Single Numeric x:\n\nTreats x as 1:x:\n\n\n\nsample(5)  # Equivalent to sample(1:5)\n\n[1] 2 3 4 1 5\n\n\n\nEdge Cases:\n\nIf size = 0 and x is empty or has length zero, the result is a zero-length vector.\nNon-integer x or n is truncated to the nearest smaller integer."
  },
  {
    "objectID": "appendix/r-base/r-sample-basics.html#examples",
    "href": "appendix/r-base/r-sample-basics.html#examples",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "Examples",
    "text": "Examples\n\n1. Random Permutation\n\nx &lt;- c(\"A\", \"B\", \"C\", \"D\")\nsample(x)  # Randomly shuffles elements, e.g., c(\"C\", \"A\", \"D\", \"B\")\n\n[1] \"C\" \"A\" \"D\" \"B\"\n\n\n\n\n2. Sampling With Replacement\n\nsample(1:4, size = 6, replace = TRUE)\n\n[1] 1 4 2 4 4 3\n\n# Outputs, e.g., c(2, 4, 1, 3, 2, 4)\n\n\n\n3. Weighted Sampling\n\nsample(letters[1:5], size = 5, prob = c(0.5, 0.2, 0.1, 0.1, 0.1), replace = TRUE)\n\n[1] \"e\" \"a\" \"a\" \"d\" \"b\"\n\n# Heavily favors \"a\" in the output\n\n\n\n4. Default Behavior for Single Integer\n\nsample(5)  # Equivalent to sample(1:5)\n\n[1] 3 5 2 4 1\n\n\n\n\n5. Safeguarding Edge Cases with sample.int\n\n# Programmatically handle edge cases for length(x) &gt; 1:\nresample &lt;- function(x, ...) x[sample.int(length(x), ...)]\n\nx &lt;- 1:10\nresample(x[x &gt; 8])   # Properly handles cases where x has length &lt; 2\n\n[1] 10  9\n\nresample(x[x &gt; 10])  # Returns an empty vector safely\n\ninteger(0)"
  },
  {
    "objectID": "appendix/r-base/r-sample-basics.html#best-practices",
    "href": "appendix/r-base/r-sample-basics.html#best-practices",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "Best Practices",
    "text": "Best Practices\n\nReproducibility: Use set.seed() to ensure reproducible randomization:\n\n\nset.seed(42)\nsample(1:5)\n\n[1] 1 5 4 3 2\n\n\n\nEdge Cases: Use sample.int for numeric sequences, especially in programmatic contexts, to avoid surprises with default sample() behavior.\nWeight Verification: Ensure that weights in prob are non-negative and not all zero.\nLarge Dataset Efficiency: Use useHash (via sample.int) for large n to optimize memory and performance."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html",
    "href": "appendix/r-packages/dplyr.html",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "The dplyr package is a powerful tool in the tidyverse suite designed for data manipulation. It simplifies and accelerates common data transformation tasks with an intuitive syntax.\nKey functions include:\n\ndplyr::select(): Select specific columns.\ndplyr::filter(): Filter rows based on conditions.\ndplyr::mutate(): Add or modify columns.\ndplyr::summarize(): Create summary statistics.\ndplyr::arrange(): Sort rows.\ndplyr::group_by(): Group data for grouped operations.\n\n\n\n\n\nFiltering large datasets for analysis.\nSummarizing data to extract insights.\nTransforming variables or creating new ones.\nSorting or organizing data for visualization or reporting.\n\n\n\n\n\nUser-friendly syntax, especially with the pipe operator |&gt;.\nOptimized for speed, making it suitable for large datasets.\nSeamless integration with other tidyverse packages.\n\n\n\n\n\n\n\nUse pacman::p_load() to load dplyr:\n\npacman::p_load(dplyr)\n\n\n\n\nCreate and manipulate a simple dataset:\n\ndata &lt;- data.frame(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\n\n# Select columns\ndata |&gt; dplyr::select(Name, Score)\n\n\n  \n\n\n# Filter rows\ndata |&gt; dplyr::filter(Age &gt; 25)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nSelects specified columns from a data frame.\n\n\n\ndplyr::select(.data, ...)\n\n.data: A data frame.\n…: Columns to select.\n\n\n\n\n\ndata |&gt; dplyr::select(Name, Score)\n\n\n  \n\n\n\nThis extracts the Name and Score columns.\n\n\n\n\n\n\nFilters rows based on a logical condition.\n\n\n\ndplyr::filter(.data, ...)\n\n.data: A data frame.\n…: Logical conditions.\n\n\n\n\n\ndata |&gt; dplyr::filter(Age &gt; 25)\n\n\n  \n\n\n\nFilters rows where Age is greater than 25.\n\n\n\n\n\n\nCreates new columns or modifies existing ones.\n\n\n\ndplyr::mutate(.data, ...)\n\n.data: A data frame.\n…: Expressions for new or modified columns.\n\n\n\n\n\ndata |&gt; dplyr::mutate(Grade = ifelse(Score &gt; 80, \"Pass\", \"Fail\"))\n\n\n  \n\n\n\nAdds a Grade column based on Score.\n\n\n\n\n\n\n\n\n\nSummarize scores by age groups.\n\n\n\n\ndata |&gt;\n    dplyr::mutate(AgeGroup = ifelse(Age &gt; 25, \"Above 25\", \"25 and Below\")) |&gt;\n    dplyr::group_by(AgeGroup) |&gt;\n    dplyr::summarize(MeanScore = mean(Score), .groups = \"drop\")\n\n\n  \n\n\n\n\n\n\nA table showing average scores for each age group.\n\n\n\n\n\n\nFind the top scorer in each age group.\n\n\n\n\ndata |&gt;\n    dplyr::group_by(Age) |&gt;\n    dplyr::arrange(desc(Score)) |&gt;\n    dplyr::slice(1)\n\n\n  \n\n\n\n\n\n\nTop scorer details by age.\n\n\n\n\n\n\n\n\n\n\ndata |&gt; dplyr::rename(FullName = Name)\n\n\n  \n\n\n\n\n\n\n\ndata |&gt;\n    dplyr::mutate(\n        NormalizedScore = Score / max(Score),\n        Status = ifelse(Age &gt; 30, \"Senior\", \"Junior\")\n    )\n\n\n  \n\n\n\n\n\n\n\nCombine with ggplot2 for visualization:\n\nlibrary(ggplot2)\ndata |&gt;\n    dplyr::mutate(Pass = Score &gt; 80) |&gt;\n    ggplot(aes(x = Age, y = Score, color = Pass)) +\n    geom_point() +\n    labs(title = \"Scores by Age\", x = \"Age\", y = \"Score\")\n\n\n\n\n\n\n\n\n\n\n\n\nUse dplyr::filter() early in a pipeline to reduce rows.\nChain operations with |&gt; for clarity and efficiency.\n\n\n\n\n\n\n\n\n\nEnsure column names are correctly spelled and case-sensitive.\n\n\n\nThis occurs when filtering results in zero rows. Check filter conditions.\n\n\n\n\n\nCan dplyr handle large datasets? Yes, but for very large data, consider data.table or databases with dbplyr.\nHow to retain row names? Convert them to a column with tibble::rownames_to_column().\n\n\n\n\n\n\nUse consistent column naming conventions.\nApply group_by() only when summarizing or calculating grouped statistics.\nDocument pipelines for reproducibility.\n\n\n\n\n\n\n\ndplyr streamlines data manipulation with a readable syntax.\nKey functions cover selecting, filtering, mutating, and summarizing data.\n\n\n\n\nExplore dplyr advanced features like joins and window functions, or integrate with dbplyr for database operations.\n\n\n\n\n\ndplyr Documentation\nR for Data Science\nTidyverse GitHub"
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#introduction",
    "href": "appendix/r-packages/dplyr.html#introduction",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "The dplyr package is a powerful tool in the tidyverse suite designed for data manipulation. It simplifies and accelerates common data transformation tasks with an intuitive syntax.\nKey functions include:\n\ndplyr::select(): Select specific columns.\ndplyr::filter(): Filter rows based on conditions.\ndplyr::mutate(): Add or modify columns.\ndplyr::summarize(): Create summary statistics.\ndplyr::arrange(): Sort rows.\ndplyr::group_by(): Group data for grouped operations.\n\n\n\n\n\nFiltering large datasets for analysis.\nSummarizing data to extract insights.\nTransforming variables or creating new ones.\nSorting or organizing data for visualization or reporting.\n\n\n\n\n\nUser-friendly syntax, especially with the pipe operator |&gt;.\nOptimized for speed, making it suitable for large datasets.\nSeamless integration with other tidyverse packages."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#getting-started",
    "href": "appendix/r-packages/dplyr.html#getting-started",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "Use pacman::p_load() to load dplyr:\n\npacman::p_load(dplyr)\n\n\n\n\nCreate and manipulate a simple dataset:\n\ndata &lt;- data.frame(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\n\n# Select columns\ndata |&gt; dplyr::select(Name, Score)\n\n\n  \n\n\n# Filter rows\ndata |&gt; dplyr::filter(Age &gt; 25)"
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#key-functions-and-features",
    "href": "appendix/r-packages/dplyr.html#key-functions-and-features",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "Selects specified columns from a data frame.\n\n\n\ndplyr::select(.data, ...)\n\n.data: A data frame.\n…: Columns to select.\n\n\n\n\n\ndata |&gt; dplyr::select(Name, Score)\n\n\n  \n\n\n\nThis extracts the Name and Score columns.\n\n\n\n\n\n\nFilters rows based on a logical condition.\n\n\n\ndplyr::filter(.data, ...)\n\n.data: A data frame.\n…: Logical conditions.\n\n\n\n\n\ndata |&gt; dplyr::filter(Age &gt; 25)\n\n\n  \n\n\n\nFilters rows where Age is greater than 25.\n\n\n\n\n\n\nCreates new columns or modifies existing ones.\n\n\n\ndplyr::mutate(.data, ...)\n\n.data: A data frame.\n…: Expressions for new or modified columns.\n\n\n\n\n\ndata |&gt; dplyr::mutate(Grade = ifelse(Score &gt; 80, \"Pass\", \"Fail\"))\n\n\n  \n\n\n\nAdds a Grade column based on Score."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#in-depth-examples",
    "href": "appendix/r-packages/dplyr.html#in-depth-examples",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "Summarize scores by age groups.\n\n\n\n\ndata |&gt;\n    dplyr::mutate(AgeGroup = ifelse(Age &gt; 25, \"Above 25\", \"25 and Below\")) |&gt;\n    dplyr::group_by(AgeGroup) |&gt;\n    dplyr::summarize(MeanScore = mean(Score), .groups = \"drop\")\n\n\n  \n\n\n\n\n\n\nA table showing average scores for each age group.\n\n\n\n\n\n\nFind the top scorer in each age group.\n\n\n\n\ndata |&gt;\n    dplyr::group_by(Age) |&gt;\n    dplyr::arrange(desc(Score)) |&gt;\n    dplyr::slice(1)\n\n\n  \n\n\n\n\n\n\nTop scorer details by age."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#advanced-features",
    "href": "appendix/r-packages/dplyr.html#advanced-features",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "data |&gt; dplyr::rename(FullName = Name)\n\n\n  \n\n\n\n\n\n\n\ndata |&gt;\n    dplyr::mutate(\n        NormalizedScore = Score / max(Score),\n        Status = ifelse(Age &gt; 30, \"Senior\", \"Junior\")\n    )\n\n\n  \n\n\n\n\n\n\n\nCombine with ggplot2 for visualization:\n\nlibrary(ggplot2)\ndata |&gt;\n    dplyr::mutate(Pass = Score &gt; 80) |&gt;\n    ggplot(aes(x = Age, y = Score, color = Pass)) +\n    geom_point() +\n    labs(title = \"Scores by Age\", x = \"Age\", y = \"Score\")\n\n\n\n\n\n\n\n\n\n\n\n\nUse dplyr::filter() early in a pipeline to reduce rows.\nChain operations with |&gt; for clarity and efficiency."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/dplyr.html#troubleshooting-and-faqs",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "Ensure column names are correctly spelled and case-sensitive.\n\n\n\nThis occurs when filtering results in zero rows. Check filter conditions.\n\n\n\n\n\nCan dplyr handle large datasets? Yes, but for very large data, consider data.table or databases with dbplyr.\nHow to retain row names? Convert them to a column with tibble::rownames_to_column()."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#best-practices",
    "href": "appendix/r-packages/dplyr.html#best-practices",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "Use consistent column naming conventions.\nApply group_by() only when summarizing or calculating grouped statistics.\nDocument pipelines for reproducibility."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#conclusion",
    "href": "appendix/r-packages/dplyr.html#conclusion",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "dplyr streamlines data manipulation with a readable syntax.\nKey functions cover selecting, filtering, mutating, and summarizing data.\n\n\n\n\nExplore dplyr advanced features like joins and window functions, or integrate with dbplyr for database operations."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#references-and-resources",
    "href": "appendix/r-packages/dplyr.html#references-and-resources",
    "title": "Using the dplyr Package in R",
    "section": "",
    "text": "dplyr Documentation\nR for Data Science\nTidyverse GitHub"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html",
    "href": "appendix/r-packages/emmeans.html",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "The emmeans package in R simplifies post-hoc analysis and estimation of marginal means from statistical models. It provides tools to estimate, compare, and test means across levels of predictors while accounting for the model structure."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#introduction",
    "href": "appendix/r-packages/emmeans.html#introduction",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "The emmeans package in R simplifies post-hoc analysis and estimation of marginal means from statistical models. It provides tools to estimate, compare, and test means across levels of predictors while accounting for the model structure."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#overview",
    "href": "appendix/r-packages/emmeans.html#overview",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Overview",
    "text": "Overview\n\nPurpose: Provides estimated marginal means (EMMs) or least-squares means.\nCore Features:\n\nCompute EMMs for any combination of factors.\nPerform pairwise comparisons or custom contrasts.\nIntegrate with popular visualization packages like ggplot2.\nEasily handle model interactions and transformations.\n\nKey Supported Models:\n\nLinear models (lm, aov).\nGeneralized linear models (glm).\nMixed-effects models (lme4, nlme)."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#what-are-marginal-means",
    "href": "appendix/r-packages/emmeans.html#what-are-marginal-means",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "What Are Marginal Means?",
    "text": "What Are Marginal Means?\nMarginal means are predicted values from a model, adjusted to account for the distribution of covariates or other factors. They provide a clearer interpretation of group effects compared to raw means."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#getting-started",
    "href": "appendix/r-packages/emmeans.html#getting-started",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "2. Getting Started",
    "text": "2. Getting Started"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#loading-the-package",
    "href": "appendix/r-packages/emmeans.html#loading-the-package",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Loading the Package",
    "text": "Loading the Package\n\nlibrary(emmeans)"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#example-dataset",
    "href": "appendix/r-packages/emmeans.html#example-dataset",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Example Dataset",
    "text": "Example Dataset\nWe will use the warpbreaks dataset, which contains data on the number of breaks in yarn (response variable breaks) across two factors: wool type (wool) and tension level (tension)."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#basic-model",
    "href": "appendix/r-packages/emmeans.html#basic-model",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Basic Model",
    "text": "Basic Model\n\n# Fitting a linear model\nlm_model &lt;- lm(breaks ~ wool * tension, data = warpbreaks)"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#basic-marginal-means",
    "href": "appendix/r-packages/emmeans.html#basic-marginal-means",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Basic Marginal Means",
    "text": "Basic Marginal Means\n\n# Marginal means by wool\nemmeans(lm_model, ~ wool)\n\n wool emmean   SE df lower.CL upper.CL\n A      31.0 2.11 48     26.8     35.3\n B      25.3 2.11 48     21.0     29.5\n\nResults are averaged over the levels of: tension \nConfidence level used: 0.95"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#key-functions-in-emmeans",
    "href": "appendix/r-packages/emmeans.html#key-functions-in-emmeans",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3. Key Functions in emmeans",
    "text": "3. Key Functions in emmeans"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#emmeans",
    "href": "appendix/r-packages/emmeans.html#emmeans",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3.1 emmeans()",
    "text": "3.1 emmeans()\nThe emmeans() function is the foundation of the package, used to estimate marginal means.\nUsage:\nemmeans(object, specs, by = NULL, options = NULL, ...)\nArguments:\n\nobject: A fitted model (e.g., lm, glm, or lmer).\nspecs: Formula specifying factors for which marginal means are computed. Use | to specify conditions (e.g., ~ wool | tension).\nby: Variables to group results.\noptions: Adjustments like type of prediction (response, link, etc.).\n\nExamples:\n\n# Marginal means for wool\nemmeans(lm_model, ~wool)\n\n wool emmean   SE df lower.CL upper.CL\n A      31.0 2.11 48     26.8     35.3\n B      25.3 2.11 48     21.0     29.5\n\nResults are averaged over the levels of: tension \nConfidence level used: 0.95 \n\n# Marginal means of tension within each wool group\nemmeans(lm_model, ~ tension | wool)\n\nwool = A:\n tension emmean   SE df lower.CL upper.CL\n L         44.6 3.65 48     37.2     51.9\n M         24.0 3.65 48     16.7     31.3\n H         24.6 3.65 48     17.2     31.9\n\nwool = B:\n tension emmean   SE df lower.CL upper.CL\n L         28.2 3.65 48     20.9     35.6\n M         28.8 3.65 48     21.4     36.1\n H         18.8 3.65 48     11.4     26.1\n\nConfidence level used: 0.95"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#contrast",
    "href": "appendix/r-packages/emmeans.html#contrast",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3.2 contrast()",
    "text": "3.2 contrast()\nThe contrast() function performs post-hoc comparisons between marginal means, such as pairwise comparisons or custom contrasts.\nUsage:\ncontrast(object, method = \"pairwise\", interaction = FALSE, by = NULL, ...)\nArguments:\n\nobject: An emmGrid object (output of emmeans()).\nmethod: Type of comparison. Common methods:\n\n\"pairwise\": Pairwise differences.\n\"poly\": Polynomial contrasts.\n\"eff\": Effect contrasts (deviation from overall mean).\n\ninteraction: Defines interaction contrasts, generating contrasts of contrasts for multi-factor interactions.\nby: Grouping variables for separate contrasts.\n\nExamples:\n\n# Pairwise comparisons of tension levels\nwarp.emm &lt;- emmeans(lm_model, ~ tension | wool)\ncontrast(warp.emm, method = \"pairwise\")\n\nwool = A:\n contrast estimate   SE df t.ratio p.value\n L - M      20.556 5.16 48   3.986  0.0007\n L - H      20.000 5.16 48   3.878  0.0009\n M - H      -0.556 5.16 48  -0.108  0.9936\n\nwool = B:\n contrast estimate   SE df t.ratio p.value\n L - M      -0.556 5.16 48  -0.108  0.9936\n L - H       9.444 5.16 48   1.831  0.1704\n M - H      10.000 5.16 48   1.939  0.1389\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n# Custom contrasts for tension levels\ncontrast(warp.emm, list(mid.vs.ends = c(-1, 2, -1) / 2, lo.vs.hi = c(1, 0, -1)))\n\nwool = A:\n contrast    estimate   SE df t.ratio p.value\n mid.vs.ends   -10.56 4.47 48  -2.363  0.0222\n lo.vs.hi       20.00 5.16 48   3.878  0.0003\n\nwool = B:\n contrast    estimate   SE df t.ratio p.value\n mid.vs.ends     5.28 4.47 48   1.182  0.2432\n lo.vs.hi        9.44 5.16 48   1.831  0.0733"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#pairs",
    "href": "appendix/r-packages/emmeans.html#pairs",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3.3 pairs()",
    "text": "3.3 pairs()\nThis function simplifies pairwise comparisons (shortcut for contrast(…, method = \"pairwise\")).\nUsage:\npairs(object, reverse = FALSE, ...)\nArguments:\n\nreverse: If TRUE, reverses the order of comparisons.\n\nExample:\n\n# Pairwise comparisons for tension\npairs(warp.emm)\n\nwool = A:\n contrast estimate   SE df t.ratio p.value\n L - M      20.556 5.16 48   3.986  0.0007\n L - H      20.000 5.16 48   3.878  0.0009\n M - H      -0.556 5.16 48  -0.108  0.9936\n\nwool = B:\n contrast estimate   SE df t.ratio p.value\n L - M      -0.556 5.16 48  -0.108  0.9936\n L - H       9.444 5.16 48   1.831  0.1704\n M - H      10.000 5.16 48   1.939  0.1389\n\nP value adjustment: tukey method for comparing a family of 3 estimates"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#advanced-functionality",
    "href": "appendix/r-packages/emmeans.html#advanced-functionality",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4. Advanced Functionality",
    "text": "4. Advanced Functionality"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#interaction-contrasts",
    "href": "appendix/r-packages/emmeans.html#interaction-contrasts",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4.1 Interaction Contrasts",
    "text": "4.1 Interaction Contrasts\nInteraction contrasts allow you to compare interactions between factors. The interaction argument in contrast() enables this.\nExample:\n\n# Interaction contrast for wool and tension\ninteraction_contrast &lt;- contrast(warp.emm, interaction = c(wool = \"eff\", tension = \"poly\"))\ninteraction_contrast\n\nwool = A:\n tension_eff estimate   SE df t.ratio p.value\n L effect       13.52 2.98 48   4.540  &lt;.0001\n M effect       -7.04 2.98 48  -2.363  0.0222\n H effect       -6.48 2.98 48  -2.177  0.0344\n\nwool = B:\n tension_eff estimate   SE df t.ratio p.value\n L effect        2.96 2.98 48   0.995  0.3247\n M effect        3.52 2.98 48   1.182  0.2432\n H effect       -6.48 2.98 48  -2.177  0.0344\n\ncoef(interaction_contrast)  # View coefficients"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#scaling-and-offsetting",
    "href": "appendix/r-packages/emmeans.html#scaling-and-offsetting",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4.2 Scaling and Offsetting",
    "text": "4.2 Scaling and Offsetting\nUse scale and offset to transform results for interpretation (e.g., converting units).\nExample:\n\n# Convert temperature from Celsius to Fahrenheit\nmod &lt;- lm(Water.Temp ~ poly(stack.loss, degree = 2), data = stackloss)\nemm &lt;- emmeans(mod, \"stack.loss\", at = list(stack.loss = 10 * (1:4)))\ncontrast(emm, \"identity\", scale = 9 / 5, offset = 32)\n\n contrast     estimate    SE df t.ratio p.value\n stack.loss10     65.9 0.833 18  79.138  &lt;.0001\n stack.loss20     72.1 1.020 18  71.059  &lt;.0001\n stack.loss30     76.8 1.160 18  66.067  &lt;.0001\n stack.loss40     80.0 1.720 18  46.389  &lt;.0001"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#adjusting-for-multiple-comparisons",
    "href": "appendix/r-packages/emmeans.html#adjusting-for-multiple-comparisons",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4.3 Adjusting for Multiple Comparisons",
    "text": "4.3 Adjusting for Multiple Comparisons\nUse the adjust argument to control corrections like Bonferroni or Tukey:\n\ncontrast(warp.emm, method = \"pairwise\", adjust = \"tukey\")\n\nwool = A:\n contrast estimate   SE df t.ratio p.value\n L - M      20.556 5.16 48   3.986  0.0007\n L - H      20.000 5.16 48   3.878  0.0009\n M - H      -0.556 5.16 48  -0.108  0.9936\n\nwool = B:\n contrast estimate   SE df t.ratio p.value\n L - M      -0.556 5.16 48  -0.108  0.9936\n L - H       9.444 5.16 48   1.831  0.1704\n M - H      10.000 5.16 48   1.939  0.1389\n\nP value adjustment: tukey method for comparing a family of 3 estimates"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#visualizing-results",
    "href": "appendix/r-packages/emmeans.html#visualizing-results",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "5. Visualizing Results",
    "text": "5. Visualizing Results"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#interaction-plots",
    "href": "appendix/r-packages/emmeans.html#interaction-plots",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Interaction Plots",
    "text": "Interaction Plots\nThe emmip() function creates interaction plots to visualize EMMs:\n\n# Interaction plot for wool and tension\nlibrary(ggplot2)\nemmip(warp.emm, wool ~ tension)"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#pairwise-comparisons-plot",
    "href": "appendix/r-packages/emmeans.html#pairwise-comparisons-plot",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Pairwise Comparisons Plot",
    "text": "Pairwise Comparisons Plot\n\n# Pairwise comparison plot\nplot(pairs(warp.emm))"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#common-issues-and-troubleshooting",
    "href": "appendix/r-packages/emmeans.html#common-issues-and-troubleshooting",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "6. Common Issues and Troubleshooting",
    "text": "6. Common Issues and Troubleshooting"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#common-issues",
    "href": "appendix/r-packages/emmeans.html#common-issues",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Common Issues",
    "text": "Common Issues\n\nDegrees of freedom not available:\n\nSome models (e.g., Bayesian) require manual specification of degrees of freedom.\n\nInvalid contrast method:\n\nEnsure the method matches the structure of your emmGrid object."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#best-practices",
    "href": "appendix/r-packages/emmeans.html#best-practices",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Best Practices",
    "text": "Best Practices\n\nAlways inspect the emmGrid object using summary() before applying contrasts.\nUse confint() to obtain confidence intervals.\nApply adjust for proper corrections in multiple comparisons."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#detailed-argument-reference",
    "href": "appendix/r-packages/emmeans.html#detailed-argument-reference",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "7. Detailed Argument Reference",
    "text": "7. Detailed Argument Reference"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#emmeans-specific-arguments",
    "href": "appendix/r-packages/emmeans.html#emmeans-specific-arguments",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "emmeans() Specific Arguments",
    "text": "emmeans() Specific Arguments\n\nspecs: Defines the terms of interest. For example:\n\n~ factor1: Marginal means for factor1.\n~ factor1 | factor2: Marginal means of factor1 grouped by factor2."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#contrast-specific-arguments",
    "href": "appendix/r-packages/emmeans.html#contrast-specific-arguments",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "contrast() Specific Arguments",
    "text": "contrast() Specific Arguments\n\ninteraction: Defines contrasts for interactions. Example:\n\n\ncontrast(warp.emm, interaction = list(wool = \"pairwise\", tension = \"eff\"))\n\nwool = A:\n tension_pairwise estimate   SE df t.ratio p.value\n L - M              20.556 5.16 48   3.986  0.0002\n L - H              20.000 5.16 48   3.878  0.0003\n M - H              -0.556 5.16 48  -0.108  0.9147\n\nwool = B:\n tension_pairwise estimate   SE df t.ratio p.value\n L - M              -0.556 5.16 48  -0.108  0.9147\n L - H               9.444 5.16 48   1.831  0.0733\n M - H              10.000 5.16 48   1.939  0.0584\n\n\n\nscale and offset: For transforming estimates. Example:\n\n\ncontrast(warp.emm, method = \"identity\", scale = 2, offset = 1)\n\nwool = A:\n contrast estimate   SE df t.ratio p.value\n L            90.1 7.29 48  12.355  &lt;.0001\n M            49.0 7.29 48   6.718  &lt;.0001\n H            50.1 7.29 48   6.871  &lt;.0001\n\nwool = B:\n contrast estimate   SE df t.ratio p.value\n L            57.4 7.29 48   7.876  &lt;.0001\n M            58.6 7.29 48   8.028  &lt;.0001\n H            38.6 7.29 48   5.286  &lt;.0001"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#conclusion",
    "href": "appendix/r-packages/emmeans.html#conclusion",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "8. Conclusion",
    "text": "8. Conclusion\nThe emmeans package provides a robust framework for estimating and interpreting marginal means. With extensive support for complex models and versatile post-hoc analysis tools, it is essential for data analysis workflows."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#next-steps",
    "href": "appendix/r-packages/emmeans.html#next-steps",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Next Steps",
    "text": "Next Steps\n\nExplore advanced vignettes in the package documentation.\nIntegrate results into visualizations with ggplot2."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#references-and-resources",
    "href": "appendix/r-packages/emmeans.html#references-and-resources",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "9. References and Resources",
    "text": "9. References and Resources\n\nemmeans CRAN Page\nVignettes: Estimated Marginal Means\nGitHub Repository"
  },
  {
    "objectID": "appendix/r-packages/ibd.html",
    "href": "appendix/r-packages/ibd.html",
    "title": "Tutorial for the ibd Package",
    "section": "",
    "text": "The ibd package provides tools for creating and analyzing Incomplete Block Designs (IBDs) in R, facilitating efficient experimentation when it’s not possible to include all treatments in each experimental block.\n\n\n\n\nThe ‘ibd’ R package (version 1.5) is designed for working with Incomplete Block Designs - experimental arrangements where not all treatments appear in every block[2]. This is particularly useful in situations where testing all treatments in a single block is impractical due to resource constraints or experimental limitations.\nKey functions in the package include:\n\nfind_design() - For discovering efficient IBD designs\nbibd() - For creating Balanced Incomplete Block Designs\npbibd() - For creating Partially Balanced Incomplete Block Designs\ndesign_efficiency() - For evaluating the statistical efficiency of designs\nanalyze_ibd() - For analyzing data collected from incomplete block experiments\n\n\n\n\n\nAgricultural field trials with limited land or resources\nClinical trials where patient availability or grouping is constrained\nIndustrial experiments where testing all treatments in each batch is unfeasible\nPlant breeding and genomic selection studies where not all lines can be tested in all locations[8]\nAny experimental scenario where resource constraints prevent complete blocking\n\n\n\n\n\nEnables efficient experimentation with a large number of treatments\nProvides statistically rigorous designs when complete blocking is impossible\nOptimizes experimental efficiency by balancing treatment comparisons\nReduces experimental costs while maintaining statistical power\nFacilitates the analysis of data collected from incomplete designs[8]\n\n\n\n\n\n\n\n\npacman::p_load(ibd)\n\n\n\n\nLet’s create a simple balanced incomplete block design for an experiment with 7 treatments in blocks of size 3:\n\n# Create a balanced incomplete block design\ndesign &lt;- ibd::bibd(v = 7, b = 3, k = 3, r = 3, lambda = 1)\n\n# View the design structure\nprint(design)\n\n[1] \"parameters do not satisfy necessary conditions\"\n\n# Check the properties of the design\nsummary(design)\n\n   Length     Class      Mode \n        1 character character \n\n\nThis creates a design where each treatment appears in 3 blocks, and each pair of treatments appears together in exactly 1 block, ensuring balance in treatment comparisons."
  },
  {
    "objectID": "appendix/r-packages/ibd.html#introduction",
    "href": "appendix/r-packages/ibd.html#introduction",
    "title": "Tutorial for the ibd Package",
    "section": "",
    "text": "The ‘ibd’ R package (version 1.5) is designed for working with Incomplete Block Designs - experimental arrangements where not all treatments appear in every block[2]. This is particularly useful in situations where testing all treatments in a single block is impractical due to resource constraints or experimental limitations.\nKey functions in the package include:\n\nfind_design() - For discovering efficient IBD designs\nbibd() - For creating Balanced Incomplete Block Designs\npbibd() - For creating Partially Balanced Incomplete Block Designs\ndesign_efficiency() - For evaluating the statistical efficiency of designs\nanalyze_ibd() - For analyzing data collected from incomplete block experiments\n\n\n\n\n\nAgricultural field trials with limited land or resources\nClinical trials where patient availability or grouping is constrained\nIndustrial experiments where testing all treatments in each batch is unfeasible\nPlant breeding and genomic selection studies where not all lines can be tested in all locations[8]\nAny experimental scenario where resource constraints prevent complete blocking\n\n\n\n\n\nEnables efficient experimentation with a large number of treatments\nProvides statistically rigorous designs when complete blocking is impossible\nOptimizes experimental efficiency by balancing treatment comparisons\nReduces experimental costs while maintaining statistical power\nFacilitates the analysis of data collected from incomplete designs[8]"
  },
  {
    "objectID": "appendix/r-packages/ibd.html#getting-started",
    "href": "appendix/r-packages/ibd.html#getting-started",
    "title": "Tutorial for the ibd Package",
    "section": "",
    "text": "pacman::p_load(ibd)\n\n\n\n\nLet’s create a simple balanced incomplete block design for an experiment with 7 treatments in blocks of size 3:\n\n# Create a balanced incomplete block design\ndesign &lt;- ibd::bibd(v = 7, b = 3, k = 3, r = 3, lambda = 1)\n\n# View the design structure\nprint(design)\n\n[1] \"parameters do not satisfy necessary conditions\"\n\n# Check the properties of the design\nsummary(design)\n\n   Length     Class      Mode \n        1 character character \n\n\nThis creates a design where each treatment appears in 3 blocks, and each pair of treatments appears together in exactly 1 block, ensuring balance in treatment comparisons."
  },
  {
    "objectID": "appendix/r-packages/purrr.html",
    "href": "appendix/r-packages/purrr.html",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "The purrr package, part of the tidyverse, provides a set of tools for functional programming in R. It simplifies iteration, mapping, and manipulation of lists, vectors, and other data structures. The functions in purrr are designed to replace and enhance base R functions like lapply() and sapply() with more readable and consistent alternatives.\nKey components:\n\nCore Functions: map(), map2(), pmap(), reduce(), pluck(), and others.\nHelpers: safely(), possibly(), transpose(), etc.\n\n\n\n\n\nApplying a function to each element of a list or vector.\nHandling nested data structures like lists of data frames.\nAggregating results using custom operations.\nSafely working with functions that might return errors.\n\n\n\n\n\nConsistent syntax and predictable outputs.\nSeamless integration with other tidyverse packages.\nTools for error handling and debugging.\n\n\n\n\n\n\n\n\nUse the pacman package to ensure purrr is loaded:\n\npacman::p_load(purrr)\n\n\n\n\nA simple example of applying a function to a list using map():\n\n# Define a list\nnumbers &lt;- list(1, 2, 3, 4, 5)\n\n# Apply a square function to each element\nsquared &lt;- numbers |&gt; map(~ .x^2)\n\n# Print results\nsquared\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\nHere, ~ .x^2 defines an anonymous function that squares each number.\n\n\n\n\n\n\n\n\nThe map() family applies a function to each element of a list or vector and returns a list, vector, or another type based on the specific map_* function used.\n\n\n\nmap(.x, .f, ...)\n\n.x: Input list or vector.\n.f: Function to apply.\n…: Additional arguments passed to .f.\n\n\n\n\n\n# Apply `sqrt()` to each element\nroots &lt;- list(4, 9, 16) |&gt; map(sqrt)\n\n# Print results\nroots\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 4\n\n\n\n\n\n\n\n\nApplies a function to two inputs simultaneously, element by element.\n\n\n\nmap2(.x, .y, .f, ...)\n\n\n\n\n# Combine two vectors by addition\nvector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(4, 5, 6)\n\nresult &lt;- map2(vector1, vector2, ~ .x + .y)\n\nresult\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] 9\n\n\n\n\n\n\n\n\nApplies a function to multiple inputs, where inputs are supplied as a list of named arguments.\n\n\n\npmap(.l, .f, ...)\n\n.l: A list of arguments.\n.f: Function to apply.\n\n\n\n\n\n# Define inputs\ninputs &lt;- list(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))\n\n# Apply a custom function\nresult &lt;- pmap(inputs, \\(a, b, c) a + b + c)\n\nresult\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 18\n\n\n\n\n\n\n\n\n\n\n\nSum a list of numbers iteratively using reduce().\n\n\n\n\nDefine the data:\n\n\nnumbers &lt;- list(1, 2, 3, 4, 5)\n\n\nApply reduce():\n\n\ntotal &lt;- reduce(numbers, \\(x, y) x + y)\ntotal\n\n[1] 15\n\n\n\nInterpret the result: The sum of the numbers is computed iteratively.\n\n\n\n\n\n\n\nUse safely() to apply a function that might fail.\n\n\n\n\nDefine the data:\n\n\ninputs &lt;- list(10, \"a\", 20)\n\n\nUse safely():\n\n\nsafe_sqrt &lt;- safely(sqrt)\nresults &lt;- map(inputs, safe_sqrt)\n\n\nInspect results:\n\n\nresults\n\n[[1]]\n[[1]]$result\n[1] 3.162278\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\nNULL\n\n[[2]]$error\n&lt;simpleError in .Primitive(\"sqrt\")(x): non-numeric argument to mathematical function&gt;\n\n\n[[3]]\n[[3]]$result\n[1] 4.472136\n\n[[3]]$error\nNULL\n\n\nThe output includes successful results and captured errors.\n\n\n\n\n\n\nRetrieve nested elements from a list.\n\n\n\n\nDefine a nested list:\n\n\nnested_list &lt;- list(a = list(b = list(c = 42)))\n\n\nUse pluck():\n\n\nvalue &lt;- pluck(nested_list, \"a\", \"b\", \"c\")\nvalue\n\n[1] 42\n\n\n\n\n\n\n\n\n\nCustomize outputs with map_* variants:\n\n# Return a vector instead of a list\nnumbers &lt;- list(1, 2, 3)\nsquared &lt;- map_dbl(numbers, ~ .x^2)\nsquared\n\n[1] 1 4 9\n\n\n\n\n\nCombine purrr with dplyr for tidy workflows:\n\nlibrary(dplyr)\ndata &lt;- tibble(x = list(1:5, 6:10, 11:15))\n\n# Apply a function to each list element\ndata |&gt; mutate(mean_x = map_dbl(x, mean))\n\n\n  \n\n\n\n\n\n\n\n\n\n\nError: “Object not found”: Ensure all variables are correctly defined and scoped.\nUnexpected list outputs: Use map_* variants (map_dbl, map_chr) for specific return types.\n\n\n\n\n\nCan I use custom functions with purrr? Yes, pass any custom or anonymous function to map().\n\n\n\n\n\n\nUse map_* variants for predictable output types.\nCombine purrr with other tidyverse packages for efficient workflows.\nLeverage error-handling tools like safely() for robust code.\n\n\n\n\n\n\nThe purrr package simplifies iteration, functional programming, and working with nested data. Its consistent syntax and integration with the tidyverse make it an essential tool for R users.\n\n\n\n\nExplore purrr’s advanced tools like transpose() and possibly().\nIntegrate with visualization packages like ggplot2 for applied analyses.\n\n\n\n\n\n\nOfficial Documentation\nTidyverse Website\nR for Data Science\n\n\n\n\n\n\n\ngraph TD\n    A[\"Data Input\"] --&gt; B[\"Apply map()\"]\n    B --&gt; C[\"Iterate over elements\"]\n    C --&gt; D[\"Transform Results\"]\n    D --&gt; E[\"Output List/Vector\"]\n\n\n\n\nFigure 1: Data Transformation Workflow"
  },
  {
    "objectID": "appendix/r-packages/purrr.html#introduction",
    "href": "appendix/r-packages/purrr.html#introduction",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "The purrr package, part of the tidyverse, provides a set of tools for functional programming in R. It simplifies iteration, mapping, and manipulation of lists, vectors, and other data structures. The functions in purrr are designed to replace and enhance base R functions like lapply() and sapply() with more readable and consistent alternatives.\nKey components:\n\nCore Functions: map(), map2(), pmap(), reduce(), pluck(), and others.\nHelpers: safely(), possibly(), transpose(), etc.\n\n\n\n\n\nApplying a function to each element of a list or vector.\nHandling nested data structures like lists of data frames.\nAggregating results using custom operations.\nSafely working with functions that might return errors.\n\n\n\n\n\nConsistent syntax and predictable outputs.\nSeamless integration with other tidyverse packages.\nTools for error handling and debugging."
  },
  {
    "objectID": "appendix/r-packages/purrr.html#getting-started",
    "href": "appendix/r-packages/purrr.html#getting-started",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "Use the pacman package to ensure purrr is loaded:\n\npacman::p_load(purrr)\n\n\n\n\nA simple example of applying a function to a list using map():\n\n# Define a list\nnumbers &lt;- list(1, 2, 3, 4, 5)\n\n# Apply a square function to each element\nsquared &lt;- numbers |&gt; map(~ .x^2)\n\n# Print results\nsquared\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\nHere, ~ .x^2 defines an anonymous function that squares each number."
  },
  {
    "objectID": "appendix/r-packages/purrr.html#key-functions-and-features",
    "href": "appendix/r-packages/purrr.html#key-functions-and-features",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "The map() family applies a function to each element of a list or vector and returns a list, vector, or another type based on the specific map_* function used.\n\n\n\nmap(.x, .f, ...)\n\n.x: Input list or vector.\n.f: Function to apply.\n…: Additional arguments passed to .f.\n\n\n\n\n\n# Apply `sqrt()` to each element\nroots &lt;- list(4, 9, 16) |&gt; map(sqrt)\n\n# Print results\nroots\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 4\n\n\n\n\n\n\n\n\nApplies a function to two inputs simultaneously, element by element.\n\n\n\nmap2(.x, .y, .f, ...)\n\n\n\n\n# Combine two vectors by addition\nvector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(4, 5, 6)\n\nresult &lt;- map2(vector1, vector2, ~ .x + .y)\n\nresult\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] 9\n\n\n\n\n\n\n\n\nApplies a function to multiple inputs, where inputs are supplied as a list of named arguments.\n\n\n\npmap(.l, .f, ...)\n\n.l: A list of arguments.\n.f: Function to apply.\n\n\n\n\n\n# Define inputs\ninputs &lt;- list(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))\n\n# Apply a custom function\nresult &lt;- pmap(inputs, \\(a, b, c) a + b + c)\n\nresult\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 18"
  },
  {
    "objectID": "appendix/r-packages/purrr.html#in-depth-examples",
    "href": "appendix/r-packages/purrr.html#in-depth-examples",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "Sum a list of numbers iteratively using reduce().\n\n\n\n\nDefine the data:\n\n\nnumbers &lt;- list(1, 2, 3, 4, 5)\n\n\nApply reduce():\n\n\ntotal &lt;- reduce(numbers, \\(x, y) x + y)\ntotal\n\n[1] 15\n\n\n\nInterpret the result: The sum of the numbers is computed iteratively.\n\n\n\n\n\n\n\nUse safely() to apply a function that might fail.\n\n\n\n\nDefine the data:\n\n\ninputs &lt;- list(10, \"a\", 20)\n\n\nUse safely():\n\n\nsafe_sqrt &lt;- safely(sqrt)\nresults &lt;- map(inputs, safe_sqrt)\n\n\nInspect results:\n\n\nresults\n\n[[1]]\n[[1]]$result\n[1] 3.162278\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\nNULL\n\n[[2]]$error\n&lt;simpleError in .Primitive(\"sqrt\")(x): non-numeric argument to mathematical function&gt;\n\n\n[[3]]\n[[3]]$result\n[1] 4.472136\n\n[[3]]$error\nNULL\n\n\nThe output includes successful results and captured errors.\n\n\n\n\n\n\nRetrieve nested elements from a list.\n\n\n\n\nDefine a nested list:\n\n\nnested_list &lt;- list(a = list(b = list(c = 42)))\n\n\nUse pluck():\n\n\nvalue &lt;- pluck(nested_list, \"a\", \"b\", \"c\")\nvalue\n\n[1] 42"
  },
  {
    "objectID": "appendix/r-packages/purrr.html#advanced-features",
    "href": "appendix/r-packages/purrr.html#advanced-features",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "Customize outputs with map_* variants:\n\n# Return a vector instead of a list\nnumbers &lt;- list(1, 2, 3)\nsquared &lt;- map_dbl(numbers, ~ .x^2)\nsquared\n\n[1] 1 4 9\n\n\n\n\n\nCombine purrr with dplyr for tidy workflows:\n\nlibrary(dplyr)\ndata &lt;- tibble(x = list(1:5, 6:10, 11:15))\n\n# Apply a function to each list element\ndata |&gt; mutate(mean_x = map_dbl(x, mean))"
  },
  {
    "objectID": "appendix/r-packages/purrr.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/purrr.html#troubleshooting-and-faqs",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "Error: “Object not found”: Ensure all variables are correctly defined and scoped.\nUnexpected list outputs: Use map_* variants (map_dbl, map_chr) for specific return types.\n\n\n\n\n\nCan I use custom functions with purrr? Yes, pass any custom or anonymous function to map()."
  },
  {
    "objectID": "appendix/r-packages/purrr.html#best-practices",
    "href": "appendix/r-packages/purrr.html#best-practices",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "Use map_* variants for predictable output types.\nCombine purrr with other tidyverse packages for efficient workflows.\nLeverage error-handling tools like safely() for robust code."
  },
  {
    "objectID": "appendix/r-packages/purrr.html#conclusion",
    "href": "appendix/r-packages/purrr.html#conclusion",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "The purrr package simplifies iteration, functional programming, and working with nested data. Its consistent syntax and integration with the tidyverse make it an essential tool for R users.\n\n\n\n\nExplore purrr’s advanced tools like transpose() and possibly().\nIntegrate with visualization packages like ggplot2 for applied analyses."
  },
  {
    "objectID": "appendix/r-packages/purrr.html#references-and-resources",
    "href": "appendix/r-packages/purrr.html#references-and-resources",
    "title": "Tutorial for the purrr Package",
    "section": "",
    "text": "Official Documentation\nTidyverse Website\nR for Data Science\n\n\n\n\n\n\n\ngraph TD\n    A[\"Data Input\"] --&gt; B[\"Apply map()\"]\n    B --&gt; C[\"Iterate over elements\"]\n    C --&gt; D[\"Transform Results\"]\n    D --&gt; E[\"Output List/Vector\"]\n\n\n\n\nFigure 1: Data Transformation Workflow"
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html",
    "href": "appendix/r-packages/quarto-tutorial.html",
    "title": "Quarto Tutorial",
    "section": "",
    "text": "Quarto is an open-source publishing system that integrates seamlessly with RStudio and Visual Studio Code (VS Code). It supports a variety of output formats, including reports, websites, presentations, and books, using a unified framework. Key packages include:\n\nquarto for rendering documents.\nrmarkdown for compatibility.\nknitr for code chunks and outputs.\n\n\n\n\n\nAcademic papers and reports.\nInteractive tutorials and books.\nData science dashboards and presentations.\nWebsites with integrated data visualizations.\n\n\n\n\n\nUnified workflows for diverse output formats.\nMarkdown simplicity with advanced functionality.\nIntegration with RStudio and VS Code for streamlined development.\nRich ecosystem supporting code in R, Python, and Julia."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#introduction",
    "href": "appendix/r-packages/quarto-tutorial.html#introduction",
    "title": "Quarto Tutorial",
    "section": "",
    "text": "Quarto is an open-source publishing system that integrates seamlessly with RStudio and Visual Studio Code (VS Code). It supports a variety of output formats, including reports, websites, presentations, and books, using a unified framework. Key packages include:\n\nquarto for rendering documents.\nrmarkdown for compatibility.\nknitr for code chunks and outputs.\n\n\n\n\n\nAcademic papers and reports.\nInteractive tutorials and books.\nData science dashboards and presentations.\nWebsites with integrated data visualizations.\n\n\n\n\n\nUnified workflows for diverse output formats.\nMarkdown simplicity with advanced functionality.\nIntegration with RStudio and VS Code for streamlined development.\nRich ecosystem supporting code in R, Python, and Julia."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#getting-started",
    "href": "appendix/r-packages/quarto-tutorial.html#getting-started",
    "title": "Quarto Tutorial",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and Updating Quarto\n\nFrom the Web\n\nVisit the Quarto Downloads Page.\nChoose the installer for your operating system:\n\nWindows: Download the .exe file and follow the on-screen instructions.\nMac: Download the .pkg file and install.\nLinux: Follow the provided instructions to install using your package manager.\n\n\n\n\nFrom the Command Line\n\nWindows: Open a terminal (e.g., Command Prompt or PowerShell) and run:\nwinget install quarto\nMac: Use Homebrew:\nbrew install quarto\nLinux: Use your distribution’s package manager. For example, on Ubuntu:\nsudo apt-get install quarto\n\n\n\nUpdating Quarto\n\nWeb: Download the latest version from the Quarto Downloads Page and reinstall.\nCommand Line:\n\nWindows:\nwinget upgrade quarto\nMac:\nbrew upgrade quarto\nLinux:\nsudo apt-get update && sudo apt-get upgrade quarto"
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#create-a-new-quarto-document",
    "href": "appendix/r-packages/quarto-tutorial.html#create-a-new-quarto-document",
    "title": "Quarto Tutorial",
    "section": "Create a New Quarto Document",
    "text": "Create a New Quarto Document\n\nOpen RStudio.\nClick File &gt; New File &gt; Quarto Document.\nIn the dialog that appears:\n\nChoose a Document or Presentation format.\nSelect HTML, PDF, or Word as the output format.\n\nClick OK to create the new .qmd file.\n\n\nWhat Happens?\nRStudio generates a Quarto Markdown (.qmd) file with a sample YAML header and placeholder content.\nExample of the generated file:\n---\ntitle: \"Untitled\"\nformat: html\n---\n# Introduction\n\nWelcome to Quarto!"
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#edit-the-quarto-file",
    "href": "appendix/r-packages/quarto-tutorial.html#edit-the-quarto-file",
    "title": "Quarto Tutorial",
    "section": "Edit the Quarto File",
    "text": "Edit the Quarto File\n\nAdd Content\n\nWrite plain text for documentation.\nUse Markdown syntax for formatting (e.g., headings, bold, italic, lists).\nAdd R code chunks for analysis and visualization.\n\n\nMarkdown Syntax Examples:\n# Heading 1\n## Heading 2\n### Heading 3\n\n**Bold Text**\n\n*Italic Text*\n\n- Item 1\n- Item 2\n\n\nR Code Chunk:\nAdd an R code block using:\n\n```{r}\nsummary(mtcars)\n```"
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#render-the-document",
    "href": "appendix/r-packages/quarto-tutorial.html#render-the-document",
    "title": "Quarto Tutorial",
    "section": "Render the Document",
    "text": "Render the Document\n\nSave the .qmd file.\nClick the Render button at the top of the script editor in RStudio.\nThe rendered document will open in the Viewer pane or your browser, depending on the output format."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#add-visualizations-and-tables",
    "href": "appendix/r-packages/quarto-tutorial.html#add-visualizations-and-tables",
    "title": "Quarto Tutorial",
    "section": "Add Visualizations and Tables",
    "text": "Add Visualizations and Tables\nUse R packages like ggplot2 for visualizations and knitr::kable() for tables.\n\nExample with Visualization:\n\n```{r}\nlibrary(ggplot2)\nggplot(mtcars, aes(x = mpg, y = hp)) + geom_point()\n```\n\n\n\nExample with a Table:\n\n```{r}\nlibrary(knitr)\nkable(head(mtcars))\n```"
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#customize-the-yaml-header",
    "href": "appendix/r-packages/quarto-tutorial.html#customize-the-yaml-header",
    "title": "Quarto Tutorial",
    "section": "Customize the YAML Header",
    "text": "Customize the YAML Header\nEdit the YAML metadata to specify title, author, date, and format.\n\nExample YAML:\n---\ntitle: \"My First Quarto Document\"\nauthor: \"John Doe\"\ndate: \"2025-01-08\"\nformat:\n  html:\n    theme: cerulean\n    toc: true\n---\n\ntheme: Changes the HTML theme (e.g., cerulean, cosmo).\ntoc: Adds a table of contents."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#work-with-multiple-outputs",
    "href": "appendix/r-packages/quarto-tutorial.html#work-with-multiple-outputs",
    "title": "Quarto Tutorial",
    "section": "Work with Multiple Outputs",
    "text": "Work with Multiple Outputs\nTo generate multiple output formats (e.g., HTML, PDF, Word), update the YAML header:\n---\ntitle: \"My Report\"\nformat:\n  html: default\n  pdf: default\n  docx: default\n---"
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#debug-and-preview",
    "href": "appendix/r-packages/quarto-tutorial.html#debug-and-preview",
    "title": "Quarto Tutorial",
    "section": "Debug and Preview",
    "text": "Debug and Preview\n\nDebugging\nIf the document doesn’t render: - Check for syntax errors in the .qmd file. - Ensure all required R packages are installed.\n\n\nPreview\n\nClick Preview to see live updates in the Viewer pane as you edit."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#add-interactivity",
    "href": "appendix/r-packages/quarto-tutorial.html#add-interactivity",
    "title": "Quarto Tutorial",
    "section": "Add Interactivity",
    "text": "Add Interactivity\nQuarto supports interactive documents using shiny or htmlwidgets.\n\nExample with an Interactive Plot:\n\n```{r}\nlibrary(plotly)\nplot_ly(data = mtcars, x = ~mpg, y = ~hp, type = \"scatter\", mode = \"markers\")\n```"
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#publish-or-share",
    "href": "appendix/r-packages/quarto-tutorial.html#publish-or-share",
    "title": "Quarto Tutorial",
    "section": "Publish or Share",
    "text": "Publish or Share\n\nHTML Output: Use html to render the .html file.\nPDF Output: Use typst to render the .pdf file.\nPDF Output: Use pdf to render the .pdf file if LaTeX is already installed.\nWord Output: Use docx to render the .docx file.\nOpen Document: Use odt to render the .odt file."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#installation-of-quarto",
    "href": "appendix/r-packages/quarto-tutorial.html#installation-of-quarto",
    "title": "Quarto Tutorial",
    "section": "1. Installation of Quarto",
    "text": "1. Installation of Quarto\nFirst, download and install Quarto from the official Quarto website. If you use RStudio, update to the latest version, as Quarto is integrated into the newer versions of RStudio."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#creating-a-quarto-document",
    "href": "appendix/r-packages/quarto-tutorial.html#creating-a-quarto-document",
    "title": "Quarto Tutorial",
    "section": "2. Creating a Quarto Document",
    "text": "2. Creating a Quarto Document\n\nOpen RStudio and select File -&gt; New File -&gt; Quarto Document….\nChoose the type of document you want to create (e.g., Document, Presentation, Website/Blog, Book).\nClick OK to open a new Quarto document with default content."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#understanding-the-quarto-yaml-header",
    "href": "appendix/r-packages/quarto-tutorial.html#understanding-the-quarto-yaml-header",
    "title": "Quarto Tutorial",
    "section": "3. Understanding the Quarto YAML Header",
    "text": "3. Understanding the Quarto YAML Header\nThe YAML header at the top of your document specifies metadata such as the title, author, date, and format. Here is an example:\n---\ntitle: \"My First Quarto Document\"\nauthor: \"Your Name\"\ndate: \"2024-08-30\"\nformat: html\n---\nYou can modify the format to pdf, docx, pptx, etc., depending on your output needs."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#adding-content-to-your-document",
    "href": "appendix/r-packages/quarto-tutorial.html#adding-content-to-your-document",
    "title": "Quarto Tutorial",
    "section": "4. Adding Content to Your Document",
    "text": "4. Adding Content to Your Document\nQuarto uses Markdown for text formatting. You can use standard Markdown syntax for headers, lists, bold, italics, and more.\n\nHeaders:\n# Main Heading\n## Sub Heading\n### Sub Sub Heading\nLists:\n- Item 1\n- Item 2\n    - Sub Item 1\n    - Sub Item 2\nBold and Italics:\n**Bold Text**\n_Italic Text_"
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#code-chunks-in-quarto",
    "href": "appendix/r-packages/quarto-tutorial.html#code-chunks-in-quarto",
    "title": "Quarto Tutorial",
    "section": "5. Code Chunks in Quarto",
    "text": "5. Code Chunks in Quarto\nYou can include code chunks to run R code. Here’s an example with your preferred table label format:\n```{r}\n#| label:tbl-summary\nsummary(cars)\n```\nThis code chunk will run in R when the document is rendered."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#callouts-in-quarto",
    "href": "appendix/r-packages/quarto-tutorial.html#callouts-in-quarto",
    "title": "Quarto Tutorial",
    "section": "6. Callouts in Quarto",
    "text": "6. Callouts in Quarto"
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#math-typesetting-in-quarto",
    "href": "appendix/r-packages/quarto-tutorial.html#math-typesetting-in-quarto",
    "title": "Quarto Tutorial",
    "section": "7. Math Typesetting in Quarto",
    "text": "7. Math Typesetting in Quarto\nQuarto uses LaTeX syntax for math typesetting, which is similar to R Markdown. You can include inline math or display math using the $ symbols.\n\nInline Math:\n\nTo include inline math, wrap your expression in a single dollar sign:\nThe formula for the area of a circle is $A = \\pi r^2$.\n\nDisplay Math:\n\nFor display math (centered on its own line), use double dollar signs:\n$$\nE = mc^2\n$$\nYou can also use LaTeX environments like align for aligned equations:\n$$\n\\begin{align}\na &= b + c \\\\\nd &= e - f\n\\end{align}\n$$\nQuarto will render these expressions beautifully in your output document."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#running-and-rendering-your-document",
    "href": "appendix/r-packages/quarto-tutorial.html#running-and-rendering-your-document",
    "title": "Quarto Tutorial",
    "section": "8. Running and Rendering Your Document",
    "text": "8. Running and Rendering Your Document\nClick the Render button in RStudio to run all code chunks and generate the output in the specified format (e.g., HTML, PDF). This will execute the code, format text, and apply callouts and math typesetting."
  },
  {
    "objectID": "appendix/r-packages/quarto-tutorial.html#using-ggplot2-for-plots-in-quarto",
    "href": "appendix/r-packages/quarto-tutorial.html#using-ggplot2-for-plots-in-quarto",
    "title": "Quarto Tutorial",
    "section": "9. Using Ggplot2 for Plots in Quarto",
    "text": "9. Using Ggplot2 for Plots in Quarto\nTo include a plot, you can use the ggplot2 package:\n```{r}\n#| echo: fenced\n#| label:fig-logistic\nlibrary(ggplot2)\ndata &lt;- data.frame(x = rnorm(100), y = rbinom(100, 1, 0.5))\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE)\n```"
  },
  {
    "objectID": "appendix/r-packages/stringr.html",
    "href": "appendix/r-packages/stringr.html",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "The stringr package, part of the tidyverse, simplifies string manipulation in R by providing a consistent and human-readable interface for common operations like pattern matching, extraction, replacement, and string metrics. It builds on the base R string functions but offers better integration with modern workflows and tools.\nKey functions include:\n\nBasic Manipulation: str_c(), str_length(), str_trim(), str_pad().\nPattern Matching: str_detect(), str_extract(), str_match().\nSubstitution: str_replace(), str_replace_all().\nSplitting and Joining: str_split(), str_flatten().\nOther Utilities: str_to_upper(), str_to_title(), str_sort().\n\n\n\n\n\nCleaning and formatting text data for analysis.\nExtracting structured information from unstructured text.\nDetecting and replacing patterns in strings.\nComparing and sorting textual data.\n\n\n\n\n\nConsistent and simple syntax.\nHandles regular expressions for powerful pattern matching.\nFully vectorized for efficient processing of large datasets.\nEasily integrates with other tidyverse tools.\n\n\n\n\n\n\n\n\nEnsure stringr is loaded using the pacman package:\n\npacman::p_load(stringr)\n\n\n\n\nA simple example of string concatenation using str_c():\n\n# Concatenate strings\nstr_c(\"Hello\", \"World\", sep = \" \")\n\n[1] \"Hello World\"\n\n\nThis outputs “Hello World”, showcasing the basic functionality of combining strings.\n\n\n\n\n\n\n\n\nCombines strings into a single string or vector of strings.\n\n\n\nstr_c(..., sep = \"\", collapse = NULL)\n\n…: Strings to combine.\nsep: Separator between strings.\ncollapse: Collapses output into a single string.\n\n\n\n\n\n# Combine names with separators\nnames &lt;- c(\"John\", \"Jane\", \"Doe\")\nstr_c(names, collapse = \", \")\n\n[1] \"John, Jane, Doe\"\n\n\n\n\n\n\n\n\nReturns the number of characters in each string.\n\n\n\nstr_length(string)\n\n\n\n\n# Get string lengths\nstrings &lt;- c(\"apple\", \"banana\", \"cherry\")\nstr_length(strings)\n\n[1] 5 6 6\n\n\n\n\n\n\n\n\nDetects if a pattern exists in a string.\n\n\n\nstr_detect(string, pattern)\n\n\n\n\n# Detect strings containing \"an\"\nstrings &lt;- c(\"apple\", \"banana\", \"cherry\")\nstr_detect(strings, \"an\")\n\n[1] FALSE  TRUE FALSE\n\n\n\n\n\n\n\n\nExtracts the first match of a pattern from a string.\n\n\n\nstr_extract(string, pattern)\n\n\n\n\n# Extract numbers from strings\nstrings &lt;- c(\"Item 1\", \"Price 20\", \"Count 100\")\nstr_extract(strings, \"\\\\d+\")\n\n[1] \"1\"   \"20\"  \"100\"\n\n\n\n\n\n\n\n\nReplaces all occurrences of a pattern in a string.\n\n\n\nstr_replace_all(string, pattern, replacement)\n\n\n\n\n# Replace spaces with underscores\nstrings &lt;- c(\"apple pie\", \"banana split\", \"cherry tart\")\nstr_replace_all(strings, \" \", \"_\")\n\n[1] \"apple_pie\"    \"banana_split\" \"cherry_tart\" \n\n\n\n\n\n\n\n\n\n\n\nIdentify and extract phone numbers from text data.\n\n\n\n\nPrepare data:\n\n\ntext &lt;- c(\"Call me at 555-123-4567\", \"My number is 987-654-3210\", \"No phone here\")\n\n\nDefine a regex pattern:\n\n\npattern &lt;- \"\\\\d{3}-\\\\d{3}-\\\\d{4}\"\n\n\nExtract phone numbers:\n\n\nphone_numbers &lt;- str_extract(text, pattern)\nphone_numbers\n\n[1] \"555-123-4567\" \"987-654-3210\" NA            \n\n\n\nOutput interpretation: Non-matching strings return NA.\n\n\n\n\n\n\n\nCheck if a dataset contains email addresses.\n\n\n\n\nInput data:\n\n\nstrings &lt;- c(\"Contact: john.doe@example.com\", \"Visit us\", \"Email: support@site.org\")\n\n\nDetect emails:\n\n\nemail_pattern &lt;- \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\"\nemail_detected &lt;- str_detect(strings, email_pattern)\nemail_detected\n\n[1]  TRUE FALSE  TRUE\n\n\n\n\n\n\n\n\nSplit sentences into individual words.\n\n\n\n\nDefine sentences:\n\n\nsentences &lt;- c(\"This is an example\", \"R is great for text\")\n\n\nSplit into words:\n\n\nwords &lt;- str_split(sentences, \" \")\nwords\n\n[[1]]\n[1] \"This\"    \"is\"      \"an\"      \"example\"\n\n[[2]]\n[1] \"R\"     \"is\"    \"great\" \"for\"   \"text\" \n\n\n\nVisualize results:\n\n\nstr_flatten(words[[1]], \", \")\n\n[1] \"This, is, an, example\"\n\n\n\n\n\n\n\n\n\nUse additional parameters for flexible pattern matching, such as fixed() for fixed strings or regex() for advanced options:\n\n# Case-insensitive matching\nstr_detect(c(\"Apple\", \"banana\"), regex(\"apple\", ignore_case = TRUE))\n\n[1]  TRUE FALSE\n\n\n\n\n\nCombine with dplyr to process text in data frames:\n\nlibrary(dplyr)\n\ndata &lt;- tibble(text = c(\"One 1\", \"Two 2\", \"Three 3\"))\ndata |&gt; mutate(numbers = str_extract(text, \"\\\\d+\"))\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRegex not matching: Ensure patterns are correctly escaped. Use \\\\ for special characters like \\d.\nUnexpected results: Verify input data type and length. NA values in input may propagate.\n\n\n\n\n\nCan stringr handle non-ASCII text? Yes, it supports UTF-8 encoded text.\nHow do I extract multiple matches? Use str_extract_all().\n\n\n\n\n\n\nLeverage vectorization for efficient processing of large datasets.\nUse clear and descriptive regex patterns.\nCombine with tidyverse tools for seamless workflows.\n\n\n\n\n\n\nThe stringr package offers powerful and intuitive tools for string manipulation in R. Its consistent syntax and comprehensive feature set make it a go-to package for text processing tasks.\n\n\n\n\nExplore advanced regex patterns.\nLearn about glue for string interpolation.\n\n\n\n\n\n\nOfficial Documentation\nRegex Cheatsheet\nR for Data Science: String Manipulation\n\n\n\n\n\n\n\ngraph TD\n    A[Raw Text Data] --&gt; B[Pattern Matching]\n    B --&gt; C[Extraction]\n    B --&gt; D[Substitution]\n    C --&gt; E[Cleaned Data]\n    D --&gt; E\n\n\n\n\nFigure 1: Text Data Workflow"
  },
  {
    "objectID": "appendix/r-packages/stringr.html#introduction",
    "href": "appendix/r-packages/stringr.html#introduction",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "The stringr package, part of the tidyverse, simplifies string manipulation in R by providing a consistent and human-readable interface for common operations like pattern matching, extraction, replacement, and string metrics. It builds on the base R string functions but offers better integration with modern workflows and tools.\nKey functions include:\n\nBasic Manipulation: str_c(), str_length(), str_trim(), str_pad().\nPattern Matching: str_detect(), str_extract(), str_match().\nSubstitution: str_replace(), str_replace_all().\nSplitting and Joining: str_split(), str_flatten().\nOther Utilities: str_to_upper(), str_to_title(), str_sort().\n\n\n\n\n\nCleaning and formatting text data for analysis.\nExtracting structured information from unstructured text.\nDetecting and replacing patterns in strings.\nComparing and sorting textual data.\n\n\n\n\n\nConsistent and simple syntax.\nHandles regular expressions for powerful pattern matching.\nFully vectorized for efficient processing of large datasets.\nEasily integrates with other tidyverse tools."
  },
  {
    "objectID": "appendix/r-packages/stringr.html#getting-started",
    "href": "appendix/r-packages/stringr.html#getting-started",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "Ensure stringr is loaded using the pacman package:\n\npacman::p_load(stringr)\n\n\n\n\nA simple example of string concatenation using str_c():\n\n# Concatenate strings\nstr_c(\"Hello\", \"World\", sep = \" \")\n\n[1] \"Hello World\"\n\n\nThis outputs “Hello World”, showcasing the basic functionality of combining strings."
  },
  {
    "objectID": "appendix/r-packages/stringr.html#key-functions-and-features",
    "href": "appendix/r-packages/stringr.html#key-functions-and-features",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "Combines strings into a single string or vector of strings.\n\n\n\nstr_c(..., sep = \"\", collapse = NULL)\n\n…: Strings to combine.\nsep: Separator between strings.\ncollapse: Collapses output into a single string.\n\n\n\n\n\n# Combine names with separators\nnames &lt;- c(\"John\", \"Jane\", \"Doe\")\nstr_c(names, collapse = \", \")\n\n[1] \"John, Jane, Doe\"\n\n\n\n\n\n\n\n\nReturns the number of characters in each string.\n\n\n\nstr_length(string)\n\n\n\n\n# Get string lengths\nstrings &lt;- c(\"apple\", \"banana\", \"cherry\")\nstr_length(strings)\n\n[1] 5 6 6\n\n\n\n\n\n\n\n\nDetects if a pattern exists in a string.\n\n\n\nstr_detect(string, pattern)\n\n\n\n\n# Detect strings containing \"an\"\nstrings &lt;- c(\"apple\", \"banana\", \"cherry\")\nstr_detect(strings, \"an\")\n\n[1] FALSE  TRUE FALSE\n\n\n\n\n\n\n\n\nExtracts the first match of a pattern from a string.\n\n\n\nstr_extract(string, pattern)\n\n\n\n\n# Extract numbers from strings\nstrings &lt;- c(\"Item 1\", \"Price 20\", \"Count 100\")\nstr_extract(strings, \"\\\\d+\")\n\n[1] \"1\"   \"20\"  \"100\"\n\n\n\n\n\n\n\n\nReplaces all occurrences of a pattern in a string.\n\n\n\nstr_replace_all(string, pattern, replacement)\n\n\n\n\n# Replace spaces with underscores\nstrings &lt;- c(\"apple pie\", \"banana split\", \"cherry tart\")\nstr_replace_all(strings, \" \", \"_\")\n\n[1] \"apple_pie\"    \"banana_split\" \"cherry_tart\""
  },
  {
    "objectID": "appendix/r-packages/stringr.html#in-depth-examples",
    "href": "appendix/r-packages/stringr.html#in-depth-examples",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "Identify and extract phone numbers from text data.\n\n\n\n\nPrepare data:\n\n\ntext &lt;- c(\"Call me at 555-123-4567\", \"My number is 987-654-3210\", \"No phone here\")\n\n\nDefine a regex pattern:\n\n\npattern &lt;- \"\\\\d{3}-\\\\d{3}-\\\\d{4}\"\n\n\nExtract phone numbers:\n\n\nphone_numbers &lt;- str_extract(text, pattern)\nphone_numbers\n\n[1] \"555-123-4567\" \"987-654-3210\" NA            \n\n\n\nOutput interpretation: Non-matching strings return NA.\n\n\n\n\n\n\n\nCheck if a dataset contains email addresses.\n\n\n\n\nInput data:\n\n\nstrings &lt;- c(\"Contact: john.doe@example.com\", \"Visit us\", \"Email: support@site.org\")\n\n\nDetect emails:\n\n\nemail_pattern &lt;- \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\"\nemail_detected &lt;- str_detect(strings, email_pattern)\nemail_detected\n\n[1]  TRUE FALSE  TRUE\n\n\n\n\n\n\n\n\nSplit sentences into individual words.\n\n\n\n\nDefine sentences:\n\n\nsentences &lt;- c(\"This is an example\", \"R is great for text\")\n\n\nSplit into words:\n\n\nwords &lt;- str_split(sentences, \" \")\nwords\n\n[[1]]\n[1] \"This\"    \"is\"      \"an\"      \"example\"\n\n[[2]]\n[1] \"R\"     \"is\"    \"great\" \"for\"   \"text\" \n\n\n\nVisualize results:\n\n\nstr_flatten(words[[1]], \", \")\n\n[1] \"This, is, an, example\""
  },
  {
    "objectID": "appendix/r-packages/stringr.html#advanced-features",
    "href": "appendix/r-packages/stringr.html#advanced-features",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "Use additional parameters for flexible pattern matching, such as fixed() for fixed strings or regex() for advanced options:\n\n# Case-insensitive matching\nstr_detect(c(\"Apple\", \"banana\"), regex(\"apple\", ignore_case = TRUE))\n\n[1]  TRUE FALSE\n\n\n\n\n\nCombine with dplyr to process text in data frames:\n\nlibrary(dplyr)\n\ndata &lt;- tibble(text = c(\"One 1\", \"Two 2\", \"Three 3\"))\ndata |&gt; mutate(numbers = str_extract(text, \"\\\\d+\"))"
  },
  {
    "objectID": "appendix/r-packages/stringr.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/stringr.html#troubleshooting-and-faqs",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "Regex not matching: Ensure patterns are correctly escaped. Use \\\\ for special characters like \\d.\nUnexpected results: Verify input data type and length. NA values in input may propagate.\n\n\n\n\n\nCan stringr handle non-ASCII text? Yes, it supports UTF-8 encoded text.\nHow do I extract multiple matches? Use str_extract_all()."
  },
  {
    "objectID": "appendix/r-packages/stringr.html#best-practices",
    "href": "appendix/r-packages/stringr.html#best-practices",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "Leverage vectorization for efficient processing of large datasets.\nUse clear and descriptive regex patterns.\nCombine with tidyverse tools for seamless workflows."
  },
  {
    "objectID": "appendix/r-packages/stringr.html#conclusion",
    "href": "appendix/r-packages/stringr.html#conclusion",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "The stringr package offers powerful and intuitive tools for string manipulation in R. Its consistent syntax and comprehensive feature set make it a go-to package for text processing tasks.\n\n\n\n\nExplore advanced regex patterns.\nLearn about glue for string interpolation."
  },
  {
    "objectID": "appendix/r-packages/stringr.html#references-and-resources",
    "href": "appendix/r-packages/stringr.html#references-and-resources",
    "title": "Tutorial for the stringr Package",
    "section": "",
    "text": "Official Documentation\nRegex Cheatsheet\nR for Data Science: String Manipulation\n\n\n\n\n\n\n\ngraph TD\n    A[Raw Text Data] --&gt; B[Pattern Matching]\n    B --&gt; C[Extraction]\n    B --&gt; D[Substitution]\n    C --&gt; E[Cleaned Data]\n    D --&gt; E\n\n\n\n\nFigure 1: Text Data Workflow"
  },
  {
    "objectID": "appendix/r-topics/probing-three-way-interaction.html",
    "href": "appendix/r-topics/probing-three-way-interaction.html",
    "title": "Probing Three-Way Interaction in ANOVA",
    "section": "",
    "text": "This tutorial will include:\npacman::p_load(emmeans)\ndata(\"auto.noise\")\nmodel &lt;- aov(noise ~ size * type * side, data = auto.noise)\nsummary(model)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsize            2  26051   13026 893.190  &lt; 2e-16 ***\ntype            1   1056    1056  72.429 1.04e-08 ***\nside            1      1       1   0.048 0.829104    \nsize:type       2    804     402  27.571 6.05e-07 ***\nsize:side       2   1293     647  44.333 8.73e-09 ***\ntype:side       1     17      17   1.190 0.286067    \nsize:type:side  2    301     151  10.333 0.000579 ***\nResiduals      24    350      15                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTable 1\nHere we see significant two-way interactions (size:type, size:side) and a significant three-way interaction (size:type:side). The presence of these interactions warns us that interpreting main effects alone (e.g. size differences averaged over type and side) could be misleading. Instead, we proceed to probe the interactions with simple effects."
  },
  {
    "objectID": "appendix/r-topics/probing-three-way-interaction.html#using-emmeans-for-simple-effects",
    "href": "appendix/r-topics/probing-three-way-interaction.html#using-emmeans-for-simple-effects",
    "title": "Probing Three-Way Interaction in ANOVA",
    "section": "Using emmeans for simple effects",
    "text": "Using emmeans for simple effects\nThe emmeans package provides multiple approaches to examine simple effects:\n\nConditioning with | in formula: We can request simple means and pairwise comparisons by conditioning on one factor. For example, emmeans(model, pairwise ~ size | type) asks: give me the means of size at each level of type, and compare them. This yields the simple main effects of size within each type (with appropriate warnings or adjustments since side is also in the model).\nUsing emmeans followed by joint_tests(): This is useful for testing a simple omnibus effect (like an \\(F\\) test for any difference among the levels of a factor within a condition). We first get an emmeans object restricted to the cells of interest, then apply joint_tests(by = …) to test that factor within each condition. This corresponds to the F-tests of simple effects.\nUsing contrast(): After obtaining an emmeans grid, we can specify custom contrasts or use built-in contrast families (pairwise for all pairwise differences, consec for consecutive comparisons, etc.) to probe specific comparisons."
  },
  {
    "objectID": "appendix/r-topics/probing-three-way-interaction.html#lets-apply-these-to-our-example",
    "href": "appendix/r-topics/probing-three-way-interaction.html#lets-apply-these-to-our-example",
    "title": "Probing Three-Way Interaction in ANOVA",
    "section": "Let’s apply these to our example:",
    "text": "Let’s apply these to our example:\n\nSimple two-way interactions–examine each two-way interaction at levels of the third factor. Given the significant three-way interaction, any two-way interaction might differ by the third factor. For instance, we check the size:type interaction at each side:\n\n\n# Simple two-way interaction: size by type at each side\nemm_size_type = emmeans(model, ~ size * type | side) \nemm_size_type |&gt; joint_tests(by = \"side\")\n\n\n\n\n\n\nmodel term\nside\ndf1\ndf2\nF.ratio\np.value\n\n\n\n\n1\nsize\nL\n2\n24\n651.714\n0.00e+00\n\n\n3\nsize\nR\n2\n24\n285.810\n0.00e+00\n\n\n12\ntype\nL\n1\n24\n46.095\n5.00e-07\n\n\n2\ntype\nR\n1\n24\n27.524\n2.23e-05\n\n\n11\nsize:type\nL\n2\n24\n23.524\n2.20e-06\n\n\n31\nsize:type\nR\n2\n24\n14.381\n7.85e-05\n\n\n\n\n\n\nThis yields F-tests for the size:type interaction separately for side = left and side = right (each test has \\(df_1 = (a-1)(b-1) = 2\\) for size(3)×type(2), and \\(df_{2} = \\text{ residual df} = 24\\)).\nThis indicates the size×type interaction is significant on the right side but maybe weaker on the left. That asymmetry is exactly what the three-way interaction captures.\n\nSimple main effects–now dive deeper. Say we want the simple effect of size at each combination of type and side. We can do that with:\n\n\n# Simple main effect of size at each combination of type and side\nemm_size_by_type_side = emmeans(model, ~ size | type * side)\npairs(emm_size_by_type_side)  # pairwise comparisons for size within each type-side combo\n\ntype = Std, side = L:\n contrast estimate   SE df t.ratio p.value\n S - M      -15.00 3.12 24  -4.811  0.0002\n S - L       71.67 3.12 24  22.984  &lt;.0001\n M - L       86.67 3.12 24  27.795  &lt;.0001\n\ntype = Octel, side = L:\n contrast estimate   SE df t.ratio p.value\n S - M        3.33 3.12 24   1.069  0.5419\n S - L       60.00 3.12 24  19.243  &lt;.0001\n M - L       56.67 3.12 24  18.174  &lt;.0001\n\ntype = Std, side = R:\n contrast estimate   SE df t.ratio p.value\n S - M      -25.00 3.12 24  -8.018  &lt;.0001\n S - L       30.00 3.12 24   9.621  &lt;.0001\n M - L       55.00 3.12 24  17.639  &lt;.0001\n\ntype = Octel, side = R:\n contrast estimate   SE df t.ratio p.value\n S - M       -1.67 3.12 24  -0.535  0.8552\n S - L       45.00 3.12 24  14.432  &lt;.0001\n M - L       46.67 3.12 24  14.967  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nEach section provides the differences between car sizes (S, M, L) when we fix type and side. The p-values tell us which differences are significant. We could also use joint_tests() on the same grid to get an omnibus F for “any size effect” at that condition, which should match the contrasts’ overall significance.\n\nUsing emmeans::joint_tests() for simple effects–an alternative to the above is:\n\n\nemmeans(model, ~ size | type * side)|&gt; joint_tests(by = c(\"type\",\"side\"))\n\n\n\n\n\n\nmodel term\ntype\nside\ndf1\ndf2\nF.ratio\np.value\n\n\n\n\n1\nsize\nStd\nL\n2\n24\n441.333\n0\n\n\n3\nsize\nOctel\nL\n2\n24\n233.905\n0\n\n\n5\nsize\nStd\nR\n2\n24\n156.000\n0\n\n\n7\nsize\nOctel\nR\n2\n24\n144.190\n0\n\n\n\n\n\n\nindicating whether the simple main effect of size is significant in each condition. One might find that, for example, size is highly significant for type A on the right side, but not significant for type B on the left side, etc. This aligns with the idea of the three-way interaction: the effect of size differs depending on type and side.\n\nComplex custom contrasts–We could even test specific theoretical contrasts. For example, if we predicted that “for type A, the difference between small and medium cars is larger on the right side than left side”, this is a contrast of cell means: (mean[S,A,right] - mean[M,A,right]) vs (mean[S,A,left] - mean[M,A,left]). We can set up such differences with the contrast() function by specifying a matrix or using the interaction argument for factorial contrasts. However, for brevity, we’ll focus on more standard simple effects.\n\nInterpreting results: After running these analyses, interpretation goes as follows: We report where significant simple effects occur. For instance, “There was a significant simple effect of car size for Type A filters on the right side (F(2,24)=…, p&lt;- .01), but no significant size effect for Type A on the left side (F(2,24)=…, p=.n.s.). Specifically, on the right side with Type A, medium cars had higher noise than small cars by 0.96 units (SE=0.16, p&lt;- .001), and large cars had significantly lower noise than both small and medium cars (all p&lt;- .001). In contrast, for Type B filters, …” etc. We weave together the story that the interaction is painting.\nCommon issues and best practices:\n\nMultiplicity: If you test many simple effects (and then pairwise comparisons within them), adjust for multiple comparisons or focus on theoretically guided comparisons. emmeans allows p-value adjustments (e.g., p.adjust = \"bonferroni\" in the contrast call).\nUnbalanced data: In unbalanced designs, use emmeans(model, specs = …) with care. The default in emmeans is equivalent to Type I (sequential) sums of squares unless you constructed the model with contrasts or used car::Anova(type=3). For unbalanced data, consider using emmeans on a model with appropriate Type-II/III methods if necessary.\nInteractions with continuous covariates: The concept of simple effects extends (simple slopes in regression, Johnson-Neyman technique, etc.), but emmeans can still help via its emtrends function for simple slopes. In a pure ANOVA (all categorical), we don’t worry about that.\nModel assumptions: Check homogeneity of variance (e.g., Levene’s test) because the pooled error approach relies on it. If violated, you might need a different approach (perhaps a mixed model with heterogeneous variance, or separate one-way ANOVAs as a last resort, though that complicates error terms).\n\nBelow is a concrete R example using a built-in dataset for demonstration (assuming auto.noise is available):\nlibrary(emmeans)\ndata(\"auto.noise\")\n# Fit three-way ANOVA\nm &lt;- lm(noise ~ size * type * side, data = auto.noise)\n\n# 3-way interaction significant? (assume yes from earlier)\nanova(m)\n\n## Simple two-way (size:type) at each side:\nemm_sxt &lt;- emmeans(m, ~ size * type | side)\njoint_tests(emm_sxt, by = \"side\")\n\n## Output interpretation:\n# For each side, see if size:type interaction is sig.\n\n## Now simple main effect of size at each combination of type and side:\nemm_size &lt;- emmeans(m, ~ size | type * side)\nsimple_size_tests &lt;- joint_tests(emm_size, by = c(\"type\",\"side\"))\nsimple_size_pairs &lt;- pairs(emm_size)\n\n# Inspect results\nsimple_size_tests\nsimple_size_pairs\nThis script will print tables for simple_size_tests (F tests for size in every type×side condition) and simple_size_pairs (the list of pairwise comparisons of size levels in each condition, with estimates and p-values). In practice, you would report the F tests to show where the effect is significant, and then the pairwise comparisons to elaborate on which specific differences drive that effect, ideally using an adjusted significance criterion.\nFinally, one might visualize the three-way interaction with an interaction plot (not asked for here), but it’s often useful to plot, say, lines for size across side, faceted by type. This can qualitatively show the interaction patterns that our tests confirm statistically."
  },
  {
    "objectID": "appendix/r-topics/probing-three-way-interaction.html#comprehension-questions",
    "href": "appendix/r-topics/probing-three-way-interaction.html#comprehension-questions",
    "title": "Probing Three-Way Interaction in ANOVA",
    "section": "4. Comprehension Questions",
    "text": "4. Comprehension Questions\nTo ensure a deep understanding of simple effects and probing interactions, consider the following questions:\nConceptual Questions:\n\nInterpreting Interactions: In your own words, what does a significant three-way interaction (A×B×C) tell you about the data? Why can focusing only on main effects be misleading in this case?\nDefinitions: Define a simple main effect in the context of a three-way ANOVA. How does it differ from an ordinary main effect?\nHierarchy of Effects: If you find a significant three-way interaction, what is the recommended sequence of follow-up analyses? (Hint: Think about simple two-way interactions and then simple main effects.)\nVisualization: Describe how you might graphically display a three-way interaction to aid interpretation. What patterns in the plot would suggest simple effects worth testing?\nTheory to Practice: A researcher finds an A×B×C interaction. They claim “Factor A matters only for one combination of B and C; otherwise it has no effect.” How would you verify this claim using simple effects analysis?\n\nComputational Questions:\n\nGiven a balanced 2×2×2 ANOVA design (all factors have 2 levels, n per cell), how many degrees of freedom does the A×B×C interaction have? If the interaction F-test has \\(df_1=1\\) and yields \\(p&lt;- 0.05\\), what does that imply about the differences in two-way interactions?\nIn the emmeans output below (for a simple effect of A at B=b and C=c), fill in the blanks and interpret the results:\nC = c1, B = b1:\nA emmean   SE df lower.CL upper.CL\na1   50.0  2.1 24    45.6    54.4\na2   55.5  2.1 24    51.1    59.9\na3   57.0  2.1 24    52.6    61.4\n\nConfidence level used: 0.95 \n\nC = c1, B = b1:\ncontrast    estimate   SE df t.ratio p.value\na1 - a2    _____      ___ __  -2.50   0.020 \na1 - a3    -7.0      2.1 24  -3.33   0.002 \na2 - a3     _____      ___ __   ?      ?   \nWhat are the missing values for the a1 - a2 estimate and for a2 - a3 (t.ratio, p.value)? Is the simple main effect of A at (B=b1,C=c1) significant?\nSuppose joint_tests(emmeans(model, ~ A | B*C)) returns a p-value of 0.8 for the simple effect of A at B=b2, C=c3. What does that imply about factor A’s impact in that condition? How would you report it?\n\nApplication Questions:\n\nYou conducted a three-way ANOVA and found no significant three-way interaction, but one of the two-way interactions (A×B) is significant. Describe how you would follow up on the A×B interaction using simple effects (mention emmeans steps and what to look for).\nIn R, if you have an emmeans object for a three-way layout (A, B, C), how can you extract the cell means for all combinations? And how could you use those to manually compute (for example) the simple effect of A at C=c2 (averaging over B)?\nA colleague ran emmeans(model, pairwise ~ A | B) in R and got a warning: \"Results may be misleading due to involvement in interactions\". They ask you why the warning appeared and how to properly obtain the comparisons they want. What is your explanation and advice?\nConsider a scenario with a 3×3×2 design where the A×B×C interaction is significant. How would you determine if the two-way interaction A×B is present at each level of C? What emmeans code would you write, and how would you interpret a nonsignificant result for one level of C versus a significant result for the other?\n\nThese questions range from conceptual understanding to practical application, ensuring you think about why and how we probe interactions."
  },
  {
    "objectID": "appendix/r-topics/probing-three-way-interaction.html#detailed-solutions",
    "href": "appendix/r-topics/probing-three-way-interaction.html#detailed-solutions",
    "title": "Probing Three-Way Interaction in ANOVA",
    "section": "5. Detailed Solutions",
    "text": "5. Detailed Solutions\n1. Interpreting Interactions: A significant three-way interaction indicates that the way any two factors interact is different across levels of the third factor. For example, maybe A and B have a strong interaction when C is low, but no interaction when C is high. If we looked only at main effects, we’d miss this nuance. Main effects average over all levels of other factors; in doing so, they can cancel out differences. It’s misleading because a main effect might be non-significant overall even if the factor has a large effect in one condition and an opposite effect in another. Or a main effect might appear significant, but in reality the effect exists only under certain conditions (and reverses elsewhere). Thus, focusing only on main effects in the presence of interactions can lead to incorrect or incomplete conclusions. Always first assess interactions: if an interaction is significant, interpret the simple effects (conditional effects) rather than the marginal (main) effects.\n2. Definitions: A simple main effect for factor A in a three-way ANOVA is the effect of A at a specific level of B and/or C. For example, “the effect of A when B = b1 and C = c2” (a fully simple case with both other factors fixed) or “the effect of A at B = b1 (averaged over C)” are both simple effects. In contrast, an ordinary main effect of A is the effect of A averaged across all levels of B and C. The difference is that simple effects are conditional on certain levels of other factors, while main effects are marginal (averaged). Simple effects help us understand interactions: if there’s no interaction, all the simple effects of A will be identical (or very similar) across levels of B and C, matching the main effect. If there is an interaction, simple effects differ, and the main effect of A becomes a blend of possibly divergent effects in each condition.\n3. Hierarchy of Effects: After finding a significant three-way interaction, the typical sequence is: - Simple two-way interactions: Pick one factor as the moderator (often the one with more levels or based on theory). For each level of that moderator, test the two-way interaction of the other two factors. For example, test A×B at each level of C (as we did with joint_tests(emm_size_type, by=\"side\") above). - If a simple two-way interaction is significant, probe that two-way interaction further: examine simple main effects within that two-way slice. E.g., if A×B at C=c1 is significant, then test A at each level of B (or B at each level of A) for C=c1. This can involve emmeans(model, ~ A | B, subset = C==c1) or similar. - Pairwise comparisons: If a simple main effect is significant (more than 2 levels), do pairwise comparisons (with adjustment) to see which specific means differ. - Throughout, maintain clarity with the error term (use pooled error from full model if assumptions hold, which emmeans does by default via the full model). - If the three-way had not been significant, we’d then interpret significant two-way interactions similarly: break them down into simple effects of one factor at levels of the other, etc.. - Always keep the number of tests reasonable and adjust for multiple comparisons to avoid false positives.\n4. Visualization: To graphically display a three-way interaction, one common approach is an interaction plot. For example, use lines to represent one factor, the x-axis for second factor, and create separate panels (facets) for the third factor. Suppose A on x-axis, Y on y-axis, lines for B, and separate plots for each level of C. If the lines (representing B) have different shapes/slopes in one panel compared to another, that indicates an A×B interaction that changes with C. Patterns: - If in all C panels the lines are roughly parallel (no crossing) and similarly spaced, there’s no significant two-way interaction difference–likely no three-way interaction. - If in Panel C1 the lines cross (interaction), but in Panel C2 they don’t (or cross differently), that’s a sign of a three-way interaction. - Such differences suggest which simple effects to test: e.g., “lines cross in C1” suggests the A×B simple interaction is significant at C1. - Also look at vertical gaps (main effects)–if a factor’s gap varies by panel, that’s an interaction involving that factor and C. - Tools like ggplot2::interaction.plot or ggplot2 with facet_wrap(~C) can create these. Visualization often guides which simple effects seem notable (though formal tests confirm it).\n5. Theory to Practice: The colleague’s claim “Factor A matters only for one combination of B and C” implies: for that specific (b, c) condition, A has an effect; elsewhere, A’s effect is zero (or negligible). To verify: - Conduct simple main effect tests of A at each combination of B and C. Using joint_tests or separate subset analyses, find the p-value for A at (b, c), and at other (b, c). - Check that A is significant at (b, c) but non-significant at all other combinations. - Also confirm the three-way interaction was significant overall (which presumably it was if such a specific effect exists). - In R, one would do: emm = emmeans(model, ~ A | B*C); joint_tests(emm, by=c(\"B\",\"C\")). - We’d report: “At B=b, C=c, factor A has a significant effect (F(…)=…, p=…), whereas in all other B–C conditions, A’s effect is not significant (all p &gt; 0.1). This pattern supports the claim that A matters only in that specific condition.” If some other condition shows a marginal effect, we’d nuance the claim. - This approach essentially probes all simple-simple effects of A to isolate where it’s active.\n6. Degrees of Freedom: In a 2×2×2 design, each factor has 1 df (2 levels–1). Two-way interactions also have \\((1*1)=1\\) df each. The three-way interaction A×B×C has \\((1*1*1) = 1\\) df. If the F-test for A×B×C with 1 df is significant (p&lt;- 0.05), it implies that the difference between the two-way interactions at different levels of the third factor is significant. For 2×2×2, it means the single two-way interaction present at C=c1 differs from the one at C=c2. Essentially, either A×B is significant in one C level and not the other, or they have opposite signs, etc. (With 1 df, “interaction differs” basically means one condition has an interaction, the other doesn’t, since an interaction in 2x2 is a single number difference of differences.) In practical terms: the pattern of A×B is not the same for C=1 vs C=2. We could say: one simple two-way interaction is “strong” and the other is zero (or they’re different in magnitude). This satisfies the definition: a three-way interaction means a two-way interaction varies across the third variable.\n7. Emmeans output interpretation: From the provided output, for C=c1, B=b1:\n\nThe means for A=a1, a2, a3 are 50.0, 55.5, 57.0.\nSo a1 - a2 difference = 50.0 - 55.5 = -5.5. That’s the estimate for a1 - a2. It likely has SE 2.1 (same SE as others) and df 24, with t = -2.50 given and p = 0.020. That matches: -5.5/2.1 = -2.619, which is slightly more than -2.50, so perhaps the t was rounded or SE slightly different. But we’ll trust the display.\na2 - a3: difference = 55.5 - 57.0 = -1.5. The SE likely still 2.1, so t.ratio = -1.5/2.1 ≈ -0.714. The p.value would be high (around 0.48). Given the symmetry, if a1-a3 was -7.0 with p=0.002, and a1-a2 was -5.5 with p=0.020, the smallest difference is a2-a3 = -1.5, which should be nonsignificant.\nSo for a2 - a3: t.ratio ≈ -0.71, p.value ≈ 0.48 (not significant).\n\nIs the simple main effect of A at (B=b1, C=c1) significant? The joint F test would combine these three means. Alternatively, seeing that a1 vs a3 is significant (p=0.002) and a1 vs a2 is marginal (0.02, usually considered significant at 0.05 level, but note with 3 pairwise tests one might adjust), it suggests an overall effect. Since at least one pairwise is significant and there are 3 groups, likely yes, the omnibus for A at (b1,c1) is significant. In fact, we could compute \\(F(2,24)\\) by hand: The differences seem sizeable (a1 is quite lower than a3). So yes, it appears significant. To be thorough: the means (50, 55.5, 57) have an obvious upward trend. We would report “At B=b1, C=c1, there is a significant effect of A (F(2,24)=…, p&lt;- 0.05); specifically, a1 is significantly lower than a3 (difference = -7.0, p=0.002) and lower than a2 (difference = -5.5, p=0.02), while a2 and a3 do not differ significantly (p=0.48).”\n8. Interpreting a non-significant simple effect: If joint_tests(emmeans(model, ~ A | B*C)) gives p = 0.8 for the test of A at B=b2, C=c3, that implies no evidence that A has any effect at that specific condition. In other words, the means for different levels of A at B=b2, C=c3 are statistically equal (high p-value means we fail to reject that all A means are equal). We would report: “At B = b2 and C = c3, factor A does not significantly influence the outcome (F(df1, df2)=…, p = 0.80). In that condition, the means for A’s levels were very similar.” This supports the notion that in the (b2,c3) group, changing A doesn’t change the response in any meaningful way. It’s an example of how a factor can be important in one context but not in another, which is the essence of interaction.\n9. Following up a significant two-way interaction (no three-way): If A×B is significant and no triple interaction, we assume the effect is consistent across C (if a third factor exists, but maybe we had only two factors? The question said three-way ANOVA, but found no 3-way, so likely still 3 factors overall). If the design has three factors and only A×B is significant: - First, confirm which two-way: A×B is significant. We then examine simple main effects of one factor at levels of the other (since no 3-way, we can even average over C if C is present, because we didn’t find interactions with C). - For each level of A, see the effect of B (or vice versa). For instance: emmeans(model, ~ B | A) and do pairs() or joint_tests() by A. This yields B’s simple effect at each A. - Or emmeans(model, ~ A | B) to get A’s simple effect at each B. Either approach is valid; typically one chooses the simpler interpretation. If A and B both have &gt;2 levels, you might do both directions to fully describe the interaction. - In R code:\nemmeans(model, pairwise ~ A | B)  # gives A’s pairwise diffs at each B\nemmeans(model, pairwise ~ B | A)  # B’s diffs at each A\n and/or\njoint_tests(emmeans(model, ~ A | B))  # omnibus F for A at each B\n\nWhat to look for: Which simple effects are significant. E.g., maybe at B1, A has no effect; at B2, A has a strong effect. That would mean the interaction was driven by those differences.\nSince no three-way, these patterns should hold regardless of C (we could also confirm no differences across C by doing the same and seeing consistency, but given no 3-way, it’s redundant).\nThen report: “We followed up the A×B interaction with simple effects. For each level of B, we tested the effect of A. Results showed that… (describe which levels differ).” For example, “At B = high, factor A had a significant effect (F(…)=…, p&lt;- …); specifically, A1 differed from A2 (p=…) etc. At B = low, however, A’s effect was not significant (p=…). This indicates the effect of A is present only under the high B condition, driving the A×B interaction.”\n\n10. Extracting cell means and computing a simple effect manually: Using emmeans, to get all cell means for a 3-way, we do:\nemm &lt;- emmeans(model, ~ A * B * C)\nemm\nThis emm object will have \\(a \\times b \\times c\\) rows, each a cell mean with its SE. To get them as a data frame, one can do as.data.frame(emm). Now, to compute the simple effect of A at C=c2 (averaging over B): Essentially we want means of A at C=c2 (collapsing B). We could:\n\nUse emmeans(model, ~ A | C, at = list(C = c2)). But if we want to “manually” compute from cell means:\nFilter emm for C == c2. Now we have means for each combination of A and B at that C.\nTo average over B for each A, we take those means and average them (weighted by equal n if balanced; emmeans by default will already give you the correct marginal mean if you specify emmeans(model, ~ A | C) directly).\nFor manual: if emm_df has columns A, B, C, and response (emmean), do:\nlibrary(dplyr)\ncell_means &lt;- as.data.frame(emmeans(model, ~ A * B * C))\n# filter C = c2\ncm_c2 &lt;- cell_means |&gt; filter(C == \"c2\")\n# Now cm_c2 has all combos of A and B at C=c2 with their means.\n# Compute mean over B for each A:\nsimpleA_c2 &lt;- cm_c2 |&gt; group_by(A) |&gt; summarize(mean_at_c2 = mean(emmean))\nsimpleA_c2\nsimpleA_c2 now contains the marginal means of A at C=c2. If the design is balanced, that’s the simple effect means (if unbalanced, the above equal averaging might not equal the model’s marginal - but emmeans handles that automatically with weighting).\nThen to compare A levels, compute differences: take differences of those means. You could also simulate an F by computing variance between those means and using the error variance from the model (though easier is to use joint_tests as done).\nThe question expects: “extract cell means for all combinations”–answer: use emmeans(model, ~ A * B * C). To compute a simple effect of A at C=c2 (averaging B), either use emmeans(model, ~ A | C, at = list(C=c2)) for direct approach, or manually average the cell means for C=c2 across B.\nEmphasize that emmeans does this heavy lifting easily, so manual is just for concept.\n\n11. Explaining the emmeans warning: The warning \"Results may be misleading due to involvement in interactions\" appears when you request marginal means (or comparisons thereof) of a factor that participates in a significant interaction in the model. For example, if A×B is significant and you ask for emmeans(model, ~ A) (the main effect of A), emmeans warns that the marginal means of A may not make sense because A’s effect varies by B. It doesn’t mean the numbers are wrong; it means interpreting them as “the effect of A” is potentially misleading. In your colleague’s case, they did emmeans(model, pairwise ~ A | B). If there is an A×B or A×something interaction, emmeans will warn. However, emmeans(model, pairwise ~ A | B) is actually the simple effect of A at each B, which is usually a fine thing to do when probing an interaction–so why a warning? Because likely there is a higher-order interaction (maybe A×B×C) that involves A and B. If A×B×C is significant, any marginal of A|B still averages over C, which might be problematic. The safe route is to include all interacting factors in emmeans specs, then use by or | appropriately.\nSo I would explain: The warning appears because A’s effects are not consistent across some other factor in the model (an interaction exists). The solution:\n\nIf probing an interaction, include the interacting factor in the conditioning. For instance, if there’s A×B×C, instead of emmeans(~A|B) (which averages over C and hence can mislead), do emmeans(~A|B*C) or even better emmeans(~A| B* C) which will give A’s effect at each combination of B and C.\nIf you truly want the marginal A|B means (averaging C), you can still get them, but you must be careful in interpretation, acknowledging they are averaged over C which has an interaction.\nIn practice: advise them to do emmeans(model, pairwise ~ A | B * C) if a three-way exists. If only A×B two-way is significant (no third factor), emmeans(~A|B) is fine (that’s exactly the simple effects of A at each B).\nThe key: always condition on all other factors involved in interactions when examining simple effects, so you’re not averaging over an interaction in progress.\nSumming up: “emmeans warns you because A is involved in an interaction, meaning its effect changes across another factor. To properly examine A’s effect, you should look at it within levels of that factor. The code emmeans(model, pairwise ~ A | B) is correct for probing the A×B interaction, and the warning is just a reminder. If there was a C factor also interacting, you’d include C too (e.g., | B * C). Always interpret such results as conditional on the other factor(s).”\n\n12. Determining A×B at each level of C: To see if A×B exists at each C level: - Use emmeans to get A×B means within each C. E.g. emm_AB = emmeans(model, ~ A * B | C). - Then do joint_tests(emm_AB, by=\"C\"), which will test the A×B interaction (as a model term) at each C level. - Alternatively, since A×B in each C is just a two-way ANOVA in that slice, you could split data by C and run an ANOVA each time. But emmeans approach is quick and uses the pooled error. - Code:\n ```r\n emm_AB &lt;- emmeans(model, ~ A * B | C)\n joint_tests(emm_AB, by = \"C\")\n ```\n\n This would output for each C = c1, c2, …:\n\n ```r\n C = c1:\n   model term    df1 df2  F.ratio p.value\n   A:B            (a-1)*(b-1)  residualDF  …   …\n C = c2:\n   model term    df1 df2  F.ratio p.value\n   A:B            …   …   …   …\n ```\n\nIf at C=c1, p &lt;- .05, that means significant A×B interaction when C=c1. If at C=c2, p &gt; .05, no significant A×B when C=c2. That pattern itself is evidence of an A×B×C three-way (one two-way significant, the other not).\nInterpretation: “We found that the A×B interaction is significant at C = c1 (F(df)=…, p=…), but not at C = c2 (p = …). This indicates that the nature of the A×B interaction differs depending on C. In fact, only under c1 do A and B interact significantly.” If both were significant but maybe to different degrees, you’d still note differences in F magnitude.\nIf both are significant, you might then compare whether they are different in magnitude via a contrast of contrasts (which is essentially the three-way test). But the question specifically: if 3-way is significant, they likely differ.\nIn summary, the emmeans code needed is shown above. If we didn’t have joint_tests, one could get simple two-way cells and run contrasts: e.g., contrast(emm_AB, interaction = \"pairwise\", by = \"C\") might directly give differences of differences per C. But joint_tests is straightforward for omnibus.\n\nThese solutions underscore the logical flow: identify interactions, break them down stepwise, use emmeans to get estimates and tests, and interpret contextually. By answering these, one demonstrates not only mechanical usage of R but an understanding of the statistical reasoning behind probing interactions."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html",
    "href": "appendix/r-topics/r-data-frames-basics.html",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Creating data frames is fundamental in R for managing and analyzing tabular data. This tutorial explores:\n\nBase R Functions: data.frame() and as.data.frame().\ntidyverse Functions: tibble::tibble() and dplyr::data_frame().\n\nData frames allow structured data storage, enabling row and column manipulations akin to spreadsheets.\n\n\n\n\nCleaning and structuring raw data.\nAggregating results from computations.\nPreparing data for visualization or modeling.\n\n\n\n\n\nBase R: Straightforward and widely supported.\ntidyverse: Intuitive syntax, compatibility with the pipe operator |&gt;, and enhanced features like printing.\n\n\n\n\n\n\n\n\n# Use pacman to load packages\npacman::p_load(tidyverse)\n\n\n\n\n\n# Create a simple data frame\ndata_base &lt;- data.frame(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\ndata_base\n\n\n  \n\n\n\n\n\n\n\n# Create a tibble\ndata_tidy &lt;- tibble::tibble(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\ndata_tidy\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\ndata.frame() creates data frames from vectors or lists.\n\n\n\n#| echo: fenced\ndata.frame(x, ..., row.names = NULL, check.rows = FALSE, check.names = TRUE)\n\n\n\n\ndata_example &lt;- data.frame(\n    ID = 1:3,\n    Category = c(\"A\", \"B\", \"A\"),\n    Value = c(100, 200, 150)\n)\ndata_example\n\n\n  \n\n\n\n\n\n\n\n\n\ntibble::tibble() constructs modern data frames with automatic column type detection.\n\n\n\ntibble::tibble(...)\n\n\n\n\ndata_tidy_example &lt;- tibble::tibble(\n    ID = 1:3,\n    Category = c(\"A\", \"B\", \"A\"),\n    Value = c(100, 200, 150)\n)\ndata_tidy_example\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nCombine two data frames by a common key.\n\n\n\n\ndf1 &lt;- tibble::tibble(ID = 1:3, Value1 = c(10, 20, 30))\ndf2 &lt;- tibble::tibble(ID = 2:4, Value2 = c(40, 50, 60))\nmerged &lt;- dplyr::left_join(df1, df2, by = \"ID\")\nmerged\n\n\n  \n\n\n\n\n\n\nObserve how NA fills for unmatched rows.\n\n\n\n\n\n\nFilter rows where Value1 &gt; 15 and select specific columns.\n\n\n\n\nfiltered &lt;- merged |&gt;\n    dplyr::filter(Value1 &gt; 15) |&gt;\n    dplyr::select(ID, Value1)\nfiltered\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\ncolnames(data_base) &lt;- c(\"FullName\", \"Years\", \"ExamScore\")\ndata_base\n\n\n  \n\n\n\n\n\n\n\ndata_tidy &lt;- data_tidy |&gt;\n    dplyr::mutate(Grade = ifelse(Score &gt; 80, \"Pass\", \"Fail\"))\ndata_tidy\n\n\n  \n\n\n\n\n\n\n\nGenerate a plot from the data.\n\nggplot2::ggplot(data_tidy, ggplot2::aes(x = Age, y = Score)) +\n    ggplot2::geom_point() +\n    ggplot2::labs(title = \"Age vs. Score\", x = \"Age\", y = \"Score\")\n\n\n\n\n\n\n\n\n\n\n\n\nUse data.table for large data sets.\nPredefine column types for better memory usage.\n\n\n\n\n\n\n\n\nData Mismatch: Ensure column lengths match when creating data frames.\n\n# Error example\n# data.frame(a = 1:3, b = 1:4)\n\nType Conversion Warnings: Avoid unintended type coercion by specifying stringsAsFactors = FALSE.\n\n\n\n\n\nWhy use tibble? tibble provides cleaner, more readable output, and better handling of column names and types.\n\n\n\n\n\n\nUse consistent column names and data types.\nValidate data integrity before further processing.\nLeverage tidyverse pipes for readability and maintainability.\n\n\n\n\n\n\n\nBase R is great for quick setups and legacy code.\ntidyverse excels for scalable, readable, and modern workflows.\n\n\n\n\nExplore advanced techniques like data.table or R6 for complex data operations.\n\n\n\n\n\nBase R Documentation\nTidyverse Guide\nR for Data Science"
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#introduction",
    "href": "appendix/r-topics/r-data-frames-basics.html#introduction",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Creating data frames is fundamental in R for managing and analyzing tabular data. This tutorial explores:\n\nBase R Functions: data.frame() and as.data.frame().\ntidyverse Functions: tibble::tibble() and dplyr::data_frame().\n\nData frames allow structured data storage, enabling row and column manipulations akin to spreadsheets.\n\n\n\n\nCleaning and structuring raw data.\nAggregating results from computations.\nPreparing data for visualization or modeling.\n\n\n\n\n\nBase R: Straightforward and widely supported.\ntidyverse: Intuitive syntax, compatibility with the pipe operator |&gt;, and enhanced features like printing."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#getting-started",
    "href": "appendix/r-topics/r-data-frames-basics.html#getting-started",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "# Use pacman to load packages\npacman::p_load(tidyverse)\n\n\n\n\n\n# Create a simple data frame\ndata_base &lt;- data.frame(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\ndata_base\n\n\n  \n\n\n\n\n\n\n\n# Create a tibble\ndata_tidy &lt;- tibble::tibble(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\ndata_tidy"
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#key-functions-and-features",
    "href": "appendix/r-topics/r-data-frames-basics.html#key-functions-and-features",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "data.frame() creates data frames from vectors or lists.\n\n\n\n#| echo: fenced\ndata.frame(x, ..., row.names = NULL, check.rows = FALSE, check.names = TRUE)\n\n\n\n\ndata_example &lt;- data.frame(\n    ID = 1:3,\n    Category = c(\"A\", \"B\", \"A\"),\n    Value = c(100, 200, 150)\n)\ndata_example\n\n\n  \n\n\n\n\n\n\n\n\n\ntibble::tibble() constructs modern data frames with automatic column type detection.\n\n\n\ntibble::tibble(...)\n\n\n\n\ndata_tidy_example &lt;- tibble::tibble(\n    ID = 1:3,\n    Category = c(\"A\", \"B\", \"A\"),\n    Value = c(100, 200, 150)\n)\ndata_tidy_example"
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#in-depth-examples",
    "href": "appendix/r-topics/r-data-frames-basics.html#in-depth-examples",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Combine two data frames by a common key.\n\n\n\n\ndf1 &lt;- tibble::tibble(ID = 1:3, Value1 = c(10, 20, 30))\ndf2 &lt;- tibble::tibble(ID = 2:4, Value2 = c(40, 50, 60))\nmerged &lt;- dplyr::left_join(df1, df2, by = \"ID\")\nmerged\n\n\n  \n\n\n\n\n\n\nObserve how NA fills for unmatched rows.\n\n\n\n\n\n\nFilter rows where Value1 &gt; 15 and select specific columns.\n\n\n\n\nfiltered &lt;- merged |&gt;\n    dplyr::filter(Value1 &gt; 15) |&gt;\n    dplyr::select(ID, Value1)\nfiltered"
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#advanced-features",
    "href": "appendix/r-topics/r-data-frames-basics.html#advanced-features",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "colnames(data_base) &lt;- c(\"FullName\", \"Years\", \"ExamScore\")\ndata_base\n\n\n  \n\n\n\n\n\n\n\ndata_tidy &lt;- data_tidy |&gt;\n    dplyr::mutate(Grade = ifelse(Score &gt; 80, \"Pass\", \"Fail\"))\ndata_tidy\n\n\n  \n\n\n\n\n\n\n\nGenerate a plot from the data.\n\nggplot2::ggplot(data_tidy, ggplot2::aes(x = Age, y = Score)) +\n    ggplot2::geom_point() +\n    ggplot2::labs(title = \"Age vs. Score\", x = \"Age\", y = \"Score\")\n\n\n\n\n\n\n\n\n\n\n\n\nUse data.table for large data sets.\nPredefine column types for better memory usage."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-data-frames-basics.html#troubleshooting-and-faqs",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Data Mismatch: Ensure column lengths match when creating data frames.\n\n# Error example\n# data.frame(a = 1:3, b = 1:4)\n\nType Conversion Warnings: Avoid unintended type coercion by specifying stringsAsFactors = FALSE.\n\n\n\n\n\nWhy use tibble? tibble provides cleaner, more readable output, and better handling of column names and types."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#best-practices",
    "href": "appendix/r-topics/r-data-frames-basics.html#best-practices",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Use consistent column names and data types.\nValidate data integrity before further processing.\nLeverage tidyverse pipes for readability and maintainability."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#conclusion",
    "href": "appendix/r-topics/r-data-frames-basics.html#conclusion",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Base R is great for quick setups and legacy code.\ntidyverse excels for scalable, readable, and modern workflows.\n\n\n\n\nExplore advanced techniques like data.table or R6 for complex data operations."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#references-and-resources",
    "href": "appendix/r-topics/r-data-frames-basics.html#references-and-resources",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Base R Documentation\nTidyverse Guide\nR for Data Science"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html",
    "href": "appendix/r-topics/r-data-transformation.html",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Data transformation and wrangling involve cleaning, reshaping, and modifying data to prepare it for analysis. This tutorial introduces:\n\nBase R Functions: subset(), transform(), merge().\ntidyverse Functions: dplyr::filter(), dplyr::mutate(), tidyr::pivot_longer(), and others.\n\n\n\n\n\nFiltering rows based on conditions.\nAdding or modifying columns.\nReshaping data between wide and long formats.\nCombining or joining datasets.\n\n\n\n\n\nBase R: Built into R, no additional installations required.\ntidyverse: Consistent syntax and powerful chaining with the pipe operator |&gt;.\n\n\n\n\n\n\n\n\n# Base R requires no additional loading.\n\n# Load tidyverse package using pacman\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\n\n\nDescription: Extract rows based on a condition.\nSyntax:\nsubset(x, subset, select = NULL)\nArguments:\n\nx: A data frame or similar object.\nsubset: Logical expression indicating rows to keep.\nselect: Columns to retain, or NULL to keep all.\n\nExample:\n\ndata(iris)\n# Filter rows where Species is \"setosa\"\nsubset(iris, Species == \"setosa\")\n\n\n  \n\n\n\n\n\n\nDescription: Add or modify columns in a data frame.\nSyntax:\ntransform(data, ...)\nArguments:\n\ndata: A data frame.\n…: Expressions specifying transformations.\n\nExample:\n\n# Add a new column for Sepal Area\niris &lt;- transform(iris, Sepal.Area = Sepal.Length * Sepal.Width)\nhead(iris)\n\n\n  \n\n\n\n\n\n\nDescription: Combine datasets based on common columns (keys).\nSyntax:\nmerge(x, y, by, all)\nArguments:\n\nx, y: Data frames to merge.\nby: Column names to use as keys. Defaults to all common columns.\nall: Logical. TRUE for full outer join; FALSE for inner join.\n\nExample:\n\n# Create example datasets\ndf1 &lt;- data.frame(ID = 1:3, Value = c(10, 20, 30))\ndf2 &lt;- data.frame(ID = 2:4, Score = c(15, 25, 35))\n# Merge on ID\nmerge(df1, df2, by = \"ID\")\n\n\n  \n\n\n\n\n\n\n\n\n\nDescription: Select rows using conditions with dplyr::filter().\nSyntax:\ndplyr::filter(.data, ...)\nArguments:\n\n.data: A data frame or tibble.\n…: Logical conditions. Rows where the condition is TRUE are retained.\n\nExample:\n\n# Filter rows where Species is \"setosa\"\niris |&gt;\n  dplyr::filter(Species == \"setosa\")\n\n\n  \n\n\n\n\n\n\nDescription: Create or modify columns using dplyr::mutate().\nSyntax:\ndplyr::mutate(.data, ...)\nArguments:\n\n.data: A data frame or tibble.\n…: Name-value pairs where names are new or existing column names, and values are expressions.\n\nExample:\n\n# Add Sepal.Area column\niris |&gt;\n  dplyr::mutate(Sepal.Area = Sepal.Length * Sepal.Width)\n\n\n  \n\n\n\n\n\n\nDescription: Convert data between wide and long formats with tidyr::pivot_longer().\nSyntax:\ntidyr::pivot_longer(data, cols, names_to, values_to)\nArguments:\n\ndata: A data frame or tibble.\ncols: Columns to pivot into rows.\nnames_to: Name of the new column containing the names of the pivoted variables.\nvalues_to: Name of the new column containing the values of the pivoted variables.\n\nExample:\n\n# Convert iris to long format\niris_long &lt;- iris |&gt;\n  tidyr::pivot_longer(\n    cols = starts_with(\"Sepal\"),\n    names_to = \"Measurement\",\n    values_to = \"Value\"\n  )\nhead(iris_long)\n\n\n  \n\n\n\n\n\n\nDescription: Combine datasets using dplyr::inner_join().\nSyntax:\ndplyr::inner_join(x, y, by)\nArguments:\n\nx, y: Data frames or tibbles to join.\nby: Column names to join by. Defaults to common column names.\n\nExample:\n\ndf1 &lt;- tibble(ID = 1:3, Value = c(10, 20, 30))\ndf2 &lt;- tibble(ID = 2:4, Score = c(15, 25, 35))\ndf1 |&gt;\n  dplyr::inner_join(df2, by = \"ID\")\n\n\n  \n\n\n\n\n\n\n\n\n\n\nObjective: Prepare mtcars for analysis by filtering rows, creating a new column, and summarizing data.\nSteps:\n\nFilter cars with mpg &gt; 20.\nAdd a column Weight_kg converting wt to kilograms.\nSummarize average MPG by the number of cylinders.\n\nCode:\n\n# Using Base R\nmtcars_filtered &lt;- subset(mtcars, mpg &gt; 20)\nmtcars_filtered &lt;- transform(mtcars_filtered, Weight_kg = wt * 453.6)\naggregate(mpg ~ cyl, data = mtcars_filtered, mean)\n\n\n  \n\n\n# Using tidyverse\nmtcars |&gt;\n  dplyr::filter(mpg &gt; 20) |&gt;\n  dplyr::mutate(Weight_kg = wt * 453.6) |&gt;\n  dplyr::group_by(cyl) |&gt;\n  dplyr::summarize(Average_MPG = mean(mpg))\n\n\n  \n\n\n\n\n\n\n\n\n\nBase R: Use tapply() for custom aggregations. tidyverse: Leverage dplyr::group_by() for grouped operations.\nExample:\n\n# Base R\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\n       4        6        8 \n26.66364 19.74286 15.10000 \n\n# tidyverse\nmtcars |&gt;\n  dplyr::group_by(cyl) |&gt;\n  dplyr::summarize(Average_MPG = mean(mpg))\n\n\n  \n\n\n\n\n\n\n\n\n\n\ntidyr::pivot_longer() errors:\n\nEnsure column names in cols exist.\nUse names_pattern for extracting variable parts.\n\nMerge Mismatches in Base R:\n\nVerify columns in by are consistent between datasets.\n\n\n\n\n\n\n\nUse descriptive column names.\nKeep data in tidy format for easier analysis.\nPrefer tidyverse for a consistent and readable workflow.\n\n\n\n\n\n\n\nBase R: Ideal for simple tasks.\ntidyverse: Superior for complex workflows with a consistent syntax.\n\n\n\n\nExplore additional tidyverse packages like lubridate (dates) or stringr (text manipulation).\n\n\n\n\n\nBase R Documentation\ntidyverse Documentation\nR for Data Science"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#introduction",
    "href": "appendix/r-topics/r-data-transformation.html#introduction",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Data transformation and wrangling involve cleaning, reshaping, and modifying data to prepare it for analysis. This tutorial introduces:\n\nBase R Functions: subset(), transform(), merge().\ntidyverse Functions: dplyr::filter(), dplyr::mutate(), tidyr::pivot_longer(), and others.\n\n\n\n\n\nFiltering rows based on conditions.\nAdding or modifying columns.\nReshaping data between wide and long formats.\nCombining or joining datasets.\n\n\n\n\n\nBase R: Built into R, no additional installations required.\ntidyverse: Consistent syntax and powerful chaining with the pipe operator |&gt;."
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#getting-started",
    "href": "appendix/r-topics/r-data-transformation.html#getting-started",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "# Base R requires no additional loading.\n\n# Load tidyverse package using pacman\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#key-functions-and-features",
    "href": "appendix/r-topics/r-data-transformation.html#key-functions-and-features",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Description: Extract rows based on a condition.\nSyntax:\nsubset(x, subset, select = NULL)\nArguments:\n\nx: A data frame or similar object.\nsubset: Logical expression indicating rows to keep.\nselect: Columns to retain, or NULL to keep all.\n\nExample:\n\ndata(iris)\n# Filter rows where Species is \"setosa\"\nsubset(iris, Species == \"setosa\")\n\n\n  \n\n\n\n\n\n\nDescription: Add or modify columns in a data frame.\nSyntax:\ntransform(data, ...)\nArguments:\n\ndata: A data frame.\n…: Expressions specifying transformations.\n\nExample:\n\n# Add a new column for Sepal Area\niris &lt;- transform(iris, Sepal.Area = Sepal.Length * Sepal.Width)\nhead(iris)\n\n\n  \n\n\n\n\n\n\nDescription: Combine datasets based on common columns (keys).\nSyntax:\nmerge(x, y, by, all)\nArguments:\n\nx, y: Data frames to merge.\nby: Column names to use as keys. Defaults to all common columns.\nall: Logical. TRUE for full outer join; FALSE for inner join.\n\nExample:\n\n# Create example datasets\ndf1 &lt;- data.frame(ID = 1:3, Value = c(10, 20, 30))\ndf2 &lt;- data.frame(ID = 2:4, Score = c(15, 25, 35))\n# Merge on ID\nmerge(df1, df2, by = \"ID\")\n\n\n  \n\n\n\n\n\n\n\n\n\nDescription: Select rows using conditions with dplyr::filter().\nSyntax:\ndplyr::filter(.data, ...)\nArguments:\n\n.data: A data frame or tibble.\n…: Logical conditions. Rows where the condition is TRUE are retained.\n\nExample:\n\n# Filter rows where Species is \"setosa\"\niris |&gt;\n  dplyr::filter(Species == \"setosa\")\n\n\n  \n\n\n\n\n\n\nDescription: Create or modify columns using dplyr::mutate().\nSyntax:\ndplyr::mutate(.data, ...)\nArguments:\n\n.data: A data frame or tibble.\n…: Name-value pairs where names are new or existing column names, and values are expressions.\n\nExample:\n\n# Add Sepal.Area column\niris |&gt;\n  dplyr::mutate(Sepal.Area = Sepal.Length * Sepal.Width)\n\n\n  \n\n\n\n\n\n\nDescription: Convert data between wide and long formats with tidyr::pivot_longer().\nSyntax:\ntidyr::pivot_longer(data, cols, names_to, values_to)\nArguments:\n\ndata: A data frame or tibble.\ncols: Columns to pivot into rows.\nnames_to: Name of the new column containing the names of the pivoted variables.\nvalues_to: Name of the new column containing the values of the pivoted variables.\n\nExample:\n\n# Convert iris to long format\niris_long &lt;- iris |&gt;\n  tidyr::pivot_longer(\n    cols = starts_with(\"Sepal\"),\n    names_to = \"Measurement\",\n    values_to = \"Value\"\n  )\nhead(iris_long)\n\n\n  \n\n\n\n\n\n\nDescription: Combine datasets using dplyr::inner_join().\nSyntax:\ndplyr::inner_join(x, y, by)\nArguments:\n\nx, y: Data frames or tibbles to join.\nby: Column names to join by. Defaults to common column names.\n\nExample:\n\ndf1 &lt;- tibble(ID = 1:3, Value = c(10, 20, 30))\ndf2 &lt;- tibble(ID = 2:4, Score = c(15, 25, 35))\ndf1 |&gt;\n  dplyr::inner_join(df2, by = \"ID\")"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#in-depth-examples",
    "href": "appendix/r-topics/r-data-transformation.html#in-depth-examples",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Objective: Prepare mtcars for analysis by filtering rows, creating a new column, and summarizing data.\nSteps:\n\nFilter cars with mpg &gt; 20.\nAdd a column Weight_kg converting wt to kilograms.\nSummarize average MPG by the number of cylinders.\n\nCode:\n\n# Using Base R\nmtcars_filtered &lt;- subset(mtcars, mpg &gt; 20)\nmtcars_filtered &lt;- transform(mtcars_filtered, Weight_kg = wt * 453.6)\naggregate(mpg ~ cyl, data = mtcars_filtered, mean)\n\n\n  \n\n\n# Using tidyverse\nmtcars |&gt;\n  dplyr::filter(mpg &gt; 20) |&gt;\n  dplyr::mutate(Weight_kg = wt * 453.6) |&gt;\n  dplyr::group_by(cyl) |&gt;\n  dplyr::summarize(Average_MPG = mean(mpg))"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#advanced-features",
    "href": "appendix/r-topics/r-data-transformation.html#advanced-features",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Base R: Use tapply() for custom aggregations. tidyverse: Leverage dplyr::group_by() for grouped operations.\nExample:\n\n# Base R\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\n       4        6        8 \n26.66364 19.74286 15.10000 \n\n# tidyverse\nmtcars |&gt;\n  dplyr::group_by(cyl) |&gt;\n  dplyr::summarize(Average_MPG = mean(mpg))"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-data-transformation.html#troubleshooting-and-faqs",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "tidyr::pivot_longer() errors:\n\nEnsure column names in cols exist.\nUse names_pattern for extracting variable parts.\n\nMerge Mismatches in Base R:\n\nVerify columns in by are consistent between datasets."
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#best-practices",
    "href": "appendix/r-topics/r-data-transformation.html#best-practices",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Use descriptive column names.\nKeep data in tidy format for easier analysis.\nPrefer tidyverse for a consistent and readable workflow."
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#conclusion",
    "href": "appendix/r-topics/r-data-transformation.html#conclusion",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Base R: Ideal for simple tasks.\ntidyverse: Superior for complex workflows with a consistent syntax.\n\n\n\n\nExplore additional tidyverse packages like lubridate (dates) or stringr (text manipulation)."
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#references-and-resources",
    "href": "appendix/r-topics/r-data-transformation.html#references-and-resources",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Base R Documentation\ntidyverse Documentation\nR for Data Science"
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "Hypothesis testing is a cornerstone of statistical analysis, used to evaluate assumptions about population parameters based on sample data. In R, several functions and packages provide extensive tools for hypothesis testing, ranging from basic tests in base R to more specialized tests available in external packages.\n\n\n\nPackages/Functions:\n\nBase R: t.test(), chisq.test(), var.test(), wilcox.test()\nAdditional Packages: stats::ks.test(), MASS::leveneTest()\n\nPurpose:\n\nTo evaluate assumptions or hypotheses about data distributions and relationships.\n\nCommon Functions:\n\nt.test() for t-tests.\nchisq.test() for Chi-square tests.\nks.test() for Kolmogorov-Smirnov tests.\nwilcox.test() for non-parametric Wilcoxon tests.\n\n\n\n\n\n\nComparing means between two groups.\nAssessing the goodness-of-fit for categorical data.\nTesting for equal variances across groups.\nNon-parametric comparisons for skewed or ordinal data.\n\n\n\n\n\nEasy to implement with a range of built-in functions.\nHighly extensible for complex scenarios.\nSeamless integration with visualization and other analytical tools.\n\n\n\n\n\n\n\nBase R provides most hypothesis testing tools. Additional packages like MASS can be used for specialized tests.\n\nlibrary(MASS)\n\n\n\n\n\n\nTesting whether the mean of a sample is equal to a specific value.\n\nset.seed(42)\nsample_data &lt;- rnorm(30, mean = 5, sd = 2)\n\nt.test(sample_data, mu = 5)\n\n\n    One Sample t-test\n\ndata:  sample_data\nt = 0.29933, df = 29, p-value = 0.7668\nalternative hypothesis: true mean is not equal to 5\n95 percent confidence interval:\n 4.199903 6.074444\nsample estimates:\nmean of x \n 5.137174 \n\n\n\n\n\n\n\n\n\n\n\n\nPerforms one-sample, two-sample, and paired t-tests to compare means.\n\n\n\nt.test(x, y = NULL, alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95)\n\nx, y: Numeric vectors of data values.\nalternative: Type of hypothesis (“two.sided”, “greater”, “less”).\nmu: Hypothesized mean (default is 0).\npaired: Whether the test is paired (default is FALSE).\nvar.equal: Assume equal variances (default is FALSE).\n\n\n\n\n\ngroup1 &lt;- rnorm(20, mean = 5)\ngroup2 &lt;- rnorm(20, mean = 6)\n\nt.test(group1, group2, alternative = \"two.sided\", var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  group1 and group2\nt = -4.6894, df = 38, p-value = 3.486e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.1057327 -0.8358574\nsample estimates:\nmean of x mean of y \n 4.807940  6.278735 \n\n\n\n\n\n\n\n\nTests for independence or goodness-of-fit in categorical data.\n\n\n\nchisq.test(x, y = NULL, correct = TRUE, simulate.p.value = FALSE, rescale.p = FALSE)\n\n\n\n\nobserved &lt;- matrix(c(30, 20, 50, 40), ncol = 2)\n\nchisq.test(observed)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  observed\nX-squared = 0.10954, df = 1, p-value = 0.7407\n\n\n\n\n\n\n\n\nPerforms the Wilcoxon rank-sum test (non-parametric alternative to the t-test).\n\n\n\nwilcox.test(x, y = NULL, alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, exact = NULL)\n\n\n\n\nx &lt;- rnorm(30)\ny &lt;- rnorm(30, mean = 1)\n\nwilcox.test(x, y, alternative = \"less\")\n\n\n    Wilcoxon rank sum exact test\n\ndata:  x and y\nW = 271, p-value = 0.003824\nalternative hypothesis: true location shift is less than 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluate the effect of a treatment by comparing preand post-treatment scores.\n\n\n\n\nPrepare the data.\nApply the paired t-test.\nVisualize the differences.\n\n\npre &lt;- c(12, 14, 11, 10, 13, 15)\npost &lt;- c(14, 16, 12, 11, 15, 17)\n\nt.test(pre, post, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pre and post\nt = -7.9057, df = 5, p-value = 0.0005211\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.208593 -1.124740\nsample estimates:\nmean difference \n      -1.666667 \n\n\n\n\n\nInterpret the p-value to determine if there is a significant difference between preand post-treatment scores.\n\n\n\n\n\n\n\n\n\nCheck if a six-sided die is fair.\n\n\n\n\nCollect observed frequencies of each side.\nSpecify expected frequencies.\nApply the Chi-square goodness-of-fit test.\n\n\nobserved &lt;- c(12, 15, 14, 13, 16, 10)\nexpected &lt;- rep(1/6, 6) * sum(observed)\n\nchisq.test(observed, p = expected / sum(expected))\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 1.75, df = 5, p-value = 0.8825\n\n\n\n\n\nEvaluate the p-value to determine if the die is fair.\n\n\n\n\n\n\n\n\n\nSet custom confidence levels in tests:\n\nt.test(group1, group2, conf.level = 0.99)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = -4.6894, df = 37.995, p-value = 3.487e-05\nalternative hypothesis: true difference in means is not equal to 0\n99 percent confidence interval:\n -2.3212628 -0.6203274\nsample estimates:\nmean of x mean of y \n 4.807940  6.278735 \n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\n\nmtcars |&gt;\n  group_by(cyl) |&gt;\n  summarise(t_result = list(t.test(mpg ~ as.factor(am))$p.value))\n\n\n  \n\n\n\n\n\n\n\n\n\nUse vectorized operations or summary statistics for efficiency:\n\nlarge_data &lt;- rnorm(1e6)\nt.test(large_data, mu = 0)\n\n\n    One Sample t-test\n\ndata:  large_data\nt = 0.59109, df = 1e+06, p-value = 0.5545\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.001370263  0.002553645\nsample estimates:\n   mean of x \n0.0005916909 \n\n\n\n\n\n\n\n\n\n\nError: Data must be numeric\n\nEnsure data is numeric by using as.numeric().\n\nWarning: p-value approximation\n\nIncrease the sample size or use simulate.p.value = TRUE.\n\n\n\n\n\n\nWhat is the minimum sample size for a t-test?\n\nAt least 5 observations per group are recommended.\n\nCan I test for normality before t-tests?\n\nUse shapiro.test() to check normality.\n\n\n\n\n\n\n\nAlways visualize your data before performing tests.\nCheck assumptions (e.g., normality for t-tests, independence for Chi-square tests).\nUse appropriate alternatives (e.g., Wilcoxon test for non-normal data).\n\n\n\n\nHypothesis testing in R is a versatile and powerful tool for statistical analysis. From basic t-tests to advanced non-parametric methods, R provides comprehensive support for validating assumptions and making data-driven decisions.\n\n\n\n\nR Documentation\nR Graph Gallery\nR Hypothesis Testing Guide"
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html#introduction",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html#introduction",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "Hypothesis testing is a cornerstone of statistical analysis, used to evaluate assumptions about population parameters based on sample data. In R, several functions and packages provide extensive tools for hypothesis testing, ranging from basic tests in base R to more specialized tests available in external packages.\n\n\n\nPackages/Functions:\n\nBase R: t.test(), chisq.test(), var.test(), wilcox.test()\nAdditional Packages: stats::ks.test(), MASS::leveneTest()\n\nPurpose:\n\nTo evaluate assumptions or hypotheses about data distributions and relationships.\n\nCommon Functions:\n\nt.test() for t-tests.\nchisq.test() for Chi-square tests.\nks.test() for Kolmogorov-Smirnov tests.\nwilcox.test() for non-parametric Wilcoxon tests.\n\n\n\n\n\n\nComparing means between two groups.\nAssessing the goodness-of-fit for categorical data.\nTesting for equal variances across groups.\nNon-parametric comparisons for skewed or ordinal data.\n\n\n\n\n\nEasy to implement with a range of built-in functions.\nHighly extensible for complex scenarios.\nSeamless integration with visualization and other analytical tools."
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html#getting-started",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html#getting-started",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "Base R provides most hypothesis testing tools. Additional packages like MASS can be used for specialized tests.\n\nlibrary(MASS)\n\n\n\n\n\n\nTesting whether the mean of a sample is equal to a specific value.\n\nset.seed(42)\nsample_data &lt;- rnorm(30, mean = 5, sd = 2)\n\nt.test(sample_data, mu = 5)\n\n\n    One Sample t-test\n\ndata:  sample_data\nt = 0.29933, df = 29, p-value = 0.7668\nalternative hypothesis: true mean is not equal to 5\n95 percent confidence interval:\n 4.199903 6.074444\nsample estimates:\nmean of x \n 5.137174"
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html#key-functions-and-features",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html#key-functions-and-features",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "Performs one-sample, two-sample, and paired t-tests to compare means.\n\n\n\nt.test(x, y = NULL, alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95)\n\nx, y: Numeric vectors of data values.\nalternative: Type of hypothesis (“two.sided”, “greater”, “less”).\nmu: Hypothesized mean (default is 0).\npaired: Whether the test is paired (default is FALSE).\nvar.equal: Assume equal variances (default is FALSE).\n\n\n\n\n\ngroup1 &lt;- rnorm(20, mean = 5)\ngroup2 &lt;- rnorm(20, mean = 6)\n\nt.test(group1, group2, alternative = \"two.sided\", var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  group1 and group2\nt = -4.6894, df = 38, p-value = 3.486e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.1057327 -0.8358574\nsample estimates:\nmean of x mean of y \n 4.807940  6.278735 \n\n\n\n\n\n\n\n\nTests for independence or goodness-of-fit in categorical data.\n\n\n\nchisq.test(x, y = NULL, correct = TRUE, simulate.p.value = FALSE, rescale.p = FALSE)\n\n\n\n\nobserved &lt;- matrix(c(30, 20, 50, 40), ncol = 2)\n\nchisq.test(observed)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  observed\nX-squared = 0.10954, df = 1, p-value = 0.7407\n\n\n\n\n\n\n\n\nPerforms the Wilcoxon rank-sum test (non-parametric alternative to the t-test).\n\n\n\nwilcox.test(x, y = NULL, alternative = c(\"two.sided\", \"less\", \"greater\"), mu = 0, paired = FALSE, exact = NULL)\n\n\n\n\nx &lt;- rnorm(30)\ny &lt;- rnorm(30, mean = 1)\n\nwilcox.test(x, y, alternative = \"less\")\n\n\n    Wilcoxon rank sum exact test\n\ndata:  x and y\nW = 271, p-value = 0.003824\nalternative hypothesis: true location shift is less than 0"
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html#in-depth-examples",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html#in-depth-examples",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "Evaluate the effect of a treatment by comparing preand post-treatment scores.\n\n\n\n\nPrepare the data.\nApply the paired t-test.\nVisualize the differences.\n\n\npre &lt;- c(12, 14, 11, 10, 13, 15)\npost &lt;- c(14, 16, 12, 11, 15, 17)\n\nt.test(pre, post, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pre and post\nt = -7.9057, df = 5, p-value = 0.0005211\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.208593 -1.124740\nsample estimates:\nmean difference \n      -1.666667 \n\n\n\n\n\nInterpret the p-value to determine if there is a significant difference between preand post-treatment scores.\n\n\n\n\n\n\n\n\n\nCheck if a six-sided die is fair.\n\n\n\n\nCollect observed frequencies of each side.\nSpecify expected frequencies.\nApply the Chi-square goodness-of-fit test.\n\n\nobserved &lt;- c(12, 15, 14, 13, 16, 10)\nexpected &lt;- rep(1/6, 6) * sum(observed)\n\nchisq.test(observed, p = expected / sum(expected))\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 1.75, df = 5, p-value = 0.8825\n\n\n\n\n\nEvaluate the p-value to determine if the die is fair."
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html#advanced-features",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html#advanced-features",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "Set custom confidence levels in tests:\n\nt.test(group1, group2, conf.level = 0.99)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = -4.6894, df = 37.995, p-value = 3.487e-05\nalternative hypothesis: true difference in means is not equal to 0\n99 percent confidence interval:\n -2.3212628 -0.6203274\nsample estimates:\nmean of x mean of y \n 4.807940  6.278735 \n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\n\nmtcars |&gt;\n  group_by(cyl) |&gt;\n  summarise(t_result = list(t.test(mpg ~ as.factor(am))$p.value))\n\n\n  \n\n\n\n\n\n\n\n\n\nUse vectorized operations or summary statistics for efficiency:\n\nlarge_data &lt;- rnorm(1e6)\nt.test(large_data, mu = 0)\n\n\n    One Sample t-test\n\ndata:  large_data\nt = 0.59109, df = 1e+06, p-value = 0.5545\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.001370263  0.002553645\nsample estimates:\n   mean of x \n0.0005916909"
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html#troubleshooting-and-faqs",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "Error: Data must be numeric\n\nEnsure data is numeric by using as.numeric().\n\nWarning: p-value approximation\n\nIncrease the sample size or use simulate.p.value = TRUE.\n\n\n\n\n\n\nWhat is the minimum sample size for a t-test?\n\nAt least 5 observations per group are recommended.\n\nCan I test for normality before t-tests?\n\nUse shapiro.test() to check normality."
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html#best-practices",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html#best-practices",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "Always visualize your data before performing tests.\nCheck assumptions (e.g., normality for t-tests, independence for Chi-square tests).\nUse appropriate alternatives (e.g., Wilcoxon test for non-normal data)."
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html#conclusion",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html#conclusion",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "Hypothesis testing in R is a versatile and powerful tool for statistical analysis. From basic t-tests to advanced non-parametric methods, R provides comprehensive support for validating assumptions and making data-driven decisions."
  },
  {
    "objectID": "appendix/r-topics/r-hypothesis-testing-basics.html#references-and-resources",
    "href": "appendix/r-topics/r-hypothesis-testing-basics.html#references-and-resources",
    "title": "Hypothesis Testing Basics",
    "section": "",
    "text": "R Documentation\nR Graph Gallery\nR Hypothesis Testing Guide"
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html",
    "href": "appendix/r-topics/r-quarto-table-figure.html",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "",
    "text": "Quarto is a publishing system that integrates R code, text, and output (figures, tables) into dynamic documents. This tutorial focuses on creating figures/tables with captions, cross-referencing them, and generating reproducible reports.\nKey Features:\n\nAdd captions using #| fig-cap or #| tbl-cap in code chunks.\nCross-reference figures/tables with @label syntax.\nCustomize outputs using ggplot2, knitr, kableExtra, and gt.\n\n\n\n\n\nAcademic papers with numbered figures/tables.\nTechnical reports with automated cross-references.\nDashboards integrating dynamic visualizations and summaries.\n\n\n\n\n\nAutomation: Update figures/tables and references dynamically.\nConsistency: Ensure captions and labels follow document structure.\nFlexibility: Output to HTML, PDF, Word, or slides."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html#introduction",
    "href": "appendix/r-topics/r-quarto-table-figure.html#introduction",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "",
    "text": "Quarto is a publishing system that integrates R code, text, and output (figures, tables) into dynamic documents. This tutorial focuses on creating figures/tables with captions, cross-referencing them, and generating reproducible reports.\nKey Features:\n\nAdd captions using #| fig-cap or #| tbl-cap in code chunks.\nCross-reference figures/tables with @label syntax.\nCustomize outputs using ggplot2, knitr, kableExtra, and gt.\n\n\n\n\n\nAcademic papers with numbered figures/tables.\nTechnical reports with automated cross-references.\nDashboards integrating dynamic visualizations and summaries.\n\n\n\n\n\nAutomation: Update figures/tables and references dynamically.\nConsistency: Ensure captions and labels follow document structure.\nFlexibility: Output to HTML, PDF, Word, or slides."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html#getting-started",
    "href": "appendix/r-topics/r-quarto-table-figure.html#getting-started",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\nLoading the Package\n\n# Load required packages\npacman::p_load(ggplot2, dplyr, kableExtra, gt)\nggplot2::theme_set(theme_minimal())\n\n\n\nBasic Usage\nFigure with Caption:\n\nggplot2::ggplot(mtcars, aes(x = wt, y = mpg)) +\n    geom_point()\n\n\n\n\n\n\n\nFigure 1: Scatterplot of car weight vs. MPG.\n\n\n\n\n\nReference the figure in text: See Figure @fig-mpg.\nTable with Caption:\n\nmtcars |&gt;\n    group_by(cyl) |&gt;\n    summarize(avg_mpg = mean(mpg)) |&gt;\n    knitr::kable()\n\n\n\n\n\n\n\ncyl\navg_mpg\n\n\n\n\n4\n26.66364\n\n\n6\n19.74286\n\n\n8\n15.10000\n\n\n\n\n\n\nTable 1: Summary of MPG by cylinders.\n\n\n\n\nReference the table: Table @tbl-summary shows… produces “Table 1 shows…”."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html#key-functions-and-features",
    "href": "appendix/r-topics/r-quarto-table-figure.html#key-functions-and-features",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "3. Key Functions and Features",
    "text": "3. Key Functions and Features\n\nQuarto Chunk Options\nDescription: Control figure/table captions, labels, and layout.\nSyntax:\n#| label: fig-example  \n#| fig-cap: \"Caption text\"  \n#| tbl-cap: \"Table caption\"  \n#| layout: \"l-body-outset\"  \nExample:\n\n```{r}\n#| label: fig-density\n#| fig-cap: \"Density plot of MPG.\"\n\nggplot(mtcars, aes(mpg)) +\n    geom_density(fill = \"skyblue\")\n```\n\n\n\n\n\n\n\nFigure 2: Density plot of MPG.\n\n\n\n\n\nRefer to the figure: Figure @fig-density shows… produces “Figure 2 shows…”.\n\n\nkableExtra::kbl()\nDescription: Create customizable tables with captions.\nSyntax:\ndata |&gt;  \n  kableExtra::kbl(  \n    caption = \"Table title\",  \n    align = \"c\",  \n    format = \"html\"  \n  ) |&gt;  \n  kableExtra::kable_styling()  \nExample:\n\nmtcars |&gt;\n    head(3) |&gt;\n    kableExtra::kbl(caption = \"Sample Cars\") |&gt;\n    kableExtra::kable_styling(full_width = FALSE)\n\n\n\n\n\nSample Cars\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n\n\n\n\n\n\nTable 2: Styled table with kableExtra."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html#in-depth-examples",
    "href": "appendix/r-topics/r-quarto-table-figure.html#in-depth-examples",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "4. In-Depth Examples",
    "text": "4. In-Depth Examples\n\nExample 1: Dynamic Figure Captions\nObjective: Generate a figure caption with variables.\nSteps:\n\nggplot(mtcars, aes(mpg)) +\n    geom_histogram(bins = 10, fill = \"steelblue\")\n\n\n\n\n\n\n\nFigure 3: MPG distribution for {{nrow(mtcars)}} cars.\n\n\n\n\n\nOutput: Caption dynamically updates if data changes.\n\n\nExample 2: Multi-Column Table\nObjective: Create a summary table grouped by cylinders.\nSteps:\n\nmtcars |&gt;\n    group_by(cyl) |&gt;\n    summarize(\n        Avg_MPG = mean(mpg),\n        SD_MPG = sd(mpg)\n    ) |&gt;\n    gt::gt() |&gt;\n    gt::tab_header(title = \"Cylinder Performance\")\n\n\n\n\n\n\n\n\n\n\nCylinder Performance\n\n\ncyl\nAvg_MPG\nSD_MPG\n\n\n\n\n4\n26.66364\n4.509828\n\n\n6\n19.74286\n1.453567\n\n\n8\n15.10000\n2.560048\n\n\n\n\n\n\n\n\nTable 3: Detailed summary by cylinders.\n\n\n\n\nOutput: Professionally formatted table with grouped data."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html#advanced-features",
    "href": "appendix/r-topics/r-quarto-table-figure.html#advanced-features",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "5. Advanced Features",
    "text": "5. Advanced Features\n\nCustomization\nConditional Table Formatting:\n\nmtcars |&gt;\n    head() |&gt;\n    mutate(\n        high_mpg = ifelse(mpg &gt; 20, \"green\", \"red\")\n    ) |&gt;\n    kableExtra::kbl() |&gt;\n    kableExtra::column_spec(6, background = mtcars$high_mpg)\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\nhigh_mpg\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\ngreen\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\ngreen\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\ngreen\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\ngreen\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\nred\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\nred\n\n\n\n\n\n\n\n\nTable 4: Table with conditional formatting.\n\n\n\n\nInteractive Figures with Plotly:\n\nplotly::ggplotly(\n    ggplot(mtcars, aes(wt, mpg)) +\n        geom_point()\n)\n\n\n\n\n\n\n\nFigure 4: Interactive MPG vs. Weight plot.\n\n\n\n\n\n\nIntegration\nCombine with dplyr:\n\nmtcars |&gt;\n    filter(cyl == 6) |&gt;\n    gt::gt()\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n\n\n\n\n\n\nTable 5: Filtered data table.\n\n\n\n\n\n\nOptimization\nCaching Large Datasets:\n#| cache: true"
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-quarto-table-figure.html#troubleshooting-and-faqs",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "6. Troubleshooting and FAQs",
    "text": "6. Troubleshooting and FAQs\n\nCommon Issues\n\nMissing Captions: Ensure #| fig-cap or #| tbl-cap is placed inside the code chunk.\nBroken Cross-References: Verify that labels match exactly (e.g., @fig-label vs. fig-label).\n\n\n\nFAQs\nQ: How to reference figures in PDF output? A: Use the same @label syntax; Quarto handles format-specific numbering.\nQ: Can I use LaTeX in captions? A: Yes, for PDF output: #| fig-cap: \"\\\\textbf{Bold} caption text\"."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html#best-practices",
    "href": "appendix/r-topics/r-quarto-table-figure.html#best-practices",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "7. Best Practices",
    "text": "7. Best Practices\n\nUse descriptive labels (e.g., fig-mpg-distribution instead of fig1).\nStore reusable components (e.g., themes, table styles) in separate scripts.\nPrefer gt for complex tables and kableExtra for quick HTML formatting."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html#conclusion",
    "href": "appendix/r-topics/r-quarto-table-figure.html#conclusion",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "8. Conclusion",
    "text": "8. Conclusion\n\nSummary\nQuarto streamlines figure/table creation, captioning, and cross-referencing. Key tools include:\n\nChunk options (#| label, #| fig-cap).\nggplot2 for figures, gt/kableExtra for tables.\n\n\n\nNext Steps\nExplore Quarto’s cross-referencing syntax for equations, sections, and theorems."
  },
  {
    "objectID": "appendix/r-topics/r-quarto-table-figure.html#references-and-resources",
    "href": "appendix/r-topics/r-quarto-table-figure.html#references-and-resources",
    "title": "Figures, Tables, Captions, and Referencing in Quarto: A Comprehensive Tutorial",
    "section": "9. References and Resources",
    "text": "9. References and Resources\n\nQuarto Documentation: https://quarto.org\nggplot2 Cheat Sheet: https://ggplot2.tidyverse.org\nCommunity Support: RStudio Community, Stack Overflow (tag: quarto)."
  },
  {
    "objectID": "appendix/stat-topics/games-howell-test.html",
    "href": "appendix/stat-topics/games-howell-test.html",
    "title": "Tutorial on the Games-Howell Test Using PMCMRplus",
    "section": "",
    "text": "The Games-Howell test is a post-hoc statistical procedure used to perform all-pairs comparisons between group means after a significant ANOVA result. It is particularly useful when the assumption of equal variances across groups is violated. Unlike the Tukey HSD test, which assumes homogeneity of variances, the Games-Howell test uses Welch’s degrees of freedom to adjust for unequal variances, making it robust for such scenarios.\nThis tutorial will guide you through using the gamesHowellTest function from the PMCMRplus package in R."
  },
  {
    "objectID": "appendix/stat-topics/games-howell-test.html#introduction-to-the-games-howell-test",
    "href": "appendix/stat-topics/games-howell-test.html#introduction-to-the-games-howell-test",
    "title": "Tutorial on the Games-Howell Test Using PMCMRplus",
    "section": "",
    "text": "The Games-Howell test is a post-hoc statistical procedure used to perform all-pairs comparisons between group means after a significant ANOVA result. It is particularly useful when the assumption of equal variances across groups is violated. Unlike the Tukey HSD test, which assumes homogeneity of variances, the Games-Howell test uses Welch’s degrees of freedom to adjust for unequal variances, making it robust for such scenarios.\nThis tutorial will guide you through using the gamesHowellTest function from the PMCMRplus package in R."
  },
  {
    "objectID": "appendix/stat-topics/games-howell-test.html#installation",
    "href": "appendix/stat-topics/games-howell-test.html#installation",
    "title": "Tutorial on the Games-Howell Test Using PMCMRplus",
    "section": "Installation",
    "text": "Installation\nFirst, install and load the PMCMRplus package if not already installed:\n\npacman::p_load(\"PMCMRplus\")"
  },
  {
    "objectID": "appendix/stat-topics/games-howell-test.html#function-syntax",
    "href": "appendix/stat-topics/games-howell-test.html#function-syntax",
    "title": "Tutorial on the Games-Howell Test Using PMCMRplus",
    "section": "Function Syntax",
    "text": "Function Syntax\nThe gamesHowellTest function has three primary methods:\n\nDefault method: gamesHowellTest(x, g, …)\n\nx: Numeric vector of data or a list of numeric vectors.\ng: Grouping vector/factor (ignored if x is a list).\n\nFormula method: gamesHowellTest(formula, data, …)\n\nformula: Formula of the form response ~ group.\ndata: Data frame containing variables in the formula.\n\naov method: gamesHowellTest(x, …)\n\nx: A fitted model object of class aov."
  },
  {
    "objectID": "appendix/stat-topics/games-howell-test.html#step-by-step-examples",
    "href": "appendix/stat-topics/games-howell-test.html#step-by-step-examples",
    "title": "Tutorial on the Games-Howell Test Using PMCMRplus",
    "section": "Step-by-Step Examples",
    "text": "Step-by-Step Examples\n\nExample 1: Using an aov Object\nWe’ll use the built-in chickwts dataset to compare chicken weights across feed types.\n\nStep 1: Check Assumptions\n\nNormality: Shapiro-Wilk test on residuals.\nEqual Variances: Bartlett test.\n\n\n# Fit ANOVA model\nfit &lt;- aov(weight ~ feed, data = chickwts)\n\n# Check normality of residuals\nshapiro.test(residuals(fit))  # p &gt; 0.05 → normality assumed\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(fit)\nW = 0.98616, p-value = 0.6272\n\n# Check homogeneity of variances\nbartlett.test(weight ~ feed, data = chickwts)  # small p → unequal variances\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by feed\nBartlett's K-squared = 3.2597, df = 5, p-value = 0.66\n\n\n\n\nStep 2: Run ANOVA\n\nanova(fit) \n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nfeed\n5\n231129.2\n46225.832\n15.3648\n0\n\n\nResiduals\n65\n195556.0\n3008.554\nNA\nNA\n\n\n\n\n\n\n\n\nStep 3: Perform Games-Howell Test\n\nres &lt;- gamesHowellTest(fit)\nsummary(res)  # Pairwise comparisons with p-values\n\n                           q value   Pr(&gt;|q|)    \nhorsebean - casein == 0    -10.384 9.4359e-06 ***\nlinseed - casein == 0       -6.192 0.00310158  **\nmeatmeal - casein == 0      -2.445 0.52927013    \nsoybean - casein == 0       -4.631 0.03604278   *\nsunflower - casein == 0      0.323 0.99990004    \nlinseed - horsebean == 0     4.267 0.06493843   .\nmeatmeal - horsebean == 0    7.155 0.00123741  **\nsoybean - horsebean == 0     6.441 0.00190148  **\nsunflower - horsebean == 0  12.791 2.3072e-07 ***\nmeatmeal - linseed == 0      3.329 0.22093066    \nsoybean - linseed == 0       1.873 0.76889970    \nsunflower - linseed == 0     7.547 0.00030424 ***\nsoybean - meatmeal == 0     -1.771 0.80599848    \nsunflower - meatmeal == 0    3.050 0.30300311    \nsunflower - soybean == 0     5.775 0.00508811  **\n\n\n\n\nStep 4: Interpret Results\n\nfeed: A significant effect of feed type on weight is observed. The Games-Howell test suggests that the means of weight are different between the different feed types.\nResiduals: The remaining variance is due to random error.\n\n\n\n\nExample 2: Using the Formula Method\nSimulate data with three groups (A, B, C) with unequal variances.\n\n# Create data\nset.seed(123)\ngroup &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each = 20))\nvalues &lt;- c(rnorm(20, mean=10, sd=2),  # Group A\n            rnorm(20, mean=15, sd=4),  # Group B\n            rnorm(20, mean=12, sd=3)) # Group C\n\n# Perform Games-Howell Test\nresult &lt;- gamesHowellTest(values ~ group)\nsummary(result)\n\n           q value   Pr(&gt;|q|)    \nB - A == 0   7.416 3.2056e-05 ***\nC - A == 0   3.713   0.033796   *\nC - B == 0  -3.567   0.041576   *"
  },
  {
    "objectID": "appendix/stat-topics/games-howell-test.html#key-output-interpretation",
    "href": "appendix/stat-topics/games-howell-test.html#key-output-interpretation",
    "title": "Tutorial on the Games-Howell Test Using PMCMRplus",
    "section": "Key Output Interpretation",
    "text": "Key Output Interpretation\n\nstatistic: Test statistic for each pair (t-value).\np.value: Adjusted p-values using the Tukey distribution.\nValues are arranged in a lower-triangle matrix. A p-value &lt;- 0.05 indicates a significant difference between groups."
  },
  {
    "objectID": "appendix/stat-topics/games-howell-test.html#assumptions-and-considerations",
    "href": "appendix/stat-topics/games-howell-test.html#assumptions-and-considerations",
    "title": "Tutorial on the Games-Howell Test Using PMCMRplus",
    "section": "Assumptions and Considerations",
    "text": "Assumptions and Considerations\n\nNormality: The test assumes normality. Check using Shapiro-Wilk or QQ-plots.\nUnequal Variances: Use Games-Howell when variances are unequal (check via Bartlett/Levene test).\nPost-Hoc Use: Conduct only after a significant ANOVA result."
  },
  {
    "objectID": "appendix/stat-topics/games-howell-test.html#when-to-use-games-howell-vs.-other-tests",
    "href": "appendix/stat-topics/games-howell-test.html#when-to-use-games-howell-vs.-other-tests",
    "title": "Tutorial on the Games-Howell Test Using PMCMRplus",
    "section": "When to Use Games-Howell vs. Other Tests",
    "text": "When to Use Games-Howell vs. Other Tests\n\nTukey HSD: Use when variances are equal.\nDunnett’s Test: Compare all groups against a control.\nBonferroni: Conservative adjustment for any pairwise test."
  },
  {
    "objectID": "appendix/stat-topics/games-howell-test.html#conclusion",
    "href": "appendix/stat-topics/games-howell-test.html#conclusion",
    "title": "Tutorial on the Games-Howell Test Using PMCMRplus",
    "section": "Conclusion",
    "text": "Conclusion\nThe gamesHowellTest is ideal for comparing group means when variances are unequal. Always validate assumptions before applying the test. Use the formula or aov method for cleaner syntax with data frames or model objects."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html",
    "href": "appendix/stat-topics/standardized-range-distribution.html",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "",
    "text": "Below is a conceptual overview and step-by-step tutorial on the Studentized Range distribution. This distribution arises frequently in multiple comparisons procedures in statistics, particularly in Tukey’s Honestly Significant Difference (HSD) test. We’ll cover the definition, context, properties, and practical computation of the Studentized Range distribution."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html#what-is-the-studentized-range-distribution",
    "href": "appendix/stat-topics/standardized-range-distribution.html#what-is-the-studentized-range-distribution",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "1. What Is the Studentized Range Distribution?",
    "text": "1. What Is the Studentized Range Distribution?\nThe Studentized Range distribution (often denoted \\(q\\)) describes the distribution of the range of a sample (i.e., the difference between the sample’s maximum and minimum) standardized by the sample’s estimated standard deviation.\nFormally, suppose we have:\n\nA sample of \\(k\\) observations, all from the same normal population with unknown variance \\(\\sigma^2\\).\nAn independent estimate of the variance based on \\(\\nu\\) (sometimes denoted \\(df\\)) degrees of freedom.\n\nDefine:\n\\[\n\\text{Range} = \\max(X_1, \\dots, X_k) - \\min(X_1, \\dots, X_k).\n\\]\nThen the statistic\n\\[\nq = \\frac{\\max(X_1, \\dots, X_k) - \\min(X_1, \\dots, X_k)}{s},\n\\]\nwhere \\(s\\) is the estimated standard deviation (based on \\(\\nu\\) degrees of freedom), follows a Studentized Range distribution with parameters \\(k\\) (the number of means or groups being compared) and \\(\\nu\\) (degrees of freedom).\n\nWhy “Studentized”?\n\n“Studentized” means it has been scaled by an estimate of the standard deviation—analogous to how a \\(t\\)-distribution arises by dividing a (normal) mean difference by an estimated standard error.\nThe Studentized Range is a generalization of the \\(t\\)-distribution for comparing more than two means."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html#context-tukeys-hsd-test",
    "href": "appendix/stat-topics/standardized-range-distribution.html#context-tukeys-hsd-test",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "2. Context: Tukey’s HSD Test",
    "text": "2. Context: Tukey’s HSD Test\nThe most common use case for the Studentized Range distribution is in Tukey’s Honestly Significant Difference (HSD) procedure, which is a multiple-comparison test for determining which means in a set of group means differ significantly:\n\nStart with an ANOVA for \\(k\\) groups (each with sample size \\(n_i\\)), total number of degrees of freedom in error \\(\\nu = N - k\\) (if each group has the same sample size \\(n\\), then \\(\\nu = k(n - 1)\\)).\nAfter finding that the overall ANOVA is significant, we want to compare each pair of group means.\nThe test statistic for comparing any two group means is\n\n\\[\nq = \\frac{\\bar{X}_i - \\bar{X}_j}{\\sqrt{\\frac{MSE}{n}}}\n\\]\nwhere \\(MSE\\) is the mean square error from the ANOVA, and \\(n\\) is the per-group sample size (if balanced).\n\nWe compare \\(q\\) to critical values from the Studentized Range distribution with parameters \\(k\\) and \\(\\nu\\).\n\nHence, the Studentized Range \\(q\\) helps control the family-wise error rate across multiple pairwise comparisons."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html#defining-the-distribution-more-rigorously",
    "href": "appendix/stat-topics/standardized-range-distribution.html#defining-the-distribution-more-rigorously",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "3. Defining the Distribution More Rigorously",
    "text": "3. Defining the Distribution More Rigorously\n\n3.1 PDF or CDF Definitions\nThe probability density function (PDF) and cumulative distribution function (CDF) of the Studentized Range are not as simple to express in closed form as, say, the normal or \\(t\\)-distribution. They involve integrals of the joint distribution of order statistics of normal variates.\n\nA simplified conceptual approach\n\nLet \\(Z_1, Z_2, \\dots, Z_k\\) be i.i.d. standard normal.\nLet \\(\\bar{Z}\\) be their mean, and let \\(S^2 = \\sum_{i=1}^k (Z_i - \\bar{Z})^2\\) be an estimate of the variance (but note carefully how the degrees of freedom tie in).\nThe difference \\(\\max(Z) - \\min(Z)\\) is the “range” in the numerator.\nScale that range by \\(\\sqrt{S^2 / \\nu}\\) to get \\(q\\).\n\nThis results in the Studentized Range distribution parameters \\(k\\) (number of sample points) and \\(\\nu\\) (degrees of freedom).\nIn practice, tables (or software functions) for \\(q\\) are used to find critical values rather than dealing with the PDF or CDF directly."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html#critical-values-and-tables",
    "href": "appendix/stat-topics/standardized-range-distribution.html#critical-values-and-tables",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "4. Critical Values and Tables",
    "text": "4. Critical Values and Tables\nWhen you see references to \\(q_{\\alpha, k, \\nu}\\), it means:\n\n\\(\\alpha\\) is the desired significance level (or tail probability),\n\\(k\\) is the number of groups,\n\\(\\nu\\) is the error degrees of freedom.\n\nFor example, if you see \\(q_{0.05, 4, 20}\\), it’s the critical value such that\n\\[\nP(Q \\geq q_{0.05, 4, 20}) = 0.05\n\\]\nwhere \\(Q \\sim q\\)-distribution with parameters \\(k=4\\), \\(\\nu=20\\).\nThese tables are typically used in Tukey’s HSD test. Statistical software (R, Python, SAS, SPSS, etc.) also can provide these critical values."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html#properties-and-uses",
    "href": "appendix/stat-topics/standardized-range-distribution.html#properties-and-uses",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "5. Properties and Uses",
    "text": "5. Properties and Uses\n\nMultiple Comparisons Adjustments\n\nTukey’s HSD is probably the best-known method leveraging the Studentized Range. It maintains the familywise error rate at a specified \\(\\alpha\\) for all pairwise comparisons.\n\nRelationship to Other Distributions\n\nFor \\(k = 2\\), the Studentized Range distribution is identical to the \\(t\\)-distribution, because the “range” between exactly two points is just their difference.\nWhen \\(k &gt; 2\\), the distribution is distinct; the critical value for a given \\(\\alpha\\) will be larger than the corresponding \\(t\\)-value to account for multiple comparisons.\n\nConservative Bound in Some Cases\n\nThe distribution is specifically tailored for pairwise range-based comparisons. In some cases (especially when group sizes are unequal or for non-pairwise contrasts), the distribution-based approach might be more conservative or only approximately correct. Adjustments exist for unbalanced designs."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html#practical-computation",
    "href": "appendix/stat-topics/standardized-range-distribution.html#practical-computation",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "6. Practical Computation",
    "text": "6. Practical Computation\nThese days, manual table lookup is less common. Statistical software typically has built-in functions for either:\n\nGenerating quantiles (critical values),\nComputing p-values for an observed Studentized Range statistic,\nDirectly implementing Tukey’s HSD test.\n\n\n6.1 In R\n\nqtukey() gives the quantiles of the Studentized Range distribution (critical values).\nptukey() gives the cumulative distribution function (CDF).\nFunctions like TukeyHSD() automatically handle calculations after an aov().\n\n\n\n6.2 In Python\nWhile there is not always a direct built-in in scipy.stats for the Studentized Range, there are packages like statsmodels that implement TukeyHSD. Alternatively, one might use tables or approximate expansions.\n\n\n6.3 In Other Software\n\nSAS has PROC GLM with MEANS statement: MEANS group / TUKEY;\nSPSS has Post Hoc tests with a Tukey option as well."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html#step-by-step-example",
    "href": "appendix/stat-topics/standardized-range-distribution.html#step-by-step-example",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "7. Step-by-Step Example",
    "text": "7. Step-by-Step Example\nLet’s run through a conceptual example for clarity.\nScenario You have 4 groups (A, B, C, D), each with 10 observations, and you’ve performed a one-way ANOVA. The ANOVA F-test is significant. You decide to do Tukey’s HSD to compare each pair of group means.\n\nObtain MSE and \\(\\nu\\)\n\nFrom your ANOVA table, suppose \\(MSE = 2.5\\).\nDegrees of freedom for error, \\(\\nu = N - k = 40 - 4 = 36\\).\n\nCompute Pairwise Mean Differences\n\nSuppose the sample means are:\n\n\n\\[\n\\bar{X}_A = 12.8,\\; \\bar{X}_B = 14.5,\\; \\bar{X}_C = 13.2,\\; \\bar{X}_D = 15.3.\n\\]\n\nPairwise differences (absolute values):\n\n\\[\n|\\bar{X}_A - \\bar{X}_B| = 1.7,\\quad\n     |\\bar{X}_A - \\bar{X}_C| = 0.4,\\quad\n     |\\bar{X}_A - \\bar{X}_D| = 2.5, \\quad\n     |\\bar{X}_B - \\bar{X}_C| = 1.3,\\quad\n     |\\bar{X}_B - \\bar{X}_D| = 0.8,\\quad\n     |\\bar{X}_C - \\bar{X}_D| = 2.1.\n\\]\n\nCalculate Standard Error\n\nWith equal sample size \\(n = 10\\), the standard error for the difference is\n\n\n\\[SE = \\sqrt{\\frac{MSE}{n}} = \\sqrt{\\frac{2.5}{10}} = \\sqrt{0.25} = 0.5.\n\\]\n\nForm the Tukey \\(q\\)-Statistic\n\nFor each pair, \\(q = \\frac{\\text{difference}}{SE}\\).\nFor instance, for \\(\\bar{X}_A - \\bar{X}_D = 2.5\\):\n\n\n\\[\nq = \\frac{2.5}{0.5} = 5.0.\n\\]\n\nCompare to Critical Value\n\nLook up (or compute via software) \\(q_{0.05, k=4, \\nu=36}\\).\nSuppose you find that \\(q_{0.05, 4, 36} \\approx 3.80\\) (this is just an example value; actual lookups may differ).\nSince \\(q = 5.0\\) for the A–D comparison is greater than 3.80, the difference between groups A and D is statistically significant at \\(\\alpha=0.05\\).\n\nInterpretation\n\nYou would interpret each pair’s \\(q\\)-value by comparing to the Studentized Range critical value. Any pair with \\(q\\) above the critical threshold is considered to have a significant difference in means."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html#key-takeaways",
    "href": "appendix/stat-topics/standardized-range-distribution.html#key-takeaways",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "8. Key Takeaways",
    "text": "8. Key Takeaways\n\nRange-Based Multiple Comparisons\n\n\nThe Studentized Range distribution is the distribution of the range (max - min) of \\(k\\) normally distributed variables, scaled by an estimate of standard deviation with \\(\\nu\\) degrees of freedom.\n\n\nConnection to Tukey’s HSD\n\n\nIt underpins Tukey’s HSD, a widely used method for controlling familywise error when comparing multiple group means post-ANOVA.\n\n\nCritical Value Lookup\n\n\nValues \\(q_{\\alpha, k, \\nu}\\) can be found in standard statistical tables or via statistical software.\nFor large \\(\\nu\\), the distribution converges somewhat to a normal-based range, but always consult the exact distribution for accurate results.\n\n\nSoftware Implementation\n\n\nBecause the PDF and CDF are complex integrals, usage typically relies on tables or built-in software routines (e.g., qtukey(), TukeyHSD(), etc.).\n\n\nDegrees of Freedom\n\n\nAlways match the appropriate error degrees of freedom, typically from an ANOVA’s residual or error term."
  },
  {
    "objectID": "appendix/stat-topics/standardized-range-distribution.html#further-reading",
    "href": "appendix/stat-topics/standardized-range-distribution.html#further-reading",
    "title": "Tukey’s Standardized Range Distribution",
    "section": "9. Further Reading",
    "text": "9. Further Reading\n\nHochberg, Y. and Tamhane, A. C. (1987). Multiple Comparison Procedures. Wiley.\nMontgomery, D. C. (2012). Design and Analysis of Experiments. Wiley.\nKirk, R. E. (2013). Experimental Design: Procedures for the Behavioral Sciences. SAGE.\n\nThese sources go into more technical depth on the Studentized Range distribution, multiple comparisons, and the context of Tukey’s HSD.\n\nFinal Note\nThe Studentized Range distribution is essential whenever you deal with range-based statistics, especially in the context of comparing multiple means simultaneously. Its principal role in Tukey’s HSD makes it a standard tool in the statistician’s toolbox for post-ANOVA analyses, ensuring control of the familywise error rate in pairwise comparisons."
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html",
    "href": "assignments/assignment10_row_column_designs.html",
    "title": "Assignment 10: Row and Column Designs",
    "section": "",
    "text": "Objective: Analyze row and column designs, including row-column interactions, post-hoc tests, and trend analyses.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 85"
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html#instructions",
    "href": "assignments/assignment10_row_column_designs.html#instructions",
    "title": "Assignment 10: Row and Column Designs",
    "section": "",
    "text": "Objective: Analyze row and column designs, including row-column interactions, post-hoc tests, and trend analyses.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 85"
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html#latin-squares",
    "href": "assignments/assignment10_row_column_designs.html#latin-squares",
    "title": "Assignment 10: Row and Column Designs",
    "section": "12.2 Latin Squares",
    "text": "12.2 Latin Squares\nData Source: Dean et al. (2017), Chapter 12, Exercise 2\n\nShow that there is only one standard \\(3 \\times 3\\) Latin square.\n\n(Hint: Given the letters in the first row and the first column, show that there is only one way to complete the Latin square.)\n\nShow that there are exactly four standard \\(4 \\times 4\\) Latin squares."
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html#video-game-experiment",
    "href": "assignments/assignment10_row_column_designs.html#video-game-experiment",
    "title": "Assignment 10: Row and Column Designs",
    "section": "12.6 Video Game Experiment",
    "text": "12.6 Video Game Experiment\nData Source: Dean et al. (2017), Chapter 12, Exercise 6\nProfessor Robert Wardrop conducted an experiment in 1991 to evaluate which of five sound modes helped him play a certain video game best. The sound modes included:\n\nModes 1–3: Three different types of background music with game sounds expected to enhance play.\nMode 4: Game sounds only, no background music.\nMode 5: No music or game sounds.\n\nThe experiment used a Latin square design with two blocking factors: day and time order of the game. The response variable was the game score (higher scores indicate better performance). The design and data are provided in Table 12.16.\n\nLatin square design showing treatments and data for the video game experiment\n\n\n\n\n\n\n\n\n\n\nTime Order\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5\n\n\n\n\n1\n1 (94)\n3 (100)\n4 (98)\n2 (101)\n5 (112)\n\n\n2\n3 (103)\n2 (111)\n1 (51)\n5 (110)\n4 (90)\n\n\n3\n4 (114)\n1 (75)\n5 (94)\n3 (85)\n2 (107)\n\n\n4\n5 (100)\n4 (74)\n2 (70)\n1 (93)\n3 (106)\n\n\n5\n2 (106)\n5 (95)\n3 (81)\n4 (90)\n1 (73)\n\n\n\n\nQuestions\n\nWrite down a possible model for the data and check the model assumptions. If the assumptions appear to be approximately satisfied, proceed with parts (b)–(f).\nPlot the adjusted data and discuss the plot.\nComplete an analysis of variance table.\nEvaluate whether blocking (day and time order) was effective.\nConstruct simultaneous 95% confidence intervals for:\n\nAll pairwise comparisons of the five sound modes.\nThe “music versus no music” contrast:\n\n\n\\[\n\\frac{1}{3}(\\tau_1 + \\tau_2 + \\tau_3) - \\frac{1}{2}(\\tau_4 + \\tau_5)\n\\] 3. The “game sound versus no game sound” contrast:\n\\[\n\\frac{1}{4}(\\tau_1 + \\tau_2 + \\tau_3 + \\tau_4) - \\tau_5\n\\]\n\nSummarize the conclusions of the experiment. Which sound mode(s) should Professor Wardrop use for optimal performance?"
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html#grading-allocation",
    "href": "assignments/assignment10_row_column_designs.html#grading-allocation",
    "title": "Assignment 10: Row and Column Designs",
    "section": "Grading Allocation",
    "text": "Grading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nLatin Squares\n(a)\n5\n\n\n\n(b)\n5\n\n\nVideo Game Experiment\n(a)\n10\n\n\n\n(b)\n10\n\n\n\n(c)\n15\n\n\n\n(d)\n10\n\n\n\n(e)\n20\n\n\n\n(f)\n10\n\n\nTotal\n\n85"
  },
  {
    "objectID": "assignments/assignment12_nested_models.html",
    "href": "assignments/assignment12_nested_models.html",
    "title": "Assignment 12: Nested Models",
    "section": "",
    "text": "Objective: Analyze nested models, including nested random effects, interactions, and post-hoc tests.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment12_nested_models.html#instructions",
    "href": "assignments/assignment12_nested_models.html#instructions",
    "title": "Assignment 12: Nested Models",
    "section": "",
    "text": "Objective: Analyze nested models, including nested random effects, interactions, and post-hoc tests.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment12_nested_models.html#general-mixed-model",
    "href": "assignments/assignment12_nested_models.html#general-mixed-model",
    "title": "Assignment 12: Nested Models",
    "section": "12.3 General Mixed Model",
    "text": "12.3 General Mixed Model\nData Source: Dean et al. (2017), Chapter 12, Exercise 3\nConsider the model:\n\\[\nY_{ijkl} = \\mu + \\alpha_i + B_j(i) + C_k(ji) + \\delta_l + (\\alpha\\delta)_{il} + (B\\delta)_{lj}(i) + \\epsilon_{ijkl}\n\\]\n\nQuestions\n\nCalculate the expected mean squares for all effects in the model.\nWhich ratio would you use to test \\(H_0: \\delta_l + (\\alpha\\delta)_{il} \\, \\text{are all equal?}\\)\nWhich ratio would you use to test \\(H_0: \\sigma^2_\\alpha = 0\\)?"
  },
  {
    "objectID": "assignments/assignment12_nested_models.html#operator-experiment",
    "href": "assignments/assignment12_nested_models.html#operator-experiment",
    "title": "Assignment 12: Nested Models",
    "section": "18.6 Operator Experiment",
    "text": "18.6 Operator Experiment\nData Source: Dean et al. (2017), Chapter 18, Exercise 6\nAn experiment aimed to determine how much variation in measured manganese concentration in steel is due to operator variability. Ten steel samples were sliced from a billet, and four operators each measured the manganese content twice per sample in a random order.\n\nQuestions\n\nWrite a model for this experiment. Clearly indicate which effects are fixed, random, crossed, and nested.\nWrite the degrees of freedom, sums of squares, and expected mean squares for each source of variation.\nInvestigate whether the normal distribution is a reasonable approximation for the error terms (using the data in Table 18.11). Transformation may be necessary.\nIf the normal distribution is reasonable, analyze the experiment. Obtain variance estimates for the random effects and identify the major sources of variation."
  },
  {
    "objectID": "assignments/assignment12_nested_models.html#table-18.11-manganese-concentrations-percentages-for-the-operator-experiment",
    "href": "assignments/assignment12_nested_models.html#table-18.11-manganese-concentrations-percentages-for-the-operator-experiment",
    "title": "Assignment 12: Nested Models",
    "section": "Table 18.11 Manganese Concentrations (Percentages) for the Operator Experiment",
    "text": "Table 18.11 Manganese Concentrations (Percentages) for the Operator Experiment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample\nOperator 1\n\nOperator 2\n\nOperator 3\n\nOperator 4\n\n\n\n\n\n\n1\n2\n1\n2\n1\n2\n1\n2\n\n\n1\n0.63\n0.60\n0.62\n0.62\n0.60\n0.60\n0.59\n0.61\n\n\n2\n0.64\n0.63\n0.63\n0.64\n0.67\n0.65\n0.62\n0.64\n\n\n3\n0.60\n0.58\n0.60\n0.61\n0.60\n0.60\n0.58\n0.60\n\n\n4\n0.75\n0.74\n0.74\n0.74\n0.74\n0.73\n0.73\n0.76\n\n\n5\n0.71\n0.68\n0.69\n0.70\n0.69\n0.67\n0.68\n0.71\n\n\n6\n0.65\n0.63\n0.62\n0.65\n0.63\n0.64\n0.62\n0.64\n\n\n7\n0.67\n0.64\n0.66\n0.67\n0.65\n0.65\n0.64\n0.66\n\n\n8\n0.65\n0.63\n0.65\n0.64\n0.62\n0.62\n0.60\n0.62\n\n\n9\n0.68\n0.66\n0.67\n0.68\n0.67\n0.67\n0.65\n0.68\n\n\n10\n0.67\n0.64\n0.66\n0.66\n0.65\n0.64\n0.64\n0.66\n\n\n\n\nGrading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nGeneral Mixed Model\n(a)\n10\n\n\n\n(b)\n10\n\n\n\n(c)\n10\n\n\nOperator Experiment\n(a)\n10\n\n\n\n(b)\n10\n\n\n\n(c)\n20\n\n\n\n(d)\n30\n\n\nTotal Score\n\n100%"
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html",
    "href": "assignments/assignment1_basics_of_experimental_design.html",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "",
    "text": "Objective: Demonstrate understanding of key concepts in experimental design and apply them to real-world scenarios.\nSubmission: Create a document that addresses the prompts below. Submit your responses as a PDF or Markdown file.\nFormat: Use clear headings and subheadings to organize your responses. Include any code or calculations as needed.\nTotal points: 100"
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#instructions",
    "href": "assignments/assignment1_basics_of_experimental_design.html#instructions",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "",
    "text": "Objective: Demonstrate understanding of key concepts in experimental design and apply them to real-world scenarios.\nSubmission: Create a document that addresses the prompts below. Submit your responses as a PDF or Markdown file.\nFormat: Use clear headings and subheadings to organize your responses. Include any code or calculations as needed.\nTotal points: 100"
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#part-1-key-concepts-of-experimental-design",
    "href": "assignments/assignment1_basics_of_experimental_design.html#part-1-key-concepts-of-experimental-design",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Part 1: Key Concepts of Experimental Design",
    "text": "Part 1: Key Concepts of Experimental Design\n\nDefining Objectives:\n\nWhat are the primary reasons for conducting experiments as outlined in Chapter 1? Provide an example for each purpose.\n\nExperimental vs. Observational Studies:\n\nExplain the differences between experimental and observational studies, emphasizing their strengths and limitations in establishing causality. Use the factory machine quality example in Chapter 1 for illustration.\n\nBasic Techniques:\n\nDescribe replication, blocking, and randomization. For each technique, provide an example that demonstrates how it improves the validity or precision of an experiment."
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#part-2-checklist-for-planning-experiments",
    "href": "assignments/assignment1_basics_of_experimental_design.html#part-2-checklist-for-planning-experiments",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Part 2: Checklist for Planning Experiments",
    "text": "Part 2: Checklist for Planning Experiments\n\nDefining Objectives:\n\nList and briefly describe the steps from Chapter 2’s checklist for planning experiments. Emphasize why defining objectives and identifying sources of variation are critical.\n\nPilot Experiments:\n\nDescribe the purpose of running a pilot experiment and how it aids in refining the experimental design.\n\nHypothetical Scenario:\n\nImagine you are designing an experiment to test the effects of different fertilizers (F1, F2, F3) on plant growth. Outline your approach to:\n\nDefine objectives.\nIdentify sources of variation.\nChoose an assignment rule for treatments."
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#part-3-real-world-applications",
    "href": "assignments/assignment1_basics_of_experimental_design.html#part-3-real-world-applications",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Part 3: Real-World Applications",
    "text": "Part 3: Real-World Applications\n\nCotton-Spinning Experiment (Chapter 2):\n\nSummarize the objectives and the experimental design used in the cotton-spinning experiment.\nExplain how blocking was used to account for variability and improve precision.\n\nImprovements:\n\nPropose one way to improve the cotton-spinning experiment for broader applicability or reduced cost."
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#part-4-numerical-application",
    "href": "assignments/assignment1_basics_of_experimental_design.html#part-4-numerical-application",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Part 4: Numerical Application",
    "text": "Part 4: Numerical Application\nYou are tasked with analyzing a hypothetical dataset to practice experimental design concepts. Use the following dataset where three treatments (A, B, C) are applied to five blocks (1–5). The response variable represents a numerical outcome.\n\n\n\nBlock\nTreatment A\nTreatment B\nTreatment C\n\n\n\n\n1\n10.1\n9.8\n10.2\n\n\n2\n10.5\n9.7\n10.3\n\n\n3\n10.3\n10.1\n10.4\n\n\n4\n10.4\n9.9\n10.0\n\n\n5\n10.2\n10.0\n10.1\n\n\n\n\nStatistical Model:\n\nWrite the model equation that represents the relationship between treatment and response while accounting for blocks.\n\nAnalysis:\n\nConduct a one-way ANOVA using R to test for treatment differences. Include R code and output.\n\nInterpretation:\n\nState the null and alternative hypotheses.\nSummarize the ANOVA results, including whether to reject the null hypothesis.\n\nDiagnostics:\n\nPerform residual analysis to evaluate the assumptions of ANOVA."
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#grading-rubric",
    "href": "assignments/assignment1_basics_of_experimental_design.html#grading-rubric",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\n\n\n\n\n\nSection\nPoints\nCriteria\n\n\n\n\nExperiment Objectives\n10\n5 points for accurately listing two objectives, 5 points for providing correct examples.\n\n\nObservational Studies vs. Experiments\n15\n8 points for correctly explaining differences, 7 points for a clear and relevant example.\n\n\nDesign Your Own Experiment\n30\n10 points each for objectives, sources of variation, and assignment rule. Clear reasoning required.\n\n\nExperiments 1 and 7\n20\n10 points per experiment: 5 for objectives, 5 for sources of variation and treatment factors.\n\n\nExperiment 4\n25\n10 points for listing sources of variation, 10 for categorization and controllability, 5 for blocking.\n\n\nTotal\n100"
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "",
    "text": "Objective: Perform multiple comparisons and contrasts for experimental data, including Scheffé’s method and simultaneous confidence intervals.\nUse R or a similar tool to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#instructions",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#instructions",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "",
    "text": "Objective: Perform multiple comparisons and contrasts for experimental data, including Scheffé’s method and simultaneous confidence intervals.\nUse R or a similar tool to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#buoyancy-experiment",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#buoyancy-experiment",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "1. Buoyancy Experiment",
    "text": "1. Buoyancy Experiment\nInvestigate the question: “Is the buoyancy of an object in water affected by different concentrations of salt in the water?”\nTasks: (a) Address steps (a)–(d) from the experimental planning checklist in detail:\n\nDefine the objectives of the experiment.\nIdentify sources of variation, including treatment factors, experimental units, and nuisance factors.\nChoose an assignment rule for the experimental units.\nSpecify measurements, procedures, and any anticipated difficulties.\nSpecify preplanned contrasts or functions to estimate. Justify your choices.\nDetermine which multiple comparison methods, if any, are appropriate and explain why.\n\n\nConduct a small pilot experiment to estimate the preliminary variance (\\(\\sigma^2\\)). Use R to simulate data if needed and summarize findings.\nRevisit and complete the checklist by evaluating findings from the pilot study and revising experimental decisions if necessary."
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#cotton-spinning-experiment-continuation-from-section-2.3-p.-13",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#cotton-spinning-experiment-continuation-from-section-2.3-p.-13",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "2. Cotton-Spinning Experiment (Continuation from Section 2.3, P. 13)",
    "text": "2. Cotton-Spinning Experiment (Continuation from Section 2.3, P. 13)\nFor the cotton-spinning experiment:\n\nIdentify and define contrasts or functions of interest. Specify which effects or comparisons will provide insight into the experimental objectives.\nProvide contrast coefficients for your selected contrasts."
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#meat-cooking-experiment-continuation-from-exercise-14-chapter-3",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#meat-cooking-experiment-continuation-from-exercise-14-chapter-3",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "3. Meat Cooking Experiment (Continuation from Exercise 14, Chapter 3)",
    "text": "3. Meat Cooking Experiment (Continuation from Exercise 14, Chapter 3)\nAn experiment was run to investigate the amount of weight lost (in grams) by ground beef ham- burgers after grilling or frying, and how much the weight loss is affected by the percentage fat in the beef before cooking. The experiment involved two factors: cooking method (factor A, with two levels frying and grilling, coded 1, 2), and fat content (factor B, with three levels 10, 15, and 20%, coded 1, 2, 3). Thus there were six treatment combinations 11, 12, 13, 21, 22, 23, relabeled as treatment levels 1, 2, …, 6, respectively. Hamburger patties weighing 110 g each were prepared from meat with the required fat content. There were 30 “cooking time slots” which were randomly assigned to the treatments in such a way that each treatment was observed five times (r = 5). The patty weights after cooking are shown in Table 3.14.\n\nConduct pairwise comparisons for the six treatments using Scheffé’s method with a 95% overall confidence level. Present your results with appropriate statistical summaries and visualizations.\nAnalyze the expressions \\(\\mu + \\frac{(\\tau_1 + \\tau_4)}{2}\\), \\(\\mu + \\frac{(\\tau_2 + \\tau_5)}{2}\\), and \\(\\mu + \\frac{(\\tau_3 + \\tau_6)}{2}\\):\n\n\nInterpret what these expressions represent in the context of the experimental setup.\nPerform pairwise comparisons among these three treatment averages using Scheffé’s method with a 95% confidence level.\nSummarize and interpret the findings in terms of practical significance and experimental insights."
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#reaction-time-experiment",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#reaction-time-experiment",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "4. Reaction Time Experiment",
    "text": "4. Reaction Time Experiment\nSource: (L. Cai, T. Li, Nishant, and A. van der Kouwe, 1996)\nThis experiment investigates the effects of auditory and visual cues on the speed of response of a human subject. A personal computer presents a stimulus to the subject, and the reaction time required to press a key is monitored. The subject is warned of the forthcoming stimulus through auditory or visual cues. The study also considers different elapsed times between cue and stimulus as a factor.\nFactors and Levels:\n\nCue Stimulus: Auditory or Visual\nElapsed Time: 5, 10, or 15 seconds\n\nTreatment Combinations and Codes:\n\n1 = Auditory, 5 seconds\n2 = Auditory, 10 seconds\n3 = Auditory, 15 seconds\n4 = Visual, 5 seconds\n5 = Visual, 10 seconds\n6 = Visual, 15 seconds\n\nThe pilot experiment data, involving one subject, are shown in Table 4.4.\nHere is the recreated Table 4.4 in a structured format:\nTable 4.4: Reaction times, in seconds, for the reaction time experiment (order of collection in parentheses).\n\n\n\n\n\n\n\n\n\nTreatment Combination\nReaction Time 1 (s) (Order)\nReaction Time 2 (s) (Order)\nReaction Time 3 (s) (Order)\n\n\n\n\n1\n0.181 (18)\n0.204 (9)\n0.170 (10)\n\n\n2\n0.187 (12)\n0.167 (3)\n0.182 (5)\n\n\n3\n0.236 (17)\n0.202 (13)\n0.198 (16)\n\n\n4\n0.269 (15)\n0.257 (7)\n0.279 (14)\n\n\n5\n0.260 (11)\n0.283 (6)\n0.235 (8)\n\n\n6\n0.258 (4)\n0.256 (1)\n0.281 (2)\n\n\n\n\nUnderstanding the Structure of the Data\nThe Reaction Time Experiment data consists of observations measuring how quickly a participant responds to an auditory or visual cue. The study manipulates two factors:\n\nCue Stimulus (Factor A): Auditory (A1) or Visual (A2).\nElapsed Time (Factor B): 5 seconds (B1), 10 seconds (B2), or 15 seconds (B3).\n\nEach treatment combination represents a specific pairing of these two factors:\n\n\n\nTreatment Combination\nCue Type\nElapsed Time\n\n\n\n\n1\nAuditory (A1)\n5 sec (B1)\n\n\n2\nAuditory (A1)\n10 sec (B2)\n\n\n3\nAuditory (A1)\n15 sec (B3)\n\n\n4\nVisual (A2)\n5 sec (B1)\n\n\n5\nVisual (A2)\n10 sec (B2)\n\n\n6\nVisual (A2)\n15 sec (B3)\n\n\n\nFor each treatment combination, reaction times were recorded over multiple trials, with the order of data collection also noted.\nTasks: (a) Identify Preplanned Contrasts:\n\nDetermine contrasts of interest, such as comparing auditory versus visual treatments. Specify the contrasts mathematically, including coefficients for the comparisons.\n\n\nPlot the Data:\n\n\nVisualize the reaction times using an appropriate plot (e.g., bar or line plot). Summarize what the plot reveals about the treatment effects.\n\n\nHypothesis Testing:\n\n\nTest the null hypothesis \\(H_0\\): The treatments have no effect on reaction time against the alternative \\(H_1\\): The treatments have an effect on reaction time. Include a detailed explanation of the statistical test used and the results obtained.\n\n\nSimultaneous Confidence Intervals:\n\n\nCompute simultaneous 90% confidence intervals for the preplanned contrasts using a chosen method (e.g., Tukey’s or Scheffé’s method). Interpret these intervals and state your conclusions about the treatment effects."
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#grading-rubric-for-the-assignment",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#grading-rubric-for-the-assignment",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "Grading Rubric for the Assignment",
    "text": "Grading Rubric for the Assignment\n\n\n\n\n\n\n\n\n\n\nTask\nExcellent (100%)\nGood (85%)\nSatisfactory (70%)\nNeeds Improvement (50%)\n\n\n\n\n1. Buoyancy Experiment\nComplete and detailed checklist, well-justified preplanned contrasts, proper simulation and analysis in R.\nDetailed checklist but minor omissions in preplanned contrasts or R simulation.\nChecklist addressed but lacks sufficient detail or justification. Simulation and analysis incomplete.\nMajor gaps in checklist completion, no simulation, or preplanned contrasts missing.\n\n\n2. Cotton-Spinning Experiment\nAll contrasts identified clearly, and coefficients are mathematically accurate and fully explained.\nMost contrasts identified, and coefficients mostly correct. Explanation somewhat unclear.\nContrasts partially correct; coefficients missing or inaccurate. Explanation lacks detail or clarity.\nIncorrect or missing contrasts; coefficients absent or mathematically incorrect.\n\n\n3. Meat Cooking Experiment\nAll pairwise comparisons and interpretations are correct and well-presented with visualizations and context.\nMost pairwise comparisons correct, with minor errors in calculations or interpretations.\nComparisons incomplete or partially correct. Visualizations or interpretations are unclear.\nIncorrect or missing pairwise comparisons, no visualizations, or failure to interpret findings.\n\n\n4. Reaction Time Experiment\nAccurate contrasts with coefficients, clear plots, correct hypothesis testing, and well-justified confidence intervals.\nCorrect contrasts and coefficients, minor issues with plots or hypothesis testing, intervals partially correct.\nPartial contrasts or coefficients; plots unclear, hypothesis testing or intervals incomplete.\nContrasts missing or incorrect, no plots, hypothesis testing or intervals omitted or inaccurate.\n\n\n\n\nGrading Allocation\nEach question is worth 25 points, distributed as follows:\n\nCompleteness (10 points): Address all aspects of the question.\nAccuracy (10 points): Provide correct methods, calculations, and R code.\nClarity (5 points): Present results in a clear, logical, and concise manner."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html",
    "href": "assignments/assignment5_two_factor_anova.html",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "",
    "text": "Objective: Explore the two-way complete model and its applications in experimental designs.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 95"
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#instructions",
    "href": "assignments/assignment5_two_factor_anova.html#instructions",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "",
    "text": "Objective: Explore the two-way complete model and its applications in experimental designs.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 95"
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.2-interaction-contrast",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.2-interaction-contrast",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.2: Interaction Contrast",
    "text": "Exercise 6.2: Interaction Contrast\nVerify that the expression\n\\[\n\\tau_{ij} - \\tau_{i.} - \\tau_{.j} + \\tau_{..}\n\\]\nis an interaction contrast for the two-way complete model.\n\nWrite the list of contrast coefficients in terms of the \\(\\tau_{ij}\\)’s under the following conditions:\n\nFactor \\(A\\) has \\(a = 3\\) levels.\nFactor \\(B\\) has \\(b = 4\\) levels."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.3-functions-of-parameters",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.3-functions-of-parameters",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.3: Functions of Parameters",
    "text": "Exercise 6.3: Functions of Parameters\nConsider the following functions under the two-way complete model:\n\n\\(\\{\\alpha^{*}_{1} - \\alpha^{*}_{2}\\}\\).\n\\(\\{(\\alpha \\beta)_{11} - (\\alpha \\beta)_{21} - (\\alpha \\beta)_{12} + (\\alpha \\beta)_{22}\\}\\).\n\n\nTasks\n\nVerify that these functions are estimable contrasts.\nDiscuss the meaning of each contrast for:\n\n\nPlot (d) from Figure 6.1 on page 140.\nPlot (g) from Figure 6.2 on page 141.\n\n\nFor \\(a = b = 3\\), provide:\n\n\nThe list of contrast coefficients for the parameters involved in the contrast.\nThe equivalent list of contrast coefficients in terms of the \\(\\tau_{ij}\\) parameters of the cell-means model."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.7-weld-strength-experiment",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.7-weld-strength-experiment",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.7: Weld Strength Experiment",
    "text": "Exercise 6.7: Weld Strength Experiment\nThe data in Table 6.22, obtained from Anderson and McLean (1974), show the weld strength of a steel bar under varying conditions of two factors:\n\nGage bar setting: The distance traveled by the weld die during the automatic weld cycle.\nTime of welding: The total time of the automatic weld cycle.\n\n\n\n\nTime of Welding (j)\n1\n2\n3\n4\n5\n\n\n\n\nGage Bar 1\n10, 12\n13, 17\n21, 30\n18, 16\n17, 21\n\n\nGage Bar 2\n15, 19\n14, 12\n30, 38\n15, 11\n14, 12\n\n\nGage Bar 3\n10, 8\n12, 9\n10, 5\n14, 15\n19, 11\n\n\n\n\nTasks\n\nUsing the cell-means model, test the hypothesis that there is no difference in the effects of the treatment combinations on weld strength.\nWrite contrasts in terms of \\(\\tau_{ij}\\) for:\n\n\nAll pairwise comparisons.\nThe difference between gage bar setting 3 and the average of the other two.\n\n\nSuggest a strategy to calculate all intervals at an overall confidence level of at least 98%.\n\n\nCompute confidence intervals for:\n\n\n\\(\\tau_{13} - \\tau_{15}\\): The difference in true mean strengths at the 3rd and 5th welding times for the first gage bar setting.\nThe difference between gage bar setting 3 and the average of the other two gage bar settings.\n\n\nCalculate an upper 90% confidence limit for \\(\\sigma^2\\).\nDetermine the total number of observations needed if the pairwise comparison intervals in part (b) must have a maximum width of 8."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.12-memory-experiment",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.12-memory-experiment",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.12: Memory Experiment",
    "text": "Exercise 6.12: Memory Experiment\nThe memory experiment investigates the effects of two factors:\n\nWord type: Fruits, nouns, and mixed types.\nType of distraction: No distraction, constant distraction, or changing distraction.\n\nThe response variable is the number of words remembered, with variance approximated by \\(30p(1-p)\\).\n\nTask\nDetermine the number of subjects required to reject the following hypotheses with a power of 0.9 at \\(\\alpha = 0.05\\):\n\n\\(H_A\\): The memorization rate is identical for the three word types.\n\\(H_B\\): The distraction types have no effect on memorization.\n\nAssume the minimum difference of interest in memorization rates is 4 words."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.16-survival-experiment",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.16-survival-experiment",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.16: Survival Experiment",
    "text": "Exercise 6.16: Survival Experiment\nThe following table provides survival times (in units of 10 hours) for animals exposed to three poisons and four treatments.\n\n\n\n\n\n\n\n\n\n\nPoison/Treatment\n1\n2\n3\n4\n\n\n\n\nI\n0.31, 0.45, 0.46, 0.43\n0.82, 1.10, 0.88, 0.72\n0.43, 0.45, 0.63, 0.76\n0.45, 0.71, 0.66, 0.62\n\n\nII\n0.36, 0.29, 0.40, 0.23\n0.92, 0.61, 0.49, 1.24\n0.44, 0.35, 0.31, 0.40\n0.56, 1.02, 0.71, 0.38\n\n\nIII\n0.22, 0.21, 0.18, 0.23\n0.30, 0.37, 0.38, 0.29\n0.23, 0.25, 0.24, 0.22\n0.30, 0.36, 0.31, 0.33\n\n\n\n\nTasks\n\nVerify assumptions of the two-way complete model using the original data. Analyze the data if the assumptions are satisfied.\nTransform the data using \\(y^{-1}\\) (reciprocal). Reassess model assumptions and reanalyze the transformed data.\nCreate interaction plots for the original and transformed data. Discuss how the interaction between poison and treatment varies with each scale."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.19-main-effects-model-verification",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.19-main-effects-model-verification",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.19: Main-Effects Model Verification",
    "text": "Exercise 6.19: Main-Effects Model Verification\nFor the two-way main-effects model with equal sample sizes, perform the following:\n\nVerify the computational formula for \\(SS_E\\) as given in equation (6.5.38).\nDemonstrate that \\(E[SSE] = (n - a - b + 1)\\sigma^2\\).\n\nHint: Use the formula \\(E[X^2] = \\text{Var}(X) + E[X]^2\\)."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#summary-of-the-grades-and-rubric",
    "href": "assignments/assignment5_two_factor_anova.html#summary-of-the-grades-and-rubric",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Summary of the Grades and Rubric",
    "text": "Summary of the Grades and Rubric\n\n\n\n\n\n\n\n\n\nExercise\nTask\nPoints\nDescription\n\n\n\n\nExercise 6.2\nDerive contrast coefficients and verify interaction contrast property.\n10\n- 4 points for correct derivation of coefficients. - 3 points for verifying the interaction contrast property. - 3 points for clarity and completeness.\n\n\nExercise 6.3\nVerify estimability, interpret contrasts, and compute coefficients.\n15\n- 5 points for verifying estimability. - 5 points for interpreting contrasts in provided plots. - 5 points for accurate and clear computation of coefficients.\n\n\nExercise 6.7\nPerform hypothesis testing, compute contrasts, intervals, and variance limits.\n25\n- 5 points each for parts (a) to (e), including hypothesis testing, contrast definitions, confidence interval computations, upper confidence limits for \\(\\sigma^2\\), and sample size needs.\n\n\nExercise 6.12\nCompute sample size for given power and significance level.\n15\n- 8 points for accurate computation of sample size. - 7 points for interpretation and justification of the calculations.\n\n\nExercise 6.16\nVerify model assumptions, perform transformation, and create interaction plots.\n20\n- 7 points for verifying assumptions. - 7 points for transformation analysis. - 6 points for interaction plots and interpretation.\n\n\nExercise 6.19\nVerify \\(SS_E\\) formula and derive expected value of \\(SSE\\).\n10\n- 5 points for verifying \\(SS_E\\) computational formula. - 5 points for deriving \\(E[SSE] = (n - a - b + 1)\\sigma^2\\)."
  },
  {
    "objectID": "assignments/assignment7_randomized_complete_block_design.html",
    "href": "assignments/assignment7_randomized_complete_block_design.html",
    "title": "Assignment 7: Randomized Complete Block Design",
    "section": "",
    "text": "Objective: Perform randomization for randomized block designs, evaluate assumptions, and conduct statistical analysis using ANOVA.\nUse R to conduct the randomization and statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your randomization and analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 60"
  },
  {
    "objectID": "assignments/assignment7_randomized_complete_block_design.html#instructions",
    "href": "assignments/assignment7_randomized_complete_block_design.html#instructions",
    "title": "Assignment 7: Randomized Complete Block Design",
    "section": "",
    "text": "Objective: Perform randomization for randomized block designs, evaluate assumptions, and conduct statistical analysis using ANOVA.\nUse R to conduct the randomization and statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your randomization and analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 60"
  },
  {
    "objectID": "assignments/assignment7_randomized_complete_block_design.html#exercise-1-randomized-complete-block-design",
    "href": "assignments/assignment7_randomized_complete_block_design.html#exercise-1-randomized-complete-block-design",
    "title": "Assignment 7: Randomized Complete Block Design",
    "section": "Exercise 1: Randomized Complete Block Design",
    "text": "Exercise 1: Randomized Complete Block Design\nData Source: Dean et al. (2017), Chapter 10, Exercise 1\nTask: Conduct a randomization for a randomized complete block design with:\n\n\\(v = 4\\) treatments,\n\\(b = 5\\) blocks,\nEach treatment is observed \\(s = 1\\) time in each block.\n\nDeliverables:\n\nShow the randomized treatment allocation within each block.\nProvide the final design in tabular format, with rows representing blocks and columns representing treatment allocations.\nInclude any R code used for the randomization. ## Exercise 2: Respiratory Exchange Ratio Experiment (Ex. 10.4)\n\nData Source: Dean et al. (2017), Chapter 10, Exercise 4\nScenario: In the resting metabolic rate experiment introduced in Example 10.4.1 (p. 311), the experimenters also measured the respiratory exchange ratio, another measure of energy expenditure. The data for the second 30 minutes of testing are provided below.\n\nThe data are from Example 10.4.1, but the respiratory exchange ratio data are used here.\n\n\nSubject\nProtocol 1\nProtocol 2\nProtocol 3\n\n\n\n\n1\n0.79\n0.80\n0.83\n\n\n2\n0.84\n0.84\n0.81\n\n\n3\n0.84\n0.93\n0.88\n\n\n4\n0.83\n0.85\n0.79\n\n\n5\n0.84\n0.78\n0.88\n\n\n6\n0.83\n0.75\n0.86\n\n\n7\n0.77\n0.76\n0.71\n\n\n8\n0.83\n0.85\n0.78\n\n\n9\n0.81\n0.77\n0.72\n\n\n\nTasks:\n\nEvaluate Assumptions:\n\nEvaluate the assumptions of the block-treatment model (Equation 10.4.1) for these data.\n\nANOVA Analysis:\n\nConstruct an analysis of variance (ANOVA) table.\nTest for equality of the effects of the protocols on the respiratory exchange ratio. Report your conclusions.\n\nEvaluate the usefulness of blocking.\n\nDeliverables:\n\nDiscuss the assumptions of the block-treatment model for the respiratory exchange ratio data.\nPresent the ANOVA table and results for the respiratory exchange ratio data.\nEvaluate the usefulness of blocking in this experiment.\nInclude any R code used for the analysis.\nProvide a clear explanation of the process and results."
  },
  {
    "objectID": "assignments/assignment7_randomized_complete_block_design.html#grading-rubric",
    "href": "assignments/assignment7_randomized_complete_block_design.html#grading-rubric",
    "title": "Assignment 7: Randomized Complete Block Design",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\n\n\n\n\n\nExercise\nCriteria\nPoints\n\n\n\n\nExercise 1\nCorrect randomized design and explanation\n20\n\n\nExercise 2\nAssumption evaluation and ANOVA results\n30\n\n\nClarity and R Code\nClearly explained process and R code used\n10"
  },
  {
    "objectID": "assignments/assignment9_analysis_of_covariance.html",
    "href": "assignments/assignment9_analysis_of_covariance.html",
    "title": "Assignment 9: Analysis of Covariance (ANCOVA)",
    "section": "",
    "text": "Objective: Explore the analysis of covariance (ANCOVA) and its applications in experimental designs.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 55"
  },
  {
    "objectID": "assignments/assignment9_analysis_of_covariance.html#instructions",
    "href": "assignments/assignment9_analysis_of_covariance.html#instructions",
    "title": "Assignment 9: Analysis of Covariance (ANCOVA)",
    "section": "",
    "text": "Objective: Explore the analysis of covariance (ANCOVA) and its applications in experimental designs.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 55"
  },
  {
    "objectID": "assignments/assignment9_analysis_of_covariance.html#exercise-7.11-antifungal-antibiotic-experiment",
    "href": "assignments/assignment9_analysis_of_covariance.html#exercise-7.11-antifungal-antibiotic-experiment",
    "title": "Assignment 9: Analysis of Covariance (ANCOVA)",
    "section": "Exercise 7.11: Antifungal Antibiotic Experiment",
    "text": "Exercise 7.11: Antifungal Antibiotic Experiment\nThis exercise investigates the effects of incubation conditions on the yield of an antifungal antibiotic.\n\nFactors and Levels:\nIncubation temperature (A): 25, 30, and 37 \\(^\\circ \\text{C}\\)\nCarbon concentration (B): 2%, 5%, and 7.5%\nNitrogen concentration (C): 0.5%, 1%, and 3%\n\nThe response variable is the antifungal yield (measured in activity against Candida albicans). Data are shown in Table 7.23.\nQuestions:\n\nAssess Main Effects Construct plots to assess the significance of main effects of \\(A\\), \\(B\\), and \\(C\\) on the response. What are your conclusions?\nInteraction Assumptions State the assumptions you made regarding interactions while analyzing main effects in part (1).\nTwo-Way Interactions Construct plots to assess the significance of two-way interactions. Do they alter your conclusions from part (1)?\nFit a Model Assuming no three-way interaction, fit a model with all main effects and two-way interactions. Discuss the significance of the effects and compare with your conclusions from part (3).\nModel Diagnostics\nEvaluate whether the assumptions of normality and equal error variances are satisfied. Identify potential outliers.\n\nTable 7.23: Data for Antifungal Antibiotic Experiment\n\n\n\nA\nB\nC\n\\(y_{ijk}\\)\n\n\n\n\n25\n2\n0.5\n25.84\n\n\n25\n2\n1\n51.86\n\n\n25\n2\n3\n32.59\n\n\n30\n5\n1\n41.11\n\n\n37\n7.5\n0.5\n51.86\n\n\n…\n…\n…\n…\n\n\n\nSource: Gupte and Kulkarni (2003), Journal of Chemical Technology and Biotechnology."
  },
  {
    "objectID": "assignments/assignment9_analysis_of_covariance.html#summary-table-of-grades-and-rubric",
    "href": "assignments/assignment9_analysis_of_covariance.html#summary-table-of-grades-and-rubric",
    "title": "Assignment 9: Analysis of Covariance (ANCOVA)",
    "section": "Summary Table of Grades and Rubric",
    "text": "Summary Table of Grades and Rubric\n\n\n\n\n\n\n\n\n\nExercise\nCriteria\nMax Points\nDescription\n\n\n\n\nExercise 7.11\nPart (a): Main Effects\n10\nConstruct main effects plots and evaluate their significance.\n\n\n\nPart (b): Interaction Assumptions\n5\nClearly state and justify interaction assumptions.\n\n\n\nPart (c): Two-Way Interactions\n15\nConstruct interaction plots, interpret results, and update conclusions from main effects analysis.\n\n\n\nPart (d): Model Fitting\n15\nFit a model including all main effects and two-way interactions, interpret significant effects.\n\n\n\nPart (e): Diagnostics\n10\nEvaluate residual diagnostics for normality, homoscedasticity, and outliers.\n\n\nTotal for Exercise 7.11\n\n55"
  },
  {
    "objectID": "data-files-book.html",
    "href": "data-files-book.html",
    "title": "Data Files for the Textbook",
    "section": "",
    "text": "These data files correspond to the book Design and Analysis of Experiments by Angela Dean, Dan Voss, and Daniel Draguljic (Springer-Verlag, 2017).\nClick on the links below to download the files.\n\nA\n\nabrasive.wear.txt\nair.freshener.txt\nair.rifle.txt\nair.velocity.txt\nair.velocity.contrasts.txt\nalcohol.txt\nammunition.txt\nanatase.txt\nantifungal.txt\n\n\n\nB\n\nballoon.txt\nbanana.txt\nbattery.txt\nbean.txt\nbeef.txt\nbicycle.txt\nbiscuit.txt\nbleach.txt\nbuttermilk.txt\n\n\n\nC\n\ncaffeine.txt\ncandle.txt\ncatalyst.txt\ncatalytic.reaction.txt\nchemical.txt\ncigarette.txt\nclean.wool.txt\ncoating.txt\ncoil.txt\ncolorfastness.txt\ncopper.txt\ncotton.spinning.txt\n\n\n\nD\n\ndairy.cow.txt\nDCIS.txt\ndecon.alpha.txt\ndecon.beta1.txt\ndecon.beta2.txt\ndessert.txt\ndetergent.txt\ndrill.advance.txt\ndrug.txt\ndye.txt\n\n\n\nE\n\neffervescent.txt\nexercise.bicycle.txt\n\n\n\nF\n\nfabric.stain.txt\nfield.txt\nfield2.txt\nfilm.viscosity.txt\nfishing.line.txt\nflour.txt\nflour.early.txt\nflour.production.txt\nfractionation.PCE.txt\nfractionation.yield.txt\n\n\n\nG\n\ngolf.ball.txt\ngolf.driver.txt\n\n\n\nH\n\nhandwheel.txt\nheart.lung.pump.txt\n\n\n\nI\n\nice.cream.txt\nice.melting.txt\ninclinometer.product.txt\ninjection.molding.txt\nink.txt\ninsole.cushion.txt\n\n\n\nL\n\nlactic.acid.txt\nlength.perception.txt\nlight.bulb.txt\nlithium.txt\nload.carrying.txt\n\n\n\nM\n\nmachine.head.txt\nmangold.txt\nmangold3.txt\nmargarine.txt\nMCFS71.txt\nMCFS71time.txt\nMCFS71x4.txt\nmeat.cooking.txt\nmemory.recall.txt\nmemory.txt\nmungbean.txt\n\n\n\nN\n\nnail.varnish.txt\nneuron.txt\n\n\n\nO\n\noats.txt\noperator.txt\n\n\n\nP\n\npah.txt\npaint.txt\npaint8.txt\npaint.followup.txt\npaper.towel.absorbency.txt\npaper.towel.strength.txt\npeas.txt\npenicillin.txt\nplasma.txt\npopcorn.micro.txt\nprojectile.txt\n\n\n\nQ\n\nquantity.perception.txt\n\n\n\nR\n\nrail.weld.txt\nreaction.time.txt\nred.blood.cell.txt\nrefinery.txt\nresin.maturity.txt\nresin.moisure.txt\nrespiratory.exchange.ratio.txt\nresting.metabolic.rate.txt\nrocket.txt\nrust.txt\n\n\n\nS\n\nsludge.txt\nsoap.txt\nsoup.txt\nspaghetti.sauce.txt\nspectometer.txt\nsteel.bar.txt\nstep.txt\nsugar.beet.txt\nsugar.beet3.txt\nsurvival.txt\nsystolic.blood.pressure.txt\n\n\n\nT\n\ntemperature.txt\ntool.coating.txt\ntrout.txt\n\n\n\nU\n\nuav.txt\nuav2sact.txt\nuav3.txt\nuav3adt.txt\n\n\n\nV\n\nvaccine.txt\nvideo.game.txt\nviscosity.txt\nvoltage.txt\n\n\n\nW\n\nwafer.txt\nwafer2.txt\nwater.boiling.txt\nwater.heating.txt\nwathering.txt\nweight.lifting.txt\nweld.strength.txt\nwildflower.txt\n\n\n\nY\n\nyeast.txt\n\n\n\nZ\n\nzinc.plating.txt",
    "crumbs": [
      "Home",
      "R Code and Datasets",
      "Datasets from the Book"
    ]
  },
  {
    "objectID": "home_assignments.html",
    "href": "home_assignments.html",
    "title": "📚 Assignments",
    "section": "",
    "text": "Important\n\n\n\nNote: Late submissions will not be accepted.",
    "crumbs": [
      "Home",
      "Course Materials",
      "Assignments"
    ]
  },
  {
    "objectID": "home_assignments.html#instructions",
    "href": "home_assignments.html#instructions",
    "title": "📚 Assignments",
    "section": "Instructions",
    "text": "Instructions\n\nVisit the Assignment Page: Click on the Assignments page to access the assignment list. Then, click on the assignment link you wish to complete.\nDownload the Assignment Template Quarto File: Download Assignment Template file and Assignment Instruction file. For the template file, right-click on page and select Save As, type the file name as assignmentX_yourlastname.qmd, and save it to your local machine.\n\nNote. Make sure the extension of the file is .qmd and not .txt.\n\nOpen in Quarto IDE: Use your preferred Quarto-supported IDE (e.g., RStudio, VS Code).\nComplete the Assignment: Follow the instructions provided for each assignment and complete the tasks.\n\nEsnure that you provide your name and the date at the top of the file in the YAML metadata section located at the beginning of the file (e.g., title, author).\nYou do not need to modify other parts of the YAML metadata.\n\nRender to PDF: Render the quartofile. The default output format is PDF.\n\nRStudio: Click on the Knit button.\nVS Code: Use the Quarto extension to render the PDF. Make sure the Quarto extension is installed.\n\nSubmit on Canvas: Upload the rendered PDF on Canvas under the corresponding assignment section.",
    "crumbs": [
      "Home",
      "Course Materials",
      "Assignments"
    ]
  },
  {
    "objectID": "home_assignments.html#assignment-list",
    "href": "home_assignments.html#assignment-list",
    "title": "📚 Assignments",
    "section": "Assignment List",
    "text": "Assignment List\nBelow is a list of all assignments for this course. Click on the links to access the assignment details and instructions. Ensure that you have the necessary software (e.g., Quarto, R, RStudio, or VS Code) to render and complete the assignments.\n\nAssignment 1: Introduction to Experimental Design\nDue Date: February 6, 2025\nAssignment 2: Completely Randomized Design (CRD)\nDue Date: February 13, 2025\nAssignment 3: Multiple Comparisons and Contrasts\nDue Date: February 20, 2025\nAssignment 4: Checking Model Assumptions\nDue Date: February 27, 2025\nAssignment 5: Two-Factor ANOVA\nDue Date: March 6, 2025\nAssignment 6: Higher-Order Factorial Designs\nDue Date: March 25, 2025\nAssignment 7: Randomized Complete Block Design (RCBD)\nDue Date: April 3, 2025\nAssignment 8: Complete Block Designs\nDue Date: April 10, 2025\nAssignment 9: Analysis of Covariance (ANCOVA)\nDue Date: April 17, 2025\nAssignment 10: Row-Column (Latin Square) Designs\nDue Date: April 24, 2025\nAssignment 11: Random and Mixed Effects Models\nDue Date: May 1, 2025\nAssignment 12: Nested Models\nDue Date: May 6, 2025\nAssignment 13: Split-Plot Designs\nDue Date: May 13, 2025",
    "crumbs": [
      "Home",
      "Course Materials",
      "Assignments"
    ]
  },
  {
    "objectID": "home_assignments.html#need-help",
    "href": "home_assignments.html#need-help",
    "title": "📚 Assignments",
    "section": "Need Help?",
    "text": "Need Help?\n\nRefer to the Course GitHub Repository for additional resources and examples.\nPost questions on the Discussion Forum on Canvas.\nSchedule a meeting with the instructor for further clarification.",
    "crumbs": [
      "Home",
      "Course Materials",
      "Assignments"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Instructor: 📚 Davood Tofighi, Ph.D.\n\nLocation: 📍 Science Math Learning Center, Room 120\n\nClass Times: 🕒 Tuesdays and Thursdays, 2:00 PM - 3:15 PM\n\nOffice Hours: 📅 Thursday, 3:30 pm - 4:30 pm\nCourse Website: UNM Canvas\n\nCourse GitHub Page: 🌐 Course GitHub Page\n\n\n\nThis course introduces key principles of experimental design and analysis of variance (ANOVA) integrated with regression models. Key topics include:\n\nRandomization, replication, and blocking principles\n\nOne-way and factorial ANOVA models\n\nManaging unbalanced data and covariates\nAdvanced designs: Latin square, split-plot, nested, and mixed-effects models\n\nDiagnostics: residual analysis, transformations, and multiple comparisons\n\n\n\n\n\nPrimary Textbook:\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nAccess the eBook via Springer eBooks.\n\nSupplemental Readings:\n\nMaterials in the public domain will be shared on the Contents page of the course GitHub site.\nNote: Some readings may not be shared due to copyright restrictions.\n\n\n\n\n\n\n📄 Syllabus: Full Course Syllabus (PDF)\n\n📝 Lecture Notes: Available weekly on the Course GitHub Page\n📅 Course Schedule: View the Full Schedule\n📚 Assignments: View All Assignments\n\nAssignments: To access the assignment details and instructions, click on the links provided in the Assignments section.\n\n\n\n\n\n\nPlease Read the Syllabus\n\n\n\nThis page provides an overview of the course, but not all critical information is listed here. You are required to read the full syllabus for policies, grading details, and due dates.\n\n\n\n\n\n\n\n\nGitHub Cache Issue\n\n\n\nGitHub caches pages, so refresh your browser to access the latest content:\n- Windows/Linux: Press Ctrl + Shift + R\n- Mac: Press Cmd + Shift + R\nFor persistent issues, clear your browser cache or use an incognito window.\n\n\n\nAssignments: Posted and submitted through Canvas Assignments.\n🛠️ Software Tools:\n\nR Project Download\n\nRStudio IDE\n\nVS Code Editor\n\n\n\n\n\n\n\n\nComponent\nWeight\n\n\n\n\n🏠 Homework Assignments\n40%\n\n\n📊 Midterm Exam\n20%\n\n\n🏁 Final Exam\n30%\n\n\nQuiz\n10%\n\n\n⭐ Extra Credit\nUp to 4%\n\n\n\n\n\n\n\nMidterm Exam:\n\n📅 Date: Thursday, March 6, 2025\n\n📚 Coverage: Topics from Weeks 1-7\n\nFinal Exam:\n\n📅 Date: Tuesday, May 13, 2025, 10:00 AM - 12:00 PM\n\n📚 Coverage: Comprehensive (Weeks 1-15)\n\n\n\n\n\n🚫 No make-up exams unless in the case of proven emergencies as outlined in the syllabus and UNM policies.\n\n📝 Notify the instructor before the exam date if a conflict arises.\n\n📄 Exams are closed book, but you are allowed:\n\nUp to three double-sided cheat sheets (US Letter size).\n\nA calculator (no cell phones or other electronics).\n\n\n\n\n\n\n\n🔔 Announcements:\nAll course updates will be posted on Canvas. Check regularly.\n📬 Questions:\n\nPost general questions on the Canvas Discussion Board.\n\nFor personal matters, contact the instructor via Canvas Email.\n\n\n\n\n\n\nAccessibility Services\nContact Accessibility Resource Center at arcsrvs@unm.edu or call 277-3506.\nUNM Policies\n\nStudent Code of Conduct\n\nAcademic Honesty Policy\n\n\nTitle IX Resources\nVisit the Office of Equal Opportunity for Title IX-related support.\n\n\n\n\nFor personal matters, email the instructor via Canvas Email.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "This course introduces key principles of experimental design and analysis of variance (ANOVA) integrated with regression models. Key topics include:\n\nRandomization, replication, and blocking principles\n\nOne-way and factorial ANOVA models\n\nManaging unbalanced data and covariates\nAdvanced designs: Latin square, split-plot, nested, and mixed-effects models\n\nDiagnostics: residual analysis, transformations, and multiple comparisons",
    "crumbs": [
      "Home",
      "Course Overview",
      "Home"
    ]
  },
  {
    "objectID": "index.html#required-textbook-and-materials",
    "href": "index.html#required-textbook-and-materials",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Primary Textbook:\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nAccess the eBook via Springer eBooks.\n\nSupplemental Readings:\n\nMaterials in the public domain will be shared on the Contents page of the course GitHub site.\nNote: Some readings may not be shared due to copyright restrictions.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Home"
    ]
  },
  {
    "objectID": "index.html#key-course-links",
    "href": "index.html#key-course-links",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "📄 Syllabus: Full Course Syllabus (PDF)\n\n📝 Lecture Notes: Available weekly on the Course GitHub Page\n📅 Course Schedule: View the Full Schedule\n📚 Assignments: View All Assignments\n\nAssignments: To access the assignment details and instructions, click on the links provided in the Assignments section.\n\n\n\n\n\n\nPlease Read the Syllabus\n\n\n\nThis page provides an overview of the course, but not all critical information is listed here. You are required to read the full syllabus for policies, grading details, and due dates.\n\n\n\n\n\n\n\n\nGitHub Cache Issue\n\n\n\nGitHub caches pages, so refresh your browser to access the latest content:\n- Windows/Linux: Press Ctrl + Shift + R\n- Mac: Press Cmd + Shift + R\nFor persistent issues, clear your browser cache or use an incognito window.\n\n\n\nAssignments: Posted and submitted through Canvas Assignments.\n🛠️ Software Tools:\n\nR Project Download\n\nRStudio IDE\n\nVS Code Editor",
    "crumbs": [
      "Home",
      "Course Overview",
      "Home"
    ]
  },
  {
    "objectID": "index.html#grading-and-assessments",
    "href": "index.html#grading-and-assessments",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Component\nWeight\n\n\n\n\n🏠 Homework Assignments\n40%\n\n\n📊 Midterm Exam\n20%\n\n\n🏁 Final Exam\n30%\n\n\nQuiz\n10%\n\n\n⭐ Extra Credit\nUp to 4%",
    "crumbs": [
      "Home",
      "Course Overview",
      "Home"
    ]
  },
  {
    "objectID": "index.html#exams-and-policies",
    "href": "index.html#exams-and-policies",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Midterm Exam:\n\n📅 Date: Thursday, March 6, 2025\n\n📚 Coverage: Topics from Weeks 1-7\n\nFinal Exam:\n\n📅 Date: Tuesday, May 13, 2025, 10:00 AM - 12:00 PM\n\n📚 Coverage: Comprehensive (Weeks 1-15)\n\n\n\n\n\n🚫 No make-up exams unless in the case of proven emergencies as outlined in the syllabus and UNM policies.\n\n📝 Notify the instructor before the exam date if a conflict arises.\n\n📄 Exams are closed book, but you are allowed:\n\nUp to three double-sided cheat sheets (US Letter size).\n\nA calculator (no cell phones or other electronics).",
    "crumbs": [
      "Home",
      "Course Overview",
      "Home"
    ]
  },
  {
    "objectID": "index.html#communication",
    "href": "index.html#communication",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "🔔 Announcements:\nAll course updates will be posted on Canvas. Check regularly.\n📬 Questions:\n\nPost general questions on the Canvas Discussion Board.\n\nFor personal matters, contact the instructor via Canvas Email.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Home"
    ]
  },
  {
    "objectID": "index.html#support-and-resources",
    "href": "index.html#support-and-resources",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Accessibility Services\nContact Accessibility Resource Center at arcsrvs@unm.edu or call 277-3506.\nUNM Policies\n\nStudent Code of Conduct\n\nAcademic Honesty Policy\n\n\nTitle IX Resources\nVisit the Office of Equal Opportunity for Title IX-related support.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Home"
    ]
  },
  {
    "objectID": "index.html#contact-information",
    "href": "index.html#contact-information",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "For personal matters, email the instructor via Canvas Email.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Home"
    ]
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html",
    "href": "lectures/week-01_intro-design_part2.html",
    "title": "Planning Experiments",
    "section": "",
    "text": "Experimental design forms the foundation of scientific inquiry whenever the goal is to understand how certain controllable inputs (factors) influence outcomes (responses). By carefully planning how to collect data and assigning treatments to experimental units, we can draw valid causal inferences about these relationships. Unlike observational studies, experiments involve actively manipulating factors to reveal their effects while controlling for unwanted variability.\nThis set of lecture notes provides a comprehensive overview of experimental design concepts and applications. We will address how to define clear objectives, classify sources of variation, choose appropriate experimental designs, apply randomization, incorporate blocking, specify statistical models, and determine appropriate sample sizes. Real-world examples and R code snippets will illustrate key ideas. Additionally, we will integrate exercises, class activities, summaries, challenges, and visual aids to reinforce understanding."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#planning-experiments",
    "href": "lectures/week-01_intro-design_part2.html#planning-experiments",
    "title": "Planning Experiments",
    "section": "",
    "text": "Experimental design forms the foundation of scientific inquiry whenever the goal is to understand how certain controllable inputs (factors) influence outcomes (responses). By carefully planning how to collect data and assigning treatments to experimental units, we can draw valid causal inferences about these relationships. Unlike observational studies, experiments involve actively manipulating factors to reveal their effects while controlling for unwanted variability.\nThis set of lecture notes provides a comprehensive overview of experimental design concepts and applications. We will address how to define clear objectives, classify sources of variation, choose appropriate experimental designs, apply randomization, incorporate blocking, specify statistical models, and determine appropriate sample sizes. Real-world examples and R code snippets will illustrate key ideas. Additionally, we will integrate exercises, class activities, summaries, challenges, and visual aids to reinforce understanding."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#objectives",
    "href": "lectures/week-01_intro-design_part2.html#objectives",
    "title": "Planning Experiments",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand how to identify and classify sources of variation in an experiment.\nDefine treatment factors, factor levels, and experimental units.\nExplore blocking, nuisance factors, and strategies to manage them.\nGrasp the principles of randomization and why it underpins statistical inference.\nExamine standard experimental designs (CRD, RBD, Factorial, Latin Square, Split-Plot) and when to use each.\nUnderstand the distinction between fixed and random effects in modeling.\nLearn how to plan data collection, run pilot experiments, and determine the number of observations (sample size).\nGain hands-on experience using R code for randomization, exploratory data analysis, and basic ANOVA.\n\n\nReadings\n\nDean et al. (2017, Ch. 3)"
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#steps-in-planning-an-experiment",
    "href": "lectures/week-01_intro-design_part2.html#steps-in-planning-an-experiment",
    "title": "Planning Experiments",
    "section": "Steps in Planning an Experiment",
    "text": "Steps in Planning an Experiment\nIt is very tempting to jump into data collection without a clear plan. However, a well-thought-out experimental design is crucial for obtaining valid and interpretable results. Here is a checklist of steps to guide you through the planning process:\n\n1. Define the Objectives of the Experiment\n\nClearly state the goals (e.g., comparing treatments, optimizing factors, studying interactions).\nWrite objectives as specific questions or hypotheses to guide the design.\n\n\n\n2. Identify All Sources of Variation\n\nTreatment Factors and Their Levels\n\nFactors actively manipulated (e.g., temperature, dosage) and their specific levels.\nExample: Dosage levels: 10 mg, 20 mg, 30 mg.\n\nExperimental Units\n\nSmallest division of material receiving a treatment independently.\nExample: Individual plants in a field trial.\n\nBlocking Factors, Noise Factors, and Covariates\n\nBlocking: Group similar units (e.g., day of testing).\nNoise: Uncontrollable variability (e.g., weather conditions).\nCovariates: Measureable properties (e.g., baseline performance).\n\n\n\n\n\n3. Choose a Rule for Assigning Experimental Units to Treatments\n\nSelect randomization techniques:\n\nCompletely Randomized Design (CRD).\nRandomized Block Design (RBD).\nSplit-Plot Design.\n\n\n\n\n4. Specify the Measurements, Procedure, and Anticipated Difficulties\n\nMeasurements\n\nSpecify precision, units, and frequency of measurement.\nExample: Growth in cm, tensile strength in MPa.\n\nProcedure\n\nStep-by-step instructions for consistent data collection.\n\nAnticipated Difficulties\n\nIdentify challenges (e.g., equipment failure) and mitigation strategies.\n\n\n\n\n\n5. Run a Pilot Experiment\n\nTest feasibility, validate instruments, and refine factor levels or treatment combinations.\n\n\n\n6. Specify the Statistical Model\n\nExample for one-way ANOVA: \\[\nY_{ij} = \\mu + \\tau_i + \\varepsilon_{ij},\n\\]\n\nwhere:\n\n\\(Y_{ij}\\): Observed response.\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect.\n\\(\\varepsilon_{ij}\\): Random error term.\n\n\n\n\n7. Outline the Analysis\n\nPlan descriptive and inferential methods (e.g., ANOVA, regression).\nInclude diagnostic checks for model assumptions.\n\n\n\n8. Calculate Sample Size\n\nPerform power analysis based on variability estimates and desired precision.\n\n\n\n9. Review and Revise\n\nRevisit and refine all decisions based on pilot results and practical constraints."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#flowchart-of-experimental-design-steps",
    "href": "lectures/week-01_intro-design_part2.html#flowchart-of-experimental-design-steps",
    "title": "Planning Experiments",
    "section": "Flowchart of Experimental Design Steps",
    "text": "Flowchart of Experimental Design Steps\n\n\n\n\n\n\n\n\nExperimentalDesign\n\n\ncluster_planning\n\nPlanning\n\n\ncluster_implementation\n\nImplementation\n\n\ncluster_analysis\n\nAnalysis\n\n\ncluster_iteration\n\nIteration\n\n\n\nDefine Objectives\n\nDefine Objectives\n\n\n\nIdentify Sources of Variation\n\nIdentify Sources of Variation\n\n\n\nDefine Objectives-&gt;Identify Sources of Variation\n\n\n\n\n\nChoose Assignment Rules\n\nChoose Assignment Rules\n\n\n\nIdentify Sources of Variation-&gt;Choose Assignment Rules\n\n\n\n\n\nSpecify Measurements & Procedure\n\nSpecify Measurements & Procedure\n\n\n\nChoose Assignment Rules-&gt;Specify Measurements & Procedure\n\n\n\n\n\nIdentify Anticipated Difficulties\n\nIdentify Anticipated Difficulties\n\n\n\nSpecify Measurements & Procedure-&gt;Identify Anticipated Difficulties\n\n\n\n\n\nRun Pilot Experiment\n\nRun Pilot Experiment\n\n\n\nIdentify Anticipated Difficulties-&gt;Run Pilot Experiment\n\n\n\n\n\nSpecify Statistical Model\n\nSpecify Statistical Model\n\n\n\nRun Pilot Experiment-&gt;Specify Statistical Model\n\n\n\n\n\nOutline Analysis\n\nOutline Analysis\n\n\n\nSpecify Statistical Model-&gt;Outline Analysis\n\n\n\n\n\nCalculate Sample Size\n\nCalculate Sample Size\n\n\n\nOutline Analysis-&gt;Calculate Sample Size\n\n\n\n\n\nReview and Revise\n\nReview and Revise\n\n\n\nCalculate Sample Size-&gt;Review and Revise\n\n\n\n\n\nReview and Revise-&gt;Define Objectives\n\n\n\n\n\nReview and Revise-&gt;Identify Sources of Variation\n\n\n\n\n\n\n\n\nFigure 1: Experimental Design Planning Flowchart"
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#identifying-experiment-objectives-and-variation-sources",
    "href": "lectures/week-01_intro-design_part2.html#identifying-experiment-objectives-and-variation-sources",
    "title": "Planning Experiments",
    "section": "Identifying Experiment Objectives and Variation Sources",
    "text": "Identifying Experiment Objectives and Variation Sources\n\nSetting Objectives\nEvery experiment must start with a clear research objective. Ask: What do we hope to learn or achieve? Objectives may include:\n\nDetermining which manufacturing process leads to the highest product quality.\nComparing new teaching methods for improved student performance.\nFinding the optimal settings of machine parameters to reduce defects.\n\nWithout well-defined objectives, the experiment risks being unfocused, making the results difficult to interpret.\n\n\nVariation Sources\nVariation in responses can arise from multiple sources:\n\nMajor Variation (Treatment Effects): Due to the factors of interest (e.g., different fertilizers or drug dosages).\nNuisance Factors (noise): Variables that influence the response but are not of primary interest (e.g., room temperature, operator skill). Some nuisance factors can be controlled or incorporated as blocking factors, while others must be accepted as noise.\n\nBy identifying and classifying sources of variation, we can decide which factors to manipulate as treatments, which to fix or control, and which to block or treat as covariates.\n\n\nExample\n\n\n\n\n\n\nFigure 2: Variation Sources in an Experiment\n\n\n\nConsider an agricultural experiment testing different fertilizer types (A, B, C). The main factors are the fertilizer types. Soil heterogeneity could be a nuisance factor. If the field has varying soil quality, it might mask the fertilizer effect. Recognizing this early allows us to incorporate blocking or other strategies to handle this nuisance variation.\n\n\nActivity (Work Out Experiment): Think-Pair-Share\n\nThink: Imagine you are testing two workout plans (high-intensity vs. moderate-intensity) to improve muscle strength.\nPair: Discuss with a partner what the main objective is and what factors could cause unwanted variation (e.g., baseline fitness level, diet).\nShare: Volunteer pairs share their classification of factors and whether they could be controlled or blocked.\n\n\n\nChallenges and Common Mistakes\n\nStarting without a clear objective can lead to inconclusive results.\nConfusing major variation with nuisance variation can misdirect the design.\n\n\n\nExercises\n\nWhy must objectives be clear before starting an experiment?\nGiven three fertilizer types and two soil conditions, identify which factors should be considered treatments and which might be nuisance factors.\nExplain how failing to control a nuisance factor increases the variance of treatment effect estimates.\n\nKey Takeaways:\n\nWell-defined objectives shape the experiment.\nClassifying factors into treatment and nuisance categories enhances design efficiency."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#treatment-factors-and-levels",
    "href": "lectures/week-01_intro-design_part2.html#treatment-factors-and-levels",
    "title": "Planning Experiments",
    "section": "Treatment Factors and Levels",
    "text": "Treatment Factors and Levels\n\nDefining Treatment Factors\nA treatment factor is any variable deliberately manipulated by the experimenter to study its effect on the response.\nExample: In a detergent study, water temperature and detergent concentration are treatment factors.\n\n\nFactor Levels\nFactor levels are the specific settings or categories of a factor. For instance, twist levels in a cotton-spinning experiment might be 1.63, 1.69, 1.78, and 1.90 turns per inch. Chosen based on subject matter knowledge, factor levels must be realistic and relevant.\n\n\nActivity (Baking Experiment): Design Your Own Factorial Experiment\nIndividually pick a factor (e.g., cooking time) and propose three levels. Pair up with another student who has chosen a different factor (e.g., oven temperature), and combine them into a factorial design. Discuss the complexity and the number of total treatment combinations.\n\n\nChallenges and Common Mistakes\n\nChoosing impractical or irrelevant factor levels can undermine the experiment’s usefulness.\nToo many factor levels complicate analysis without necessarily adding insight.\n\n\n\nExercises\n\nWhat criteria guide the selection of factor levels?\nFor a factor with 4 levels and another with 3 levels, determine how many treatment combinations exist.\nExplain how replicates per factor level (with total resources fixed) decreases the variance of treatment effect estimates.\n\nKey Takeaways: - Treatments factors are deliberately manipulated variables. - Factor levels must be meaningful, informed by real-world considerations or pilot data."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#experimental-units-blocking-and-nuisance-factors",
    "href": "lectures/week-01_intro-design_part2.html#experimental-units-blocking-and-nuisance-factors",
    "title": "Planning Experiments",
    "section": "Experimental Units, Blocking, and Nuisance Factors",
    "text": "Experimental Units, Blocking, and Nuisance Factors\n\nExperimental Units\nThe experimental unit is the smallest entity to which treatments are independently assigned. This concept is crucial for valid statistical inference.\n\n\nExamples of Experimental Units\n\nPlants in a field trial if each plant gets a different fertilizer.\nEngine test benches if each engine is run under a unique setup.\nStudents in a classroom if each student receives a different teaching method.\nPatients in a clinical trial if each patient gets a different drug.\n\n\n\nBlocking Factors\nWhen a known nuisance factor could confound results, use blocking. Blocking forms groups of similar experimental units, each block receiving all treatments. This controls for block-to-block variation, increasing precision.\nEquation (Block Design):\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_j + \\varepsilon_{ij}\n\\] {#eq:block-design}\nHere, \\(\\tau_i\\) is the treatment effect and \\(\\beta_j\\) the block effect.\n\n\nNoise Factors and Covariates\n\nNoise Factors: Uncontrollable variations like daily temperature fluctuations.\nCovariates: Measured continuous variables (e.g., initial weight) that help explain variation when included in the model.\n\n\n\nFactor Classification Flowchart\n\n\n\n\n\n\nflowchart TD\n    A[All Factors]\n    B[Interested in studying the effect?]\n    C[Treatment Factors]\n    D[Nuisance Factors]\n    E[Can we group by factor levels?]\n    F[Block Factor]\n    G[Noise/Uncontrolled Factor]\n\n    A --&gt; B\n    B --&gt;|Yes| C\n    B --&gt;|No| D\n    D --&gt; E\n    E --&gt;|Yes| F\n    E --&gt;|No| G\n\n\n\n\nFigure 3: Factor Classification Flowchart\n\n\n\n\n\n\n\nActivity: Blocking Brainstorm\nConsider a baking experiment where ovens differ in temperature calibration. Could ovens form blocks? Discuss how blocking might improve detection of differences in recipes.\n\n\nChallenges and Common Mistakes\n\nConfusing the experimental unit with observational units leads to incorrect analysis.\nIgnoring a known source of variation that could be blocked may inflate residual error.\n\n\n\nExercises\n\nConceptual: Differentiate between experimental and observational units.\nNumerical: If you have 4 treatments and 6 blocks (each block gets all 4 treatments), how many total experimental units are there?\nProof/Derivation: Show that including a block factor in the model reduces the residual variance compared to an unblocked design.\n\nKey Takeaways:\n\nCorrectly identifying experimental units is essential.\nBlocking controls known nuisance factors, improving the experiment’s power to detect treatment effects."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#principles-of-randomization",
    "href": "lectures/week-01_intro-design_part2.html#principles-of-randomization",
    "title": "Planning Experiments",
    "section": "Principles of Randomization",
    "text": "Principles of Randomization\n\nRandomization Concept\nRandomization ensures that each experimental unit has an equal chance of receiving any treatment. This prevents systematic bias and justifies the assumptions underlying standard statistical tests.\nExample: Use a random number generator to assign treatments to plots, ensuring no predetermined pattern.\n\n\nHow to Perform Randomization\nRandomization ensures unbiased allocation of experimental units to treatments and reduces the risk of systematic error. Here’s how you can perform randomization in practice:\n\nProcedure\n\nList Experimental Units:\n\nCreate a list of all experimental units (e.g., subjects, plots of land).\n\nList Treatments:\n\nSpecify the treatments or factor levels to be assigned.\n\nDetermine Design Type:\n\nChoose a design type: Completely Randomized Design (CRD), Randomized Block Design (RBD), or Split-Plot Design.\n\nGenerate Random Numbers:\n\nUse random number generation to assign treatments to units.\n\nAssign Treatments:\n\nPair random numbers with treatments to allocate them to experimental units.\n\nVerify:\n\nEnsure assignments are correct and check for any unintended patterns.\n\n\n\n\n\nPseudocode for Randomization\n\nCompletely Randomized Design\nINPUT: List of experimental units, List of treatments\nOUTPUT: Randomized assignments of treatments to units\n\n1. Define N = Total number of experimental units\n2. Define T = List of treatments\n3. Repeat T to match the number of experimental units if unbalanced\n4. Shuffle the list of treatments randomly\n5. Assign shuffled treatments to the experimental units\n6. Return randomized assignments\n\n\nRandomized Block Design\nINPUT: List of blocks, List of treatments\nOUTPUT: Randomized assignments within each block\n\n1. For each block in blocks:\n    a. Shuffle the list of treatments randomly\n    b. Assign shuffled treatments to experimental units in the block\n2. Combine all block-level assignments\n3. Return randomized assignments\n\n\n\nR Implementation: Randomization Examples\n\n1. Completely Randomized Design\n\n\n\nN &lt;- 12 # Number of experimental units, Step 1\n# Define experimental units and treatments\nexperimental_units &lt;- 1:N ## 10 experimental units\ntreatments &lt;- c(\"A\", \"B\", \"C\") ## 3 treatments, Step 2\n\n# Repeat treatments to match the number of units. The code covers both balanced and unbalanced designs.\ntreatments &lt;- rep(treatments, length.out = N)  ## Step 3\n\n# For a balanced design, we can also use:\n# treatments &lt;- rep(treatments, each = N/length(treatments))\n\n# Shuffle (randomize) treatments randomly to assign to units, Step 4\nset.seed(123) ## For reproducibility\nrandomized_treatments &lt;- sample(treatments)\n\n# Create a data frame with randomized assignments\nrandomization_crd &lt;- data.frame(Unit = experimental_units, Treatment = randomized_treatments)  ## Step 5\nprint(randomization_crd)\n\n   Unit Treatment\n1     1         C\n2     2         C\n3     3         A\n4     4         B\n5     5         C\n6     6         B\n7     7         B\n8     8         A\n9     9         C\n10   10         B\n11   11         A\n12   12         A\n\n\n\nTable 1: Randomized Assignments in CRD\n\n\n\n\n\n2. Randomized Block Design\n\n\n\n# Define blocks and treatments\nblocks &lt;- 1:4 # 4 blocks\ntreatments &lt;- c(\"A\", \"B\", \"C\") # 3 treatments\n\n# Create a data frame to store results\nrandomization_rbd &lt;- data.frame(Block = integer(), Unit = integer(), Treatment = character())\n\n# Randomize within each block\nset.seed(123)\nfor (block in blocks) {\n    # Randomize treatments\n    randomized_treatments &lt;- sample(treatments)\n\n    # Assign treatments to units within the block\n    block_data &lt;- data.frame(\n        Block = block,\n        Unit = seq_len(length(treatments)),\n        Treatment = randomized_treatments\n    )\n    randomization_rbd &lt;- rbind(randomization_rbd, block_data)\n}\n\nprint(randomization_rbd)\n\n   Block Unit Treatment\n1      1    1         C\n2      1    2         A\n3      1    3         B\n4      2    1         B\n5      2    2         A\n6      2    3         C\n7      3    1         B\n8      3    2         C\n9      3    3         A\n10     4    1         A\n11     4    2         B\n12     4    3         C\n\n\n\nTable 2: Randomized Assignments in RBD\n\n\n\n\n\n\nExplanation in R\n\nrep(): Repeats the treatment list to match the total number of experimental units.\nsample(): Randomizes the order of the treatments.\nset.seed(): Ensures reproducibility of the randomization process.\nfor loop (RBD): Iterates through blocks, randomizing treatments for each.\n\nThese R scripts provide easy-to-follow approaches to randomization, ensuring balanced and unbiased treatment assignment. Let me know if you’d like further assistance with advanced designs!\n\n\n\nSee the help page for rep() in the Appendix for more details.\nSee the help page for sample() in the Appendix for more details.\n\n\n\nR Example: Simple Random Assignment\n\nset.seed(123)\ntreatments &lt;- rep(c(\"A\", \"B\", \"C\"), each = 4)\nassignments &lt;- sample(treatments)\nassignments\n\n [1] \"A\" \"C\" \"C\" \"A\" \"B\" \"C\" \"B\" \"A\" \"C\" \"B\" \"A\" \"B\"\n\n\n\n\nActivity: Randomization Drill\nWrite down three treatments on slips of paper and randomly assign them to hypothetical units. Discuss how randomization prevents biased placement of favorable treatments.\n\n\nChallenges and Common Mistakes\n\nAssigning treatments alphabetically or in a patterned manner is not randomization.\nRandomization must be deliberate, not haphazard.\n\n\n\nExercises\n\nWhy is randomization crucial for valid statistical inference?\nWith 12 units and 3 treatments, show one method of random assignment.\nUnder randomization and the null hypothesis, show that the expected difference between treatment means is zero.\n\nKey Takeaways:\n\nRandomization removes systematic bias.\nIt underpins the validity of ANOVA and other inferential methods."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#standard-experimental-designs",
    "href": "lectures/week-01_intro-design_part2.html#standard-experimental-designs",
    "title": "Planning Experiments",
    "section": "Standard Experimental Designs",
    "text": "Standard Experimental Designs\n\nCompletely Randomized Design (CRD)\n\nNo blocking, treatments assigned entirely at random.\nSuitable when units are homogeneous.\n\n\n\nRandomized Block Design (RBD)\n\nIntroduce a blocking factor to control known variation.\nEach block receives all treatments, improving the precision of treatment comparisons.\n\n\n\nRow-Column and Latin Squares\n\nControl for two perpendicular nuisance factors simultaneously.\n\n\n\nFactorial Designs\n\nStudy multiple factors and their interactions simultaneously.\nEfficiently explore how factors work together.\n\n\n\nSplit-Plot Designs\n\nUseful when some factors are harder to change than others.\nCreates a hierarchy of experimental units (e.g., fields as main plots and subplots within fields).\n\n\n\nActivity: Design Match-Up\nGiven three scenarios (no nuisance factor, known nuisance factor, and multiple factors with a complex structure), choose the appropriate design (CRD, RBD, Factorial, Latin Square).\n\n\nChallenges and Common Mistakes\n\nUsing a CRD when a known nuisance factor should be blocked.\nIgnoring interactions in factorial experiments.\n\n\n\nExercises\n\nConceptual: Under what conditions would you choose a CRD over an RBD?\nNumerical: Compute the number of runs in a 2x4 factorial with 3 replicates.\nProof/Derivation: Show how the variance decomposition changes from CRD to RBD, highlighting the block effect.\n\nKey Takeaways:\n\nDifferent designs address different research needs.\nFactorial designs detect interactions, blocking improves precision, and split-plots handle difficult-to-change factors."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#model-specification-and-effect-types",
    "href": "lectures/week-01_intro-design_part2.html#model-specification-and-effect-types",
    "title": "Planning Experiments",
    "section": "Model Specification and Effect Types",
    "text": "Model Specification and Effect Types\n\nLinear Models and ANOVA\nExperiments often use a linear model to relate responses to treatments and blocks:\n\\[\nY = \\mu + \\text{treatment effects} + \\text{block effects} + \\text{error}\n\\qquad(1)\\]\nANOVA decomposes total variation into components attributable to treatments, blocks, and error, enabling inference on treatment effects.\n\n\nFixed vs. Random Effects\n\nFixed Effects: Chosen levels of interest; inferences apply only to those tested levels.\nRandom Effects: Levels are a random sample from a broader population; inferences generalize beyond observed levels.\n\n\n\nActivity: Fixed or Random?\nConsider an experiment with 3 specific fertilizer brands (likely fixed) vs. an experiment using 3 randomly chosen brands from a large market (random). Discuss how interpretation changes.\n\n\nChallenges and Common Mistakes\n\nTreating random factors as fixed or vice versa leads to incorrect conclusions.\nConfusing the meaning of random effects with randomization.\n\n\n\nExercises\n\nConceptual: Why is it important to distinguish fixed and random effects?\nNumerical: In an RBD, show how expected mean squares differ for a random block factor vs. a fixed block factor.\nProof/Derivation: Derive the expected mean squares for a two-way ANOVA with one random factor.\n\nKey Takeaways:\n\nCorrect model specification ensures valid inference.\nFixed vs. random distinction affects interpretation and generalized conclusions."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#planning-data-collection-and-pilot-experiments",
    "href": "lectures/week-01_intro-design_part2.html#planning-data-collection-and-pilot-experiments",
    "title": "Planning Experiments",
    "section": "Planning Data Collection and Pilot Experiments",
    "text": "Planning Data Collection and Pilot Experiments\n\nMeasurement and Procedure\nCareful data collection planning ensures relevance and quality:\n\nChoose appropriate measurement scales and ensure instruments are calibrated.\nTrain personnel to reduce measurement errors.\n\n\n\nPilot Experiments\nA small-scale pilot run can identify unforeseen difficulties, refine factor levels, and confirm data collection procedures before launching the main study.\n\n\nActivity: Pilot Study Discussion\nDiscuss what a pilot experiment might reveal in a crop study (e.g., unexpected soil pests) and how that influences the main experiment’s design.\n\n\nChallenges and Common Mistakes\n\nOmitting a pilot study can lead to costly mistakes in the main experiment.\nRelying on untested procedures risks invalid data collection.\n\n\n\nExercises\n\nWhy are pilot experiments beneficial?\nIf pilot data suggest variance = 4 and you want a margin of error = 1, how many samples per treatment are needed (assuming normality)?\nShow how a pilot-based variance estimate informs sample size calculations.\n\nKey Takeaways:\n\nPlanning and pilot testing prevent wasted resources.\nWell-designed procedures and preliminary runs ensure high-quality, interpretable data."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#determining-the-number-of-observations",
    "href": "lectures/week-01_intro-design_part2.html#determining-the-number-of-observations",
    "title": "Planning Experiments",
    "section": "Determining the Number of Observations",
    "text": "Determining the Number of Observations\n\nSample Size and Power\nThe number of observations affects the experiment’s ability to detect true effects:\n\nPower: Probability of detecting a true effect if it exists.\nLarger sample size generally increases power but also increases cost.\n\nPower calculations balance desired precision, variance estimates, and available resources.\n\n\nActivity: Sample Size Calculation\nGiven an estimated variance and a desired effect size, estimate the required sample size. Discuss trade-offs between resource constraints and statistical power.\n\n\nChallenges and Common Mistakes\n\nChoosing sample size arbitrarily can lead to low power or wasted resources.\nOverly large samples may be unnecessary and expensive.\n\n\n\nExercises\n\nConceptual: Explain why sample size must be justified.\nNumerical: Given effect size = 2 (SD=3) and desired power = 0.8, calculate required sample size per group.\nProof/Derivation: Derive a basic formula relating power, effect size, and sample size for a one-way ANOVA.\n\nKey Takeaways:\n\nAdequate sample size ensures meaningful, reliable conclusions.\nPower analysis guides optimal resource allocation."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#practical-example-in-r",
    "href": "lectures/week-01_intro-design_part2.html#practical-example-in-r",
    "title": "Planning Experiments",
    "section": "Practical Example in R",
    "text": "Practical Example in R\n\nExample: A Simple CRD\nCompare three battery types (A, B, C) with 4 replicates each.\n\n\nR Code\n\n\n\nset.seed(123)\ntreatments &lt;- rep(c(\"A\", \"B\", \"C\"), each = 4)\nassignments &lt;- sample(treatments)\n\n# Simulate some response data\nresponse &lt;- rnorm(12, mean = 50, sd = 5)\n\n# Fit ANOVA model\nmodel &lt;- aov(response ~ factor(assignments))\nsummary(model)\n\n                    Df Sum Sq Mean Sq F value Pr(&gt;F)\nfactor(assignments)  2   14.2    7.08   0.163  0.852\nResiduals            9  391.5   43.50               \n\n\n\nTable 3: ANOVA Table\n\n\n\nInterpret the ANOVA table:\n\nIf the p-value for factor(assignments) is small, it suggests a difference among battery types.\n\n\n\nActivity: Interpret R Output\nExamine the ANOVA table. Identify the F-statistic and p-value, and discuss whether treatments differ.\n\n\nChallenges and Common Mistakes\n\nMisinterpreting p-values without context.\nIgnoring model assumptions such as normality and equal variances.\n\n\n\nExercises\n\nConceptual: Explain why randomization is essential before running ANOVA.\nNumerical: Modify the code for 2 treatments with 5 replicates and interpret results.\nProof/Derivation: Show how the F-statistic relates to the ratio of variances in ANOVA.\n\nKey Takeaways:\n\nR facilitates practical application of design concepts.\nInterpreting software output requires understanding design principles and model assumptions."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#summary-of-key-takeaways",
    "href": "lectures/week-01_intro-design_part2.html#summary-of-key-takeaways",
    "title": "Planning Experiments",
    "section": "Summary of Key Takeaways",
    "text": "Summary of Key Takeaways\n\nObjectives: Clear objectives guide the entire design and analysis.\nFactors and Levels: Treatment factors and meaningful factor levels ensure relevance.\nExperimental Units and Blocking: Correct identification of units and use of blocking increases precision.\nRandomization: Ensures unbiased estimates and valid statistical inference.\nDesigns (CRD, RBD, Factorial, etc.): Different designs solve different problems; factorial designs explore interactions, blocking controls known variation.\nFixed vs. Random Effects: Proper classification determines the scope of inferences.\nPlanning and Pilots: Thoughtful planning and preliminary trials prevent costly errors.\nSample Size and Power: Adequate sample size ensures detectable effects without wasting resources.\nR Implementation: Practical coding examples reinforce theoretical principles."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#definitions-of-key-terms",
    "href": "lectures/week-01_intro-design_part2.html#definitions-of-key-terms",
    "title": "Planning Experiments",
    "section": "Definitions of Key Terms",
    "text": "Definitions of Key Terms\n\nObjective: The main research question or goal of the experiment.\nNuisance Factor: A variable that affects the response but is not of primary interest.\nTreatment Factor: A variable manipulated by the experimenter.\nFactor Levels: The specific settings or categories of a factor.\nExperimental Unit: The entity to which a treatment is applied independently.\nBlock Factor: A factor used to group units into homogeneous sets.\nRandomization: Assigning treatments randomly to avoid bias.\nCRD (Completely Randomized Design): A design without blocking; treatments are assigned randomly.\nRBD (Randomized Block Design): A design that uses blocks to control known nuisance factors.\nFactorial Design: A design that includes multiple factors simultaneously.\nFixed Effects: Effects of chosen factor levels of specific interest.\nRandom Effects: Effects of factor levels considered as a random sample from a population.\nPilot Experiment: A small preliminary study to refine methods before the main experiment.\nPower: The probability of detecting a true treatment effect.\nANOVA: A statistical method for comparing means across groups."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html",
    "href": "lectures/week-03_comparisons-contrasts.html",
    "title": "Multiple Comparisons and Contrasts",
    "section": "",
    "text": "In many experiments, researchers compare multiple treatments or conditions to understand their effects on a response variable. For example, consider a horticulturalist testing various fertilizers on plant growth or a biologist examining different drugs on patient outcomes. After performing an ANOVA and finding evidence that not all treatment means are equal, the next step is to determine which treatments differ, and by how much.\nMultiple Comparison Procedures (MCPs) and contrasts address this challenge:\n\nMCPs control the overall probability of false positives (Type I errors) when making many comparisons.\nContrasts focus on specific, pre-defined linear combinations of treatment means, often reflecting scientific questions.\n\nThe combination of MCPs and contrasts ensures valid inferences, mitigating the risk of drawing erroneous conclusions from multiple tests while retaining interpretability and power.\n\n\n\nUnderstand the rationale for performing multiple comparisons after ANOVA.\nExplore common MCPs: Fisher’s LSD, Tukey’s HSD, Bonferroni, and Scheffé methods.\nLearn how to construct orthogonal and non-orthogonal contrasts and interpret their results.\nUtilize R to perform multiple comparisons and test contrasts on real or simulated data.\nDiscuss Type I error control, power considerations, and the trade-offs in choosing different procedures.\nDevelop deeper theoretical insights (in the Appendix) and practice through exercises of medium and challenging difficulty.\n\n\n\n\nTo implement MCPs and contrasts in R, we will use the following packages:\n\npacman::p_load(emmeans, pwr, multcomp, agricolae, lsmeans, car)\n\n\nemmeans: For computing contrasts and post-hoc tests.\npwr: For power analysis and sample size calculations.\nmultcomp: For multiple comparison procedures.\nagricolae: For conducting multiple comparisons and post-hoc tests.\nlsmeans: For computing least-squares means and contrasts.\ncar: For linear contrasts and hypothesis testing.\n\n\n\n\nImagine testing four battery types (A, B, C, D) to see which lasts longest. After ANOVA suggests at least one mean differs, you want to pinpoint where differences lie:\n\nAre A and B different?\nIs C worse than the average of A and B?\nDoes D stand apart?\n\nWithout controlling for multiple testing, repeated pairwise comparisons inflate the risk of false positives. MCPs handle this by adjusting significance criteria, while contrasts let you test targeted hypotheses, such as comparing the average performance of two treatments against another group’s mean.\n\n\n\nMail Sorting Analogy: Consider sorting through a large pile of mail (treatment comparisons). Without rules (MCPs), you might mistakenly pick “important” letters (differences) that are actually junk (Type I errors). MCPs act like filters, reducing the chance you misclassify too many letters as important.\nMagnifying Glass for Specific Questions (Contrasts): Instead of looking at all letters at once, a contrast is like using a magnifying glass to focus on a subset of letters (treatment means) to answer a specific question."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#objectives",
    "href": "lectures/week-03_comparisons-contrasts.html#objectives",
    "title": "Multiple Comparisons and Contrasts",
    "section": "",
    "text": "Understand the rationale for performing multiple comparisons after ANOVA.\nExplore common MCPs: Fisher’s LSD, Tukey’s HSD, Bonferroni, and Scheffé methods.\nLearn how to construct orthogonal and non-orthogonal contrasts and interpret their results.\nUtilize R to perform multiple comparisons and test contrasts on real or simulated data.\nDiscuss Type I error control, power considerations, and the trade-offs in choosing different procedures.\nDevelop deeper theoretical insights (in the Appendix) and practice through exercises of medium and challenging difficulty."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#r-packages",
    "href": "lectures/week-03_comparisons-contrasts.html#r-packages",
    "title": "Multiple Comparisons and Contrasts",
    "section": "",
    "text": "To implement MCPs and contrasts in R, we will use the following packages:\n\npacman::p_load(emmeans, pwr, multcomp, agricolae, lsmeans, car)\n\n\nemmeans: For computing contrasts and post-hoc tests.\npwr: For power analysis and sample size calculations.\nmultcomp: For multiple comparison procedures.\nagricolae: For conducting multiple comparisons and post-hoc tests.\nlsmeans: For computing least-squares means and contrasts.\ncar: For linear contrasts and hypothesis testing."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#motivation-and-context",
    "href": "lectures/week-03_comparisons-contrasts.html#motivation-and-context",
    "title": "Multiple Comparisons and Contrasts",
    "section": "",
    "text": "Imagine testing four battery types (A, B, C, D) to see which lasts longest. After ANOVA suggests at least one mean differs, you want to pinpoint where differences lie:\n\nAre A and B different?\nIs C worse than the average of A and B?\nDoes D stand apart?\n\nWithout controlling for multiple testing, repeated pairwise comparisons inflate the risk of false positives. MCPs handle this by adjusting significance criteria, while contrasts let you test targeted hypotheses, such as comparing the average performance of two treatments against another group’s mean.\n\n\n\nMail Sorting Analogy: Consider sorting through a large pile of mail (treatment comparisons). Without rules (MCPs), you might mistakenly pick “important” letters (differences) that are actually junk (Type I errors). MCPs act like filters, reducing the chance you misclassify too many letters as important.\nMagnifying Glass for Specific Questions (Contrasts): Instead of looking at all letters at once, a contrast is like using a magnifying glass to focus on a subset of letters (treatment means) to answer a specific question."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#what-is-a-contrast",
    "href": "lectures/week-03_comparisons-contrasts.html#what-is-a-contrast",
    "title": "Multiple Comparisons and Contrasts",
    "section": "What is a Contrast?",
    "text": "What is a Contrast?\nA contrast is a linear combination of treatment means that sums to zero:\n\\[\nL = \\sum_{i=1}^{v} c_i \\mu_i, \\quad \\text{with } \\sum_{i=1}^{v} c_i = 0.\n\\]\nThis ensures we’re measuring relative differences. Examples:\n\nPairwise difference: \\(\\mu_1 - \\mu_2\\).\nComparison of groups: \\(\\frac{\\mu_1 + \\mu_2}{2} - \\frac{\\mu_3 + \\mu_4}{2}\\)."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#variance-of-a-contrast",
    "href": "lectures/week-03_comparisons-contrasts.html#variance-of-a-contrast",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Variance of a Contrast",
    "text": "Variance of a Contrast\nIf each treatment \\(i\\) has \\(n_i\\) observations and errors are \\(N(0,\\sigma^2)\\), then:\n\\[\n\\text{Var}(L) = \\sigma^2 \\sum_{i=1}^v \\frac{c_i^2}{n_i}.\n\\]\nThis variance helps form confidence intervals and tests for the contrast.\n\nExample\nIn a battery life study with treatments A, B, C, D, suppose we suspect that (A and B) as a group differ from (C and D). We might form a contrast:\n\\[\nL = \\frac{\\mu_A + \\mu_B}{2} - \\frac{\\mu_C + \\mu_D}{2}.\n\\]\nThis contrast directly tests a scientific hypothesis: Do the top two battery types outperform the bottom two on average?"
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#fishers-least-significant-difference-lsd",
    "href": "lectures/week-03_comparisons-contrasts.html#fishers-least-significant-difference-lsd",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Fisher’s Least Significant Difference (LSD)",
    "text": "Fisher’s Least Significant Difference (LSD)\n\nConduct an ANOVA test first. If significant, proceed.\nCompare pairwise means using the standard t-test but do not adjust alpha for multiple tests.\nPros: Simple, more powerful.\nCons: Higher chance of Type I errors when many comparisons are made."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#tukeys-honest-significant-difference-hsd",
    "href": "lectures/week-03_comparisons-contrasts.html#tukeys-honest-significant-difference-hsd",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Tukey’s Honest Significant Difference (HSD)",
    "text": "Tukey’s Honest Significant Difference (HSD)\n\nDesigned for pairwise comparisons.\nControls the family-wise error rate (FWER).\nFor equal sample sizes:\n\n\\[\n\\text{HSD} = q_{\\alpha,v,n-v} \\sqrt{\\frac{\\text{MSE}}{r}},\n\\]\nwhere \\(q\\) is from the studentized range distribution, \\(v\\) is the number of groups, \\(n\\) is the total sample size, and \\(r\\) is the number of observations per group.\n\n\nFor a more detailed explanation of Tukey’s HSD, see the tutorial page Tukey’s HSD in the appendix."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#tukeys-hsd-procedure",
    "href": "lectures/week-03_comparisons-contrasts.html#tukeys-hsd-procedure",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Tukey’s HSD Procedure",
    "text": "Tukey’s HSD Procedure\n\nCompute all pairwise differences.\nCalculate the HSD value.\nIf the difference between two means exceeds the HSD, they are significantly different."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#bonferroni-method",
    "href": "lectures/week-03_comparisons-contrasts.html#bonferroni-method",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Bonferroni Method",
    "text": "Bonferroni Method\n\nDivide \\(\\alpha\\) by the number of comparisons \\(m\\).\nVery conservative for large \\(m\\).\nEnsures strong control of FWER."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#scheffés-method",
    "href": "lectures/week-03_comparisons-contrasts.html#scheffés-method",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Scheffé’s Method",
    "text": "Scheffé’s Method\n\nApplies to any linear contrast, not just pairwise differences.\nMore general but often more conservative (wide confidence intervals).\nScheffé’s method is useful when the final set of contrasts is not fully decided a priori."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#example-1-pairwise-comparisons-with-tukeys-hsd",
    "href": "lectures/week-03_comparisons-contrasts.html#example-1-pairwise-comparisons-with-tukeys-hsd",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Example 1: Pairwise Comparisons with Tukey’s HSD",
    "text": "Example 1: Pairwise Comparisons with Tukey’s HSD\nData Setup (Simulated):\n\n\n\nset.seed(123)\ndata &lt;- data.frame(\n    Treatment = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 5),\n    Response = c(\n        rnorm(5, 10, 2),\n        rnorm(5, 15, 2),\n        rnorm(5, 20, 2),\n        rnorm(5, 25, 2)\n    )\n)\n\nfit &lt;- aov(Response ~ Treatment, data = data)\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment    3  631.3  210.43    47.7 3.33e-08 ***\nResiduals   16   70.6    4.41                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTable 1: ANOVA Data\n\n\n\n\n\n\n# Apply Tukey’s HSD if ANOVA is significant\nTukeyHSD(fit)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Response ~ Treatment, data = data)\n\n$Treatment\n         diff        lwr       upr     p adj\nB-A  4.524222  0.7237295  8.324714 0.0170952\nC-A 10.228663  6.4281709 14.029155 0.0000050\nD-A 14.831544 11.0310518 18.632036 0.0000000\nC-B  5.704441  1.9039494  9.504933 0.0028150\nD-B 10.307322  6.5068303 14.107814 0.0000045\nD-C  4.602881  0.8023889  8.403373 0.0151748\n\n\n\nTable 2: Tukey’s HSD for Pairwise Comparisons\n\n\n\n\nInterpretation\n\nLook at the output: intervals that do not include zero indicate significant differences.\nTukey is effective for all pairs while maintaining the overall confidence level."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#example-2-testing-a-specific-contrast",
    "href": "lectures/week-03_comparisons-contrasts.html#example-2-testing-a-specific-contrast",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Example 2: Testing a Specific Contrast",
    "text": "Example 2: Testing a Specific Contrast\nDefining a Contrast:\n\n\n\n# Suppose we want to test (A,B) vs (C,D)\ncontrast_vec &lt;- c(0.5, 0.5, -0.5, -0.5)\nemm &lt;- emmeans(fit, ~Treatment)\ncontrast(emm, method = list(\"AB_vs_CD\" = contrast_vec))\n\n contrast estimate    SE df t.ratio p.value\n AB_vs_CD    -10.3 0.939 16 -10.932  &lt;.0001\n\n\n\nTable 3: Contrast Testing\n\n\n\n\n\nFor more detail on the emmeans package, see the tutorial page emmeans in the appendix.\n\nInterpretation\nThe output gives an estimate, standard error, t-value, and p-value. If p &lt; alpha, we conclude there is evidence that (A,B) differ from (C,D)."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#medium-difficulty",
    "href": "lectures/week-03_comparisons-contrasts.html#medium-difficulty",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nPairwise Comparisons: Given four treatments, run a simulation in R, fit an ANOVA, and use Tukey’s HSD. Interpret at least one pairwise comparison.\nConstructing Contrasts: For three treatments (X, Y, Z), form a contrast comparing X with the average of Y and Z. Compute its variance using a known MSE and sample sizes."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#challenging-problems",
    "href": "lectures/week-03_comparisons-contrasts.html#challenging-problems",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nScheffé’s Method for Arbitrary Contrasts: Generate data from five treatments and test a complex contrast (e.g., \\((\\mu_1+\\mu_2)-(\\mu_4+\\mu_5)\\)). Implement Scheffé’s method and interpret the wide confidence interval.\nPower Analysis for MCPs: Using the pwr package in R, simulate a scenario with four treatments and known variance. Determine sample size needed to achieve 80% power under Tukey’s HSD for detecting a specified difference."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#conclusion",
    "href": "lectures/week-03_comparisons-contrasts.html#conclusion",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Conclusion",
    "text": "Conclusion\nMCPs and contrasts extend the basic ANOVA framework to address realistic experimental questions:\n\nMCPs control the “false alarm” rate across multiple inferences.\nContrasts provide targeted insight into predetermined scientific hypotheses.\nModern statistical software facilitates these procedures, ensuring robust, interpretable results."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#overview-of-classical-methods",
    "href": "lectures/week-03_comparisons-contrasts.html#overview-of-classical-methods",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Overview of Classical Methods",
    "text": "Overview of Classical Methods\n\nBonferroni Correction\n\nAdjustment: \\(\\alpha^* = \\alpha / m\\) for each of the \\(m\\) tests.\nConfidence Intervals: Expand accordingly by using \\(t_{\\alpha^*/2,\\,df}\\) for each test.\nAdvantages: Easy to understand and implement; mathematically guarantees control of the FWER under very general conditions (no need for independence).\nDisadvantages: Can be overly conservative, especially for large \\(m\\).\n\nHolm’s Step-Down Method\n\nAn adaptive Bonferroni: Sort p-values from smallest to largest: \\(p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}.\\)\nCompare \\(p_{(1)}\\) to \\(\\alpha/m\\). If significant, compare \\(p_{(2)}\\) to \\(\\alpha/(m-1)\\), and so on.\nAdvantages: Less conservative than Bonferroni, still guarantees FWER control.\nExample: If the smallest p-value is less than \\(\\alpha/m\\), reject that hypothesis; then move to the next one with a slightly higher threshold, \\(\\alpha/(m-1)\\). Continue until a test fails to reject or until all tests are done.\n\nHochberg’s Step-Up Method\n\nAnother adaptive approach, sometimes more powerful than Holm but requires certain assumptions (e.g., independence or some positive dependence structure).\nSort p-values in descending order (largest to smallest) and proceed in a step-up fashion.\n\nScheffé’s Method (specific to ANOVA)\n\nDesigned to test any possible linear contrast among means after a one-way ANOVA.\nUses an \\(F\\)-ratio scaling factor \\(\\sqrt{(v-1)\\,F_{v-1,\\,df,\\,\\alpha}}\\) to widen intervals.\nPros: Very flexible when many post-hoc contrasts might be considered.\nCons: Often considered conservative because it covers the “worst-case” scenario of an infinite number of possible contrasts.\n\nTukey’s Honestly Significant Difference (HSD)\n\nOptimized for all pairwise comparisons among \\(v\\) groups.\nRelies on the Studentized range statistic \\(q\\).\nTypically more powerful than Scheffé when only pairwise comparisons are needed.\n\nDunnett’s Procedure\n\nSpecial-case procedure for comparing several treatments to one control.\nUses a multivariate \\(t\\)-distribution for its critical values, producing narrower intervals (and higher power) than a general method like Bonferroni if this is the only family of tests performed."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#illustration-pairwise-comparisons-after-anova",
    "href": "lectures/week-03_comparisons-contrasts.html#illustration-pairwise-comparisons-after-anova",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Illustration: Pairwise Comparisons After ANOVA",
    "text": "Illustration: Pairwise Comparisons After ANOVA\nScenario: You have 4 treatments in a one-way ANOVA, and you want to compare each pair of treatments \\((v=4, m=6\\) pairwise comparisons). - Bonferroni: Each test at \\(\\alpha/6\\approx 0.0083\\). - Tukey’s HSD: R software would provide a critical difference based on the studentized range \\(q_{4, df, \\alpha}\\). This typically yields narrower confidence intervals than Bonferroni because it leverages the specific nature of pairwise testing.\n\nIn R:\nmodel &lt;- aov(response ~ treatment, data = your_data)\ntukey_result &lt;- TukeyHSD(model, \"treatment\", conf.level = 0.95)\ntukey_result\n\nTukeyHSD() automatically applies the Studentized range logic to give adjusted p-values and confidence intervals for each pairwise comparison."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#false-discovery-rate-fdr",
    "href": "lectures/week-03_comparisons-contrasts.html#false-discovery-rate-fdr",
    "title": "Multiple Comparisons and Contrasts",
    "section": "False Discovery Rate (FDR)",
    "text": "False Discovery Rate (FDR)\nFor studies with large numbers of hypotheses, such as genomics, proteomics, or large-scale medical analyses, controlling the FWER (i.e., the chance of any false positive) can be very stringent and can drastically reduce statistical power. An alternative is to control the False Discovery Rate (FDR), which is defined as:\n\\[\n\\text{FDR} = \\mathbb{E}\\!\\biggl[\\frac{\\text{Number of False Rejections}}{\\text{Total Number of Rejections}}\\biggr].\n\\]\nIn other words, among all the rejections you make, the FDR method aims to keep the expected proportion of those that are false under a specified level (\\(\\alpha\\)). This is often less conservative than controlling the familywise error rate and can be more suitable when you have hundreds or thousands of tests."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#benjaminihochberg-bh-procedure",
    "href": "lectures/week-03_comparisons-contrasts.html#benjaminihochberg-bh-procedure",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Benjamini–Hochberg (BH) Procedure",
    "text": "Benjamini–Hochberg (BH) Procedure\n\nSteps:\n\nSort p-values from smallest to largest: \\(p_{(1)} \\le p_{(2)} \\le \\cdots \\le p_{(m)}\\).\nFind the largest \\(k\\) such that\n\n\\[\np_{(k)} \\; \\le \\; \\frac{k}{m}\\,\\alpha.\n\\]\n\nReject the null hypotheses for all \\(p_{(i)}, i \\le k\\).\nInterpretation: You keep rejecting from the smallest p-value up to a point where the p-value is no longer below a threshold adjusted by \\(\\frac{k}{m}\\alpha\\).\nResult: On average, the proportion of “rejected hypotheses” that are false positives is capped by \\(\\alpha\\).\n\nExample: Suppose \\(m=1000\\) tests, with \\(\\alpha=0.05\\). You list p-values in ascending order. If the 40th smallest p-value still satisfies \\(p_{(40)} \\le (40/1000)*0.05 = 0.002\\), you reject that one. But if the 41st smallest p-value is 0.003, which does not meet the criterion \\(\\le 0.00205\\), you stop at 40 rejections.\n\n\nIn R:\np_vals &lt;- c(…)  # your vector of raw p-values\np_adjusted &lt;- p.adjust(p_vals, method = \"BH\")\n# p_adjusted now contains BH-adjusted p-values\nsignificant &lt;- p_adjusted &lt;-  0.05\n\np.adjust(…, method=\"BH\") applies the Benjamini–Hochberg procedure by default."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#other-fdr-variants",
    "href": "lectures/week-03_comparisons-contrasts.html#other-fdr-variants",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Other FDR Variants",
    "text": "Other FDR Variants\n\nBenjamini–Yekutieli (BY): Accommodates certain dependency structures by replacing \\(\\frac{k}{m}\\alpha\\) with \\(\\frac{k}{m} \\frac{\\alpha}{\\sum_{i=1}^m (1/i)}\\). More conservative.\nStorey’s q-Value approach: Focuses on direct estimation of the proportion of null hypotheses that are true, providing a “q-value” analogous to an FDR-adjusted p-value."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#comparing-fdr-vs.-fwer",
    "href": "lectures/week-03_comparisons-contrasts.html#comparing-fdr-vs.-fwer",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Comparing FDR vs. FWER",
    "text": "Comparing FDR vs. FWER\n\n\n\n\n\n\n\n\nCriterion\nFamilywise Error Rate (FWER)\nFalse Discovery Rate (FDR)\n\n\n\n\nDefinition\nProbability of at least one false rejection in a set of tests.\nExpected proportion of false positives among all rejections.\n\n\nInterpretation\nVery stringent; zero-tolerance approach: “I want no false positives.”\nAllows some false positives but controls proportion of errors among significant findings.\n\n\nMethods\nBonferroni, Holm, Hochberg, Tukey, Scheffé, Dunnett, etc.\nBenjamini–Hochberg, Benjamini–Yekutieli, q-value, etc.\n\n\nUse Cases\nSmall to moderate number of comparisons (e.g., typical ANOVA-based multiple comparisons).\nLarge-scale testing (e.g., genomics, proteomics, data-mining). More powerful in “big data” scenarios."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#practical-notes-and-r-guidance",
    "href": "lectures/week-03_comparisons-contrasts.html#practical-notes-and-r-guidance",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Practical Notes and R Guidance",
    "text": "Practical Notes and R Guidance\n\nBalancing Stringency and Discovery\n\nIn typical design-of-experiments contexts (small \\(m\\)), controlling FWER with classical methods (Tukey, Dunnett, Scheffé) is common.\nIn high-dimensional settings (large \\(m\\)), FDR-based approaches provide more power while accepting a controlled fraction of false positives.\n\nImplementation Tips\n\nAlways report which correction method was used.\nFor one-way ANOVA with moderate group sizes:\n\nTukeyHSD for pairwise tests in base R.\nglht() in multcomp can handle Dunnett’s or other customized contrasts.\n\nFor large sets of p-values (e.g., from omics or data-mining):\n\nUse p.adjust(…, method = \"BH\") or a specialized package for more advanced FDR control.\n\n\nBe Aware of Underlying Assumptions\n\nSome methods (e.g., Hochberg’s step-up) assume independence or positive correlation among tests.\nIf dependencies are complex, it can alter the conservativeness of the corrections."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#extended-example",
    "href": "lectures/week-03_comparisons-contrasts.html#extended-example",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Extended Example",
    "text": "Extended Example\nHigh-Throughput Screening: Suppose you test 1000 compounds for their effect on tumor cell viability. You collect p-values for each compound’s effect vs. control. Without any correction:\n\\[\n\\text{FWER} = 1 - (1-0.05)^{1000} \\approx 1 \\; (\\text{virtually } 100\\%\\text{ chance of at least one false positive}).\n\\]\n\nFWER Control (Bonferroni): \\(\\alpha^* = 0.05 / 1000 = 0.00005.\\) This drastically reduces your chance of even a single false discovery, but many true effects could be missed.\nFDR Control (Benjamini–Hochberg): Typically yields a threshold that might let you reject dozens or hundreds of null hypotheses while controlling the expected proportion of false positives among your rejections at 5%.\n\n\nIn R:\n# Suppose p_raw is a vector of 1000 p-values from the tests\np_vals_adj_bonf &lt;- p.adjust(p_raw, method = \"bonferroni\")\np_vals_adj_bh &lt;- p.adjust(p_raw, method = \"BH\")\n\n# Summaries:\nsum(p_vals_adj_bonf &lt; 0.05)  # how many are called significant under Bonferroni\nsum(p_vals_adj_bh &lt; 0.05)    # how many are called significant under BH\n\nExpect the Bonferroni set of significant tests to be a subset of the BH set.\nThe BH procedure, by allowing for a controlled fraction of false positives, typically identifies more “discoveries.”"
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#conclusion-1",
    "href": "lectures/week-03_comparisons-contrasts.html#conclusion-1",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Conclusion",
    "text": "Conclusion\nBoth familywise error rate (FWER) and false discovery rate (FDR) control are essential concepts for valid statistical inference in the face of multiple comparisons:\n\nFWER methods (like Bonferroni, Holm, Tukey, Scheffé, Dunnett) ensure that the probability of any false positive across all tests remains below \\(\\alpha\\).\nFDR methods (like Benjamini–Hochberg) allow some false positives but control their proportion among all discoveries, making them particularly valuable for large-scale testing contexts.\n\nUnderstanding these frameworks—and carefully choosing which to apply—helps maintain the integrity of your findings while balancing the risk of missing real effects."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#references-and-further-reading",
    "href": "lectures/week-03_comparisons-contrasts.html#references-and-further-reading",
    "title": "Multiple Comparisons and Contrasts",
    "section": "References and Further Reading",
    "text": "References and Further Reading\n\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B, 57(1), 289–300.\nHochberg, Y., & Tamhane, A. C. (1987). Multiple Comparison Procedures. Wiley.\nMontgomery, D. C. (2017). Design and Analysis of Experiments. 9th ed. Wiley.\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer. Storey, J. D. (2002). A direct approach to false discovery rates. Journal of the Royal Statistical Society: Series B, 64(3), 479–498."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#references",
    "href": "lectures/week-03_comparisons-contrasts.html#references",
    "title": "Multiple Comparisons and Contrasts",
    "section": "References",
    "text": "References\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nMontgomery, D. C. (2020). Design and Analysis of Experiments. Wiley.\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#proofs-and-derivations",
    "href": "lectures/week-03_comparisons-contrasts.html#proofs-and-derivations",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Proofs and Derivations",
    "text": "Proofs and Derivations\n\nVariance of a Contrast\nFor a contrast \\(L = \\sum c_i \\mu_i\\), assuming independent samples of size \\(n_i\\) and variance \\(\\sigma^2\\):\n\\[\n\\hat{L} = \\sum c_i \\bar{Y}_i, \\quad \\text{Var}(\\hat{L}) = \\sigma^2 \\sum \\frac{c_i^2}{n_i}.\n\\]\nProof involves linearity of expectation and variance properties under independence and homoscedasticity (Dean et al., 2017).\n\n\nDerivation of Tukey’s HSD\nTukey’s HSD critical value is derived from the studentized range distribution \\(q\\):\n\nLet \\(\\text{max diff} = \\max_i(\\bar{Y}_i) - \\min_i(\\bar{Y}_i)\\).\nUnder \\(H_0\\), \\(\\frac{\\text{max diff}}{\\sqrt{\\text{MSE}/n}}\\) follows a studentized range distribution.\nInvert this to find the critical range \\(q_{\\alpha; v,n-v}\\).\n\n(Montgomery, 2020)\n\n\nScheffé’s Method\nScheffé’s intervals are based on the fact that any linear contrast of treatment means can be bounded using a \\((v-1)F\\)-distribution multiplier:\n\\[\nL \\in \\hat{L} \\pm \\sqrt{(v-1)F_{\\alpha; v-1, n-v}}\\cdot \\text{SE}(\\hat{L}).\n\\]\nThe proof uses the union-intersection principle and geometry of the parameter space (Christensen, 2018)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html",
    "href": "lectures/week-05_factorial-anova.html",
    "title": "Factorial ANOVA",
    "section": "",
    "text": "Understand the structure and terminology of two-factor factorial designs.\nDistinguish between main effects and interaction effects in a two-way ANOVA.\nConduct a two-way ANOVA, interpret results, and create interaction plots in R.\nHandle complexities such as unbalanced designs and understand different types of sums of squares."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#objectives",
    "href": "lectures/week-05_factorial-anova.html#objectives",
    "title": "Factorial ANOVA",
    "section": "",
    "text": "Understand the structure and terminology of two-factor factorial designs.\nDistinguish between main effects and interaction effects in a two-way ANOVA.\nConduct a two-way ANOVA, interpret results, and create interaction plots in R.\nHandle complexities such as unbalanced designs and understand different types of sums of squares."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#r-packages",
    "href": "lectures/week-05_factorial-anova.html#r-packages",
    "title": "Factorial ANOVA",
    "section": "R Packages",
    "text": "R Packages\nTo run the R code examples in these notes, we recommend installing the following packages:\n\n# Required R Packages\npacman::p_load(dplyr, ggplot2, lsmeans, emmeans, here, readr)  \n\n\ndplyr: Data manipulation and summarization.\nggplot2: Data visualization.\nlsmeans and emmeans: Post hoc comparisons and multiple comparisons.\npacman: Package management.\nhere: File path management.\nreadr: Reading data files."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#checking-the-assumptions-on-the-model",
    "href": "lectures/week-05_factorial-anova.html#checking-the-assumptions-on-the-model",
    "title": "Factorial ANOVA",
    "section": "Checking the Assumptions on the Model",
    "text": "Checking the Assumptions on the Model\nThe assumptions for both the two-way complete model and the two-way main-effects model are that the error random variables have equal variances, are mutually independent, and are normally distributed. The strategy for checking these assumptions follows the procedures outlined in Chapter 5 (see Dean et al. 2017)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#standardized-residuals",
    "href": "lectures/week-05_factorial-anova.html#standardized-residuals",
    "title": "Factorial ANOVA",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nThe standardized residuals are computed as\n\\[\nz_{ijt} = \\frac{y_{ijt} - \\hat{y}_{ijt}}{\\sqrt{SS_{Error}/(n - 1)}},\n\\]\nwhere\n\n\\(\\hat{y}_{ijt} = \\hat{\\tau}_{ij} = \\hat{\\alpha}_i + \\hat{\\beta}_j + (\\alpha\\beta)_{ij}\\) (two-way complete model), or\n\\(\\hat{y}_{ijt} = \\hat{\\tau}_{ij} = \\hat{\\alpha}_i + \\hat{\\beta}_j\\) (two-way main-effects model),\n\ndepending on which model is used. A “hat” denotes a least squares estimate.\nTypical residual plots to check:\n\nObservation Order vs. Residuals to assess independence.\nLevels of Each Factor (and possibly \\(\\hat{y}_{ijt}\\)) vs. Residuals to detect outliers and variance inhomogeneity.\nNormal Q-Q Plot (Normal Scores) to assess normality.\n\nWhen using the main-effects model, interaction plots (e.g., Figures 6.1, 6.2 in reference texts) help visualize any potential interaction. Another method is to plot standardized residuals against one factor’s levels while labeling or grouping by the second factor’s levels."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#concept-of-interaction",
    "href": "lectures/week-05_factorial-anova.html#concept-of-interaction",
    "title": "Factorial ANOVA",
    "section": "Concept of Interaction",
    "text": "Concept of Interaction\nDefinition: An interaction occurs when the effect of one factor depends on the level of the other factor.\n\nIn literature, Fisher (1926) emphasized that interaction reveals non-additivity among factors. Dean et al. (2017) define interaction as “the failure of factors to combine additively in their effects on the response.”\nGraphical perspective: If lines in an interaction plot are not parallel, an interaction is likely present.\n\n\nWhy Interaction Matters\n\nIf significant, you cannot interpret main effects alone.\nMust interpret or compare the cell means \\(\\mu_{ij}\\) directly."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#scenario-effect-of-diet-and-exercise-on-weight-loss",
    "href": "lectures/week-05_factorial-anova.html#scenario-effect-of-diet-and-exercise-on-weight-loss",
    "title": "Factorial ANOVA",
    "section": "Scenario: Effect of Diet and Exercise on Weight Loss",
    "text": "Scenario: Effect of Diet and Exercise on Weight Loss\nSuppose a researcher wants to study the impact of diet type and exercise routine on weight loss. The two factors are:\n\nFactor A: Diet Type\n\n\\(A_1\\): Low-Carb Diet\n\\(A_2\\): Low-Fat Diet\n\nFactor B: Exercise Routine\n\n\\(B_1\\): Cardio Exercise\n\\(B_2\\): Strength Training\n\n\nThe response variable is weight loss (kg) after 3 months."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#case-1-no-interaction-additive-effects",
    "href": "lectures/week-05_factorial-anova.html#case-1-no-interaction-additive-effects",
    "title": "Factorial ANOVA",
    "section": "Case 1: No Interaction (Additive Effects)",
    "text": "Case 1: No Interaction (Additive Effects)\nIf the effects of diet and exercise are purely additive, we might see the following mean weight loss data:\n\n\n\n\n\n\n\n\n\nExercise Type\nLow-Carb Diet (\\(A_1\\))\nLow-Fat Diet (\\(A_2\\))\nEffect of Diet\n\n\n\n\nCardio (\\(B_1\\))\n8 kg\n5 kg\n3 kg\n\n\nStrength Training (\\(B_2\\))\n6 kg\n3 kg\n3 kg\n\n\nEffect of Exercise\n2 kg\n2 kg"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#key-observations",
    "href": "lectures/week-05_factorial-anova.html#key-observations",
    "title": "Factorial ANOVA",
    "section": "Key Observations",
    "text": "Key Observations\n\nMain Effect of Diet: Low-carb results in 3 kg more weight loss than low-fat on average (marginal means: \\((8+6)/2 = 7\\) vs. \\((5+3)/2 = 4\\)).\nMain Effect of Exercise: Cardio results in 2 kg more weight loss than strength training on average (marginal means: \\((8+5)/2 = 6.5\\) vs. \\((6+3)/2 = 4.5\\)).\nNo Interaction: The effect of diet is consistent across exercise types (difference = 2 kg for both), and the effect of exercise is consistent across diets (difference = 3 kg for both)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#case-2-significant-interaction-non-additive-effects",
    "href": "lectures/week-05_factorial-anova.html#case-2-significant-interaction-non-additive-effects",
    "title": "Factorial ANOVA",
    "section": "Case 2: Significant Interaction (Non-Additive Effects)",
    "text": "Case 2: Significant Interaction (Non-Additive Effects)\nIf diet and exercise interact, the results may look different:\n\n\n\n\n\n\n\n\n\nExercise Type\nLow-Carb Diet (\\(A_1\\))\nLow-Fat Diet (\\(A_2\\))\nEffect of Exercise\n\n\n\n\nCardio (\\(B_1\\))\n10 kg\n3 kg\n7 kg\n\n\nStrength Training (\\(B_2\\))\n4 kg\n5 kg\n-1 kg\n\n\nEffect of Diet\n6 kg\n-2 kg"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#key-observations-1",
    "href": "lectures/week-05_factorial-anova.html#key-observations-1",
    "title": "Factorial ANOVA",
    "section": "Key Observations:",
    "text": "Key Observations:\n\nOn a Low-Carb Diet, Cardio leads to the highest weight loss (10 kg), while Strength Training leads to much less weight loss (4 kg).\nOn a Low-Fat Diet, Cardio is less effective (3 kg), and Strength Training is slightly better (5 kg).\nInteraction Effect: The impact of exercise type depends on the diet.\n\nCardio is very effective with a Low-Carb Diet (+10 kg loss) but ineffective with a Low-Fat Diet (only 3 kg loss).\nStrength Training performs better with Low-Fat than Low-Carb Diet.\nThe simple main effects of diet and exercise change direction in different conditions."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#how-interaction-changes-interpretation",
    "href": "lectures/week-05_factorial-anova.html#how-interaction-changes-interpretation",
    "title": "Factorial ANOVA",
    "section": "How Interaction Changes Interpretation",
    "text": "How Interaction Changes Interpretation\n\nWithout Interaction (Additive Model):\n\nThe best strategy would be Low-Carb Diet and Cardio, assuming effects are independent.\nStrength Training would be equally beneficial across diets.\n\nWith Interaction (Non-Additive Model):\n\nCardio only works well with a Low-Carb Diet.\nStrength Training works better with a Low-Fat Diet.\nRecommending Cardio to someone on a Low-Fat Diet might be counterproductive.\nIf a person prefers Strength Training, they might be better off on a Low-Fat Diet rather than a Low-Carb Diet."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#graphical-representation",
    "href": "lectures/week-05_factorial-anova.html#graphical-representation",
    "title": "Factorial ANOVA",
    "section": "Graphical Representation",
    "text": "Graphical Representation\nIf we plot weight loss against exercise type, the interaction would appear as non-parallel lines:\n\nNo interaction: Two parallel lines (consistent gap between exercise types).\nStrong interaction: Lines cross or diverge significantly, indicating a change in ranking of factors.\n\n# Create data frames for both scenarios\n\nno_interaction &lt;- data.frame(\n    Diet = rep(c(\"Low-Carb\", \"Low-Fat\"), each = 2),\n    Exercise = rep(c(\"Cardio\", \"Strength\"), times = 2),\n    Weight_Loss = c(8, 6, 5, 3)\n)\n\nwith_interaction &lt;- data.frame(\n    Diet = rep(c(\"Low-Carb\", \"Low-Fat\"), each = 2),\n    Exercise = rep(c(\"Cardio\", \"Strength\"), times = 2),\n    Weight_Loss = c(10, 4, 3, 5)\n)\n\n# Plot 1: No Interaction\nplot1 &lt;- ggplot(no_interaction, aes(x = Diet, y = Weight_Loss, color = Exercise, group = Exercise)) +\n    geom_line() +\n    geom_point(size = 3) +\n    labs(y = \"Weight Loss (kg)\", title = \"No Interaction\")\n\n# Plot 2: With Interaction\nplot2 &lt;- ggplot(with_interaction, aes(x = Diet, y = Weight_Loss, color = Exercise, group = Exercise)) +\n    geom_line() +\n    geom_point(size = 3) +\n    labs(y = \"Weight Loss (kg)\", title = \"Strong Interaction\")\n\nplot1\nplot2\n# Combine plots\n\n# cowplot::plot_grid(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n(a) No Interaction (Additive Effects)\n\n\n\n\n\n\n\n\n\n\n\n(b) Significant Interaction (Non-Additive Effects)\n\n\n\n\n\n\n\nFigure 1: Diet and Exercise Interaction Examples"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#interpretation-and-conclusion",
    "href": "lectures/week-05_factorial-anova.html#interpretation-and-conclusion",
    "title": "Factorial ANOVA",
    "section": "Interpretation and Conclusion",
    "text": "Interpretation and Conclusion\nThis example shows how interaction effects significantly impact decision-making. Ignoring interactions could lead to misleading conclusions:\n\nA “best diet” recommendation without considering exercise type would be incorrect.\nA “best exercise” recommendation without considering diet type would be misleading.\n\nThus, interaction terms are crucial in Two-Factor ANOVA to uncover hidden dependencies between factors and provide accurate interpretations of experimental results."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#sample-data-in-r",
    "href": "lectures/week-05_factorial-anova.html#sample-data-in-r",
    "title": "Factorial ANOVA",
    "section": "Sample Data in R",
    "text": "Sample Data in R\n\nbattery_data &lt;- data.frame(\n    BatteryType = rep(c(\"Alkaline\", \"HeavyDuty\"), each = 6),\n    Brand = rep(c(\"NameBrand\", \"StoreBrand\"), times = 2, each = 3),\n    Life = c(\n        32.1, 33.3, 31.9, 28.7, 29.1, 29.3,\n        25.5, 26.1, 25.7, 20.2, 20.6, 21.1\n    )\n)\nbattery_data\n\n\n\n\n\n\n\n\nBatteryType\nBrand\nLife\n\n\n\n\nAlkaline\nNameBrand\n32.1\n\n\nAlkaline\nNameBrand\n33.3\n\n\nAlkaline\nNameBrand\n31.9\n\n\nAlkaline\nStoreBrand\n28.7\n\n\nAlkaline\nStoreBrand\n29.1\n\n\nAlkaline\nStoreBrand\n29.3\n\n\nHeavyDuty\nNameBrand\n25.5\n\n\nHeavyDuty\nNameBrand\n26.1\n\n\nHeavyDuty\nNameBrand\n25.7\n\n\nHeavyDuty\nStoreBrand\n20.2\n\n\nHeavyDuty\nStoreBrand\n20.6\n\n\nHeavyDuty\nStoreBrand\n21.1\n\n\n\n\n\n\n\nTable 1: Battery Life Data\n\n\n\n\nWe have 2 (BatteryType) × 2 (Brand) × 3 replicates = 12 observations."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#model-fitting",
    "href": "lectures/week-05_factorial-anova.html#model-fitting",
    "title": "Factorial ANOVA",
    "section": "Model Fitting",
    "text": "Model Fitting\nUse aov() in base R or lm() for a two-way ANOVA with interaction:\n\n\n\nbattery_data$BatteryType &lt;- factor(battery_data$BatteryType)\nbattery_data$Brand &lt;- factor(battery_data$Brand)\n\nmodel_full &lt;- aov(Life ~ BatteryType * Brand, data = battery_data)\nsummary(model_full)\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nBatteryType        1 170.25  170.25 706.934 4.31e-09 ***\nBrand              1  54.61   54.61 226.768 3.74e-07 ***\nBatteryType:Brand  1   2.25    2.25   9.356   0.0156 *  \nResiduals          8   1.93    0.24                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTable 2: Two-Way ANOVA Model Fitting\n\n\n\n\nExplanation\n\nLife ~ BatteryType * Brand includes both main effects and their interaction.\nsummary(model_full) prints the Type I ANOVA table by default."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#interpreting-the-significant-interaction-effect-in-two-factor-anova",
    "href": "lectures/week-05_factorial-anova.html#interpreting-the-significant-interaction-effect-in-two-factor-anova",
    "title": "Factorial ANOVA",
    "section": "Interpreting The Significant Interaction Effect in Two-Factor ANOVA",
    "text": "Interpreting The Significant Interaction Effect in Two-Factor ANOVA\nSince the interaction effect (\\(p = 0.0156\\)) is statistically significant (\\(p &lt; 0.05\\)), we do not interpret the main effects of Battery Type and Brand separately. Instead, we focus on the interaction, which means that the effect of one factor depends on the level of the other factor."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#interaction-effect-interpretation",
    "href": "lectures/week-05_factorial-anova.html#interaction-effect-interpretation",
    "title": "Factorial ANOVA",
    "section": "Interaction Effect Interpretation",
    "text": "Interaction Effect Interpretation"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#interaction-batterytypebrand-f-9.356-p-0.0156",
    "href": "lectures/week-05_factorial-anova.html#interaction-batterytypebrand-f-9.356-p-0.0156",
    "title": "Factorial ANOVA",
    "section": "Interaction (BatteryType:Brand, \\(F\\) = 9.356, \\(p\\) = 0.0156)",
    "text": "Interaction (BatteryType:Brand, \\(F\\) = 9.356, \\(p\\) = 0.0156)\n\nThe significant interaction suggests that the impact of Battery Type on Battery Life is different for different Brands.\nIn other words, the difference between Alkaline and Heavy Duty batteries varies depending on whether the battery is a Name Brand or a Store Brand.\nBecause the interaction term is significant, we must analyze the individual cell means rather than summarizing the main effects."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#cell-means-group-means-by-factor-levels",
    "href": "lectures/week-05_factorial-anova.html#cell-means-group-means-by-factor-levels",
    "title": "Factorial ANOVA",
    "section": "Cell Means (Group Means by Factor Levels)",
    "text": "Cell Means (Group Means by Factor Levels)\nTo understand the interaction, we analyze the mean Battery Life for each combination of Battery Type and Brand.\n\n\n\nBattery Type\nBrand\nMean Battery Life (\\(\\bar{Y}_{ij}\\))\n\n\n\n\nAlkaline\nName Brand\n\\(\\bar{Y}_{11} = 32.0\\)\n\n\nAlkaline\nStore Brand\n\\(\\bar{Y}_{12} = 28.7\\)\n\n\nHeavy Duty\nName Brand\n\\(\\bar{Y}_{21} = 25.7\\)\n\n\nHeavy Duty\nStore Brand\n\\(\\bar{Y}_{22} = 20.7\\)"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#key-observations-from-the-cell-means",
    "href": "lectures/week-05_factorial-anova.html#key-observations-from-the-cell-means",
    "title": "Factorial ANOVA",
    "section": "Key Observations from the Cell Means",
    "text": "Key Observations from the Cell Means\n\nAlkaline batteries last longer than Heavy Duty batteries within each brand, but the size of this effect is not the same for both brands.\nName Brand batteries consistently outperform Store Brand batteries, but the gap is larger for Alkaline batteries than for Heavy Duty batteries.\nThe difference between Alkaline and Heavy Duty is larger for Store Brand (8 hours) than for Name Brand (6.3 hours).\nThis suggests a non-additive effect, where the difference between battery types depends on the brand."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#calculating-the-interaction-effect",
    "href": "lectures/week-05_factorial-anova.html#calculating-the-interaction-effect",
    "title": "Factorial ANOVA",
    "section": "Calculating the Interaction Effect",
    "text": "Calculating the Interaction Effect\nMathematically, the interaction effect can be computed as:\n\\[\n\\text{Interaction Effect} = (\\bar{Y}_{11} - \\bar{Y}_{12}) - (\\bar{Y}_{21} - \\bar{Y}_{22})\n\\] \\[\n= (32.0 - 28.7) - (25.7 - 20.7)\n\\] \\[\n= 3.3 - 5.0\n\\] \\[\n= -1.7\n\\]\n\nWhat This Means:\n\nThe difference between Alkaline and Heavy Duty batteries is not the same across brands.\nThe gap is larger for Store Brand batteries than for Name Brand batteries.\nThe negative interaction effect (-1.7) means that the effect of Battery Type is slightly weaker for Name Brand batteries compared to Store Brand batteries."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#graphical-representation-1",
    "href": "lectures/week-05_factorial-anova.html#graphical-representation-1",
    "title": "Factorial ANOVA",
    "section": "Graphical Representation",
    "text": "Graphical Representation\nA graphical interaction plot** helps visualize the effect:\n\ninteraction.plot(\n    x.factor     = battery_data$BatteryType,\n    trace.factor = battery_data$Brand,\n    response     = battery_data$Life,\n    type         = \"b\",\n    pch          = c(1, 19),\n    col          = c(\"blue\", \"red\"),\n    xlab         = \"Battery Type\",\n    ylab         = \"Mean Battery Life\",\n    trace.label  = \"Brand\"\n)\n\n\n\n\n\n\n\nFigure 2: Interaction Plot for Battery Life\n\n\n\n\n\n\nIf lines cross or deviate from parallel, that supports the significant interaction."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#expected-patterns",
    "href": "lectures/week-05_factorial-anova.html#expected-patterns",
    "title": "Factorial ANOVA",
    "section": "Expected Patterns:",
    "text": "Expected Patterns:\n\nParallel Lines → No interaction (the effect of one factor is consistent across levels of the other).\nNon-Parallel Lines → Interaction present (the effect of one factor changes based on the other).\nLines Crossing → Strong interaction (e.g., one factor reverses the effect of the other).\n\nSince the interaction is significant, we expect the lines to be non-parallel, indicating that the difference in battery life between Alkaline and Heavy Duty depends on the brand."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#contrasts-for-main-effects-and-interactions",
    "href": "lectures/week-05_factorial-anova.html#contrasts-for-main-effects-and-interactions",
    "title": "Factorial ANOVA",
    "section": "Contrasts for Main Effects and Interactions",
    "text": "Contrasts for Main Effects and Interactions\nRecalling the cell-means model\n\\[\ny_{ijt} = \\mu + \\tau_{ij} + \\varepsilon_{ijt},\n\\]\nequivalent to a one-way ANOVA model with \\(ab\\) treatments, we have that all contrasts in \\(\\tau_{ij}\\) are estimable. In two-way layouts, common contrasts include:\n\nTreatment Contrasts \\(\\sum_{i,j} d_{ij}\\,\\tau_{ij}\\).\nInteraction Contrasts measure non-parallelism in interaction plots. For instance,\n\n\\[\n(\\tau_{sh} - \\tau_{(s+1)h}) - (\\tau_{sq} - \\tau_{(s+1)q}).\n\\]\nUsing the two-way complete model \\(\\tau_{ij} = \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij}\\), an interaction contrast zeroes out if \\((\\alpha\\beta)_{ij}=0\\) for all \\(i,j\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#writing-contrasts-as-coefficient-lists",
    "href": "lectures/week-05_factorial-anova.html#writing-contrasts-as-coefficient-lists",
    "title": "Factorial ANOVA",
    "section": "Writing Contrasts as Coefficient Lists",
    "text": "Writing Contrasts as Coefficient Lists\nWhen working in the two-way complete model, we can express contrasts in two ways:\n\nAs a list of coefficients of \\(\\alpha_i^*\\), \\(\\beta_j^*\\), and \\((\\alpha\\beta)_{ij}\\).\nAs a list of coefficients of the \\(\\tau_{ij}\\).\n\nExample 6.3.1 (Battery Experiment) A two-factor experiment with two levels each: “duty” (1 = alkaline, 2 = heavy duty) and “brand” (1 = name brand, 2 = store brand). Treatment combinations are \\((1,1)\\), \\((1,2)\\), \\((2,1)\\), and \\((2,2)\\), each with \\(r=4\\) observations. An interaction contrast might be\n\\[\n\\tau_{11} - \\tau_{12} - \\tau_{21} + \\tau_{22}\n\\;=\\; (\\alpha\\beta)_{11} - (\\alpha\\beta)_{12} - (\\alpha\\beta)_{21} + (\\alpha\\beta)_{22}.\n\\]"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#least-squares-estimators",
    "href": "lectures/week-05_factorial-anova.html#least-squares-estimators",
    "title": "Factorial ANOVA",
    "section": "Least Squares Estimators",
    "text": "Least Squares Estimators\nFrom standard ANOVA theory,\n\n\\(\\hat{\\mu} + \\hat{\\tau}_{ij} = \\hat{\\mu} + \\hat{\\alpha}_i + \\hat{\\beta}_j + \\widehat{(\\alpha\\beta)}_{ij} = \\bar{Y}_{ij\\cdot}\\),\n\\(\\text{Var}(\\bar{Y}_{ij\\cdot}) = \\sigma^2 / r_{ij}\\).\n\nAn interaction contrast \\(\\sum_{i,j} d_{ij}\\,\\tau_{ij}\\) has LSE \\(\\sum_{i,j} d_{ij}\\,\\bar{Y}_{ij\\cdot}\\) with variance \\(\\sigma^2 \\sum d_{ij}^2/r_{ij}\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#estimation-of-sigma2",
    "href": "lectures/week-05_factorial-anova.html#estimation-of-sigma2",
    "title": "Factorial ANOVA",
    "section": "Estimation of \\(\\sigma^2\\)",
    "text": "Estimation of \\(\\sigma^2\\)\nBecause the two-way complete model is equivalent to a one-way ANOVA with \\(ab\\) cells, an unbiased estimate of \\(\\sigma^2\\) is obtained via the error sum of squares:\n\\[\nSS_{Error} = \\sum_{i,j,t} (y_{ijt} - \\bar{Y}_{ij\\cdot})^2,\n\\]\nwith \\(v = ab\\) levels and \\(n = \\sum_{i,j} r_{ij}\\) total observations. The corresponding mean square error is\n\\[\nMS_{Error} = \\frac{SS_{Error}}{n - v}.\n\\]\nExample 6.4.2 (Reaction Time Experiment) A pilot experiment with \\(a=2\\) levels of “Cue Stimulus” and \\(b=3\\) levels of “Cue Time,” each having \\(r=3\\) replications, yields data in Table 6.2. The sum of squares for error is computed as\n\\[\nSS_{Error} = \\sum_{i,j,t} y_{ijt}^2 \\;-\\; \\sum_{i,j} r_{ij}\\,\\bar{Y}_{ij\\cdot}^2 \\;=\\; 0.00347,\n\\]\nand hence \\(MS_{Error} = SS_{Error}/(n-ab)\\).\n\nMultiple Comparisons with Unequal Variances\nIf variance heterogeneity persists and cannot be corrected by transformations, one can use Satterthwaite’s approximation (Christensen 2018)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#analysis-of-variance-for-the-complete-model",
    "href": "lectures/week-05_factorial-anova.html#analysis-of-variance-for-the-complete-model",
    "title": "Factorial ANOVA",
    "section": "Analysis of Variance for the Complete Model",
    "text": "Analysis of Variance for the Complete Model\nWe commonly examine three hypotheses:\n\nInteraction \\(H_{0}^{AB}\\): \\((\\alpha\\beta)_{ij} = 0\\) for all \\(i,j\\).\nMain Effect of A \\(H_{0}^{A}\\): \\(\\alpha_1^* = \\alpha_2^* = \\cdots = \\alpha_a^*\\).\nMain Effect of B \\(H_{0}^{B}\\): \\(\\beta_1^* = \\beta_2^* = \\cdots = \\beta_b^*\\).\n\nIf \\(H_{0}^{AB}\\) is not rejected, we then focus on main effects. Otherwise, the cell-means model is typically used to compare all \\(ab\\) treatment combinations.\nExample 6.4.5 (Reaction Time Experiment, continued) An ANOVA table yields:\n\n\\(MS_{AB}/MS_{Error} = 1.46 &lt; F_{2,12,0.01} = 6.93\\) (Fail to reject interaction)\n\\(MS_{A}/MS_{Error} = 81.38 &gt; 9.33\\) (Reject \\(H_{0}^{A}\\))\n\\(MS_{B}/MS_{Error} = 2.00 &lt; 6.93\\) (Fail to reject \\(H_{0}^{B}\\))"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#least-squares-estimators-equal-r",
    "href": "lectures/week-05_factorial-anova.html#least-squares-estimators-equal-r",
    "title": "Factorial ANOVA",
    "section": "Least Squares Estimators (Equal \\(r\\))",
    "text": "Least Squares Estimators (Equal \\(r\\))\nIf \\(r\\) is constant across all \\(ab\\) cells, the LSE of \\(E[y_{ijt}] = \\mu + \\alpha_i + \\beta_j\\) is\n\\[\n\\hat{\\mu} + \\hat{\\alpha}_i + \\hat{\\beta}_j = \\bar{Y}_{i\\cdot\\cdot} + \\bar{Y}_{\\cdot j \\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}.\n\\]\nPairwise comparisons (e.g., \\(\\alpha_p - \\alpha_s\\)) become \\(\\bar{Y}_{p\\cdot\\cdot} - \\bar{Y}_{s\\cdot\\cdot}\\).\nExample 6.5.1 (Nail Varnish Experiment) Two solvents (\\(A\\)) and three brands of varnish (\\(B\\)), each with \\(r=5\\). From the data, the difference in solvents is estimated as\n\\[\n\\hat{\\alpha}_1 - \\hat{\\alpha}_2 = \\bar{Y}_{1\\cdot\\cdot} - \\bar{Y}_{2\\cdot\\cdot} \\approx -3.6040.\n\\]"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#estimation-of-sigma2-main-effects-model",
    "href": "lectures/week-05_factorial-anova.html#estimation-of-sigma2-main-effects-model",
    "title": "Factorial ANOVA",
    "section": "Estimation of \\(\\sigma^2\\) (Main-Effects Model)",
    "text": "Estimation of \\(\\sigma^2\\) (Main-Effects Model)\nWith \\(n = abr\\), the sum of squares of errors is\n\\[\nSS_{Error} = \\sum_{i=1}^a \\sum_{j=1}^b \\sum_{t=1}^{r}\n\\left( y_{ijt} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j \\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot}\\right)^2.\n\\]\nThen\n\\[\nMS_{Error} = \\frac{SS_{Error}}{n - a - b + 1}.\n\\]\nExample 6.5.2 (Nail Varnish Experiment, continued) With \\(a=2\\), \\(b=3\\), \\(r=5\\), the sum of squares \\(SS_{Error}=216.7762\\) and \\(MS_{Error} = 216.7762/(30-2-3+1) \\approx 8.3375\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#multiple-comparisons-in-the-main-effects-model",
    "href": "lectures/week-05_factorial-anova.html#multiple-comparisons-in-the-main-effects-model",
    "title": "Factorial ANOVA",
    "section": "Multiple Comparisons in the Main-Effects Model",
    "text": "Multiple Comparisons in the Main-Effects Model\nFor equal sample sizes, Tukey, Bonferroni, and Scheffé methods still apply to main-effect contrasts. For example, a set of 100\\((1-\\alpha)\\%\\) simultaneous CIs for factor-A contrasts has the form:\n\\[\n\\sum_i c_i \\alpha_i \\;\\in\\; \\sum_i c_i \\bar{Y}_{i\\cdot\\cdot}\n\\;\\pm\\; w \\; \\sqrt{MS_{Error} \\,\\sum_i \\frac{c_i^2}{br}},\n\\]\nwhere \\(w\\) depends on the chosen procedure (Dean et al. 2017).\nExample 6.5.3 (Nail Varnish Experiment, continued) Simultaneous confidence intervals for differences among three varnishes can be constructed via Tukey’s HSD with \\(q_{3,26,0.01} = 4.54\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#analysis-of-variance-for-equal-sample-sizes",
    "href": "lectures/week-05_factorial-anova.html#analysis-of-variance-for-equal-sample-sizes",
    "title": "Factorial ANOVA",
    "section": "Analysis of Variance for Equal Sample Sizes",
    "text": "Analysis of Variance for Equal Sample Sizes\nWhen \\(r\\) is constant:\n\n\\(H_{0}^B: \\{\\beta_1 = \\beta_2 = \\dots = \\beta_b\\}\\) is tested via\n\n\\[\n\\text{reject if}\\;\\; \\frac{MS_{B}}{MS_{Error}} &gt; F_{b-1,\\,n-a-b+1,\\,\\alpha}.\n\\]\n\n\\(H_{0}^A: \\{\\alpha_1 = \\alpha_2 = \\dots = \\alpha_a\\}\\) analogously.\n\nExample 6.5.4 (Nail Varnish Experiment) An ANOVA table yields \\(F\\)-ratios for factor \\(A\\) (solvent) and factor \\(B\\) (varnish). Solvent is significant while varnish is not."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#calculating-sample-sizes",
    "href": "lectures/week-05_factorial-anova.html#calculating-sample-sizes",
    "title": "Factorial ANOVA",
    "section": "Calculating Sample Sizes",
    "text": "Calculating Sample Sizes\nBoth the power-based approach (Sections 3.6) and confidence-interval-width approach (Sections 4.5) can be extended to two-way designs (Montgomery 2020). For main-effect differences of size \\(A\\) or \\(B\\), formulas such as\n\\[\nr = \\frac{2a\\,\\sigma^2\\,\\phi^2}{b\\,A^2}, \\quad\nr = \\frac{2b\\,\\sigma^2\\,\\phi^2}{a\\,B^2}\n\\]\nmay be used to assure adequate power, where \\(\\phi\\) is a function of the noncentral \\(F\\)-distribution (see also Christensen 2018)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#one-observation-per-cell",
    "href": "lectures/week-05_factorial-anova.html#one-observation-per-cell",
    "title": "Factorial ANOVA",
    "section": "One Observation Per Cell",
    "text": "One Observation Per Cell\nWhen \\(r=1\\) per cell, if an interaction is possible, we cannot estimate \\(\\sigma^2\\) directly (because \\(\\text{df for error} = ab(r-1) = 0\\)). The study design must:\n\nAssume interaction is negligible (use main-effects model), or\nHave \\(\\sigma^2\\) known in advance, or\nModel only a reduced set of contrasts (sparsity of effects)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#analysis-based-on-orthogonal-contrasts",
    "href": "lectures/week-05_factorial-anova.html#analysis-based-on-orthogonal-contrasts",
    "title": "Factorial ANOVA",
    "section": "Analysis Based on Orthogonal Contrasts",
    "text": "Analysis Based on Orthogonal Contrasts\nTwo contrasts \\(c_i\\tau_i\\) and \\(k_i\\tau_i\\) are orthogonal if\n\\[\n\\sum_i \\frac{c_i\\,k_i}{r_i} = 0.\n\\]\nA complete set of \\(v-1\\) orthogonal contrasts partitions the treatment sum of squares:\n\\[\nSS_{Total} = \\sum_{q=1}^{v-1} SS_{c_q}.\n\\]\nExample 6.7.1 (Battery Experiment, continued) Duty, brand, and interaction contrasts form a complete set of \\(3\\) orthogonal contrasts for \\(4\\) total treatments, and their sums of squares add to the total."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#tukeys-test-for-additivity",
    "href": "lectures/week-05_factorial-anova.html#tukeys-test-for-additivity",
    "title": "Factorial ANOVA",
    "section": "Tukey’s Test for Additivity",
    "text": "Tukey’s Test for Additivity\nWhen the interaction is thought to be of the form \\((\\alpha\\beta)_{ij} = \\gamma\\,\\alpha_i\\,\\beta_j\\), Tukey’s test uses only 1 df. The decision rule is\n\\[\n\\text{reject } H_0^\\gamma \\text{ if }\n\\frac{ss_{AB}^*}{SS_{Error}/e} &gt; F_{1,e,\\alpha},\n\\]\nwhere \\(ss_{AB}^*\\) is a computed sum of squares specifically for that parametric form of interaction."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#a-real-experimentair-velocity-experiment",
    "href": "lectures/week-05_factorial-anova.html#a-real-experimentair-velocity-experiment",
    "title": "Factorial ANOVA",
    "section": "A Real Experiment—Air Velocity Experiment",
    "text": "A Real Experiment—Air Velocity Experiment\nTable 6.11 shows data on air velocity for three rib heights (\\(A\\)) and six Reynolds numbers (\\(B\\)) with \\(r=1\\) per cell. Investigators suspected some interaction but believed certain higher-order trends would be negligible. They pooled those negligible interaction contrasts for an error estimate with 3 df, then tested the remaining contrasts individually. See Table 6.12 for the resulting ANOVA."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#model-checking-and-residual-diagnostics",
    "href": "lectures/week-05_factorial-anova.html#model-checking-and-residual-diagnostics",
    "title": "Factorial ANOVA",
    "section": "Model Checking and Residual Diagnostics",
    "text": "Model Checking and Residual Diagnostics\nBefore finalizing an ANOVA conclusion, we must check:\n\nIndependence: Plot residuals vs. observation order.\nConstant Variance: Plot residuals (or standardized residuals) vs. fitted values, factor levels, or treatment combos.\nNormality: Use a Normal Q-Q plot of residuals.\n\nThe standardized residual often used is:\n\\[\nz_{ijt} = \\frac{y_{ijt} - \\hat{y}_{ijt}}{\\sqrt{\\tfrac{SS_{Error}}{n - 1}}}.\n\\]\nDepending on whether we assume a complete or main-effects model, \\(\\hat{y}_{ijt}\\) includes:\n\n\\(\\hat{\\tau}_{ij} = \\hat{\\alpha}_i + \\hat{\\beta}_j + \\widehat{(\\alpha\\beta)}_{ij}\\) (complete model), or\n\\(\\hat{\\tau}_{ij} = \\hat{\\alpha}_i + \\hat{\\beta}_j\\) (main-effects model)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#fitting-the-two-way-complete-model",
    "href": "lectures/week-05_factorial-anova.html#fitting-the-two-way-complete-model",
    "title": "Factorial ANOVA",
    "section": "Fitting the Two-Way Complete Model",
    "text": "Fitting the Two-Way Complete Model\nA typical R formula for the two-way complete model is:\nmodelAB = aov(y ~ fA + fB + fA:fB, data = react_df)\nor equivalently:\nmodelAB = aov(y ~ fA * fB, data = react_df)\nHere, fA and fB are factor variables in R. The operator * adds both main effects and the interaction term. We often check:\n\nanova(modelAB)        # \"Type I\" sums of squares\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nfA\n1\n0.0235445\n0.0235445\n81.375000\n0.0000011\n\n\nfB\n2\n0.0011581\n0.0005791\n2.001344\n0.1777992\n\n\nfA:fB\n2\n0.0008463\n0.0004232\n1.462558\n0.2701353\n\n\nResiduals\n12\n0.0034720\n0.0002893\nNA\nNA\n\n\n\n\n\ndrop1(modelAB, ~., test = \"F\")  # \"Type III\" sums of squares\n\n\n\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n\n\nNA\nNA\n0.0034720\n-141.9611\nNA\nNA\n\n\nfA\n1\n0.0104167\n0.0138887\n-119.0070\n36.002304\n0.0000621\n\n\nfB\n2\n0.0018802\n0.0053522\n-138.1711\n3.249232\n0.0745201\n\n\nfA:fB\n2\n0.0008463\n0.0043183\n-142.0346\n1.462558\n0.2701353"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#type-i-vs.-type-iii-sums-of-squares",
    "href": "lectures/week-05_factorial-anova.html#type-i-vs.-type-iii-sums-of-squares",
    "title": "Factorial ANOVA",
    "section": "Type I vs. Type III Sums of Squares",
    "text": "Type I vs. Type III Sums of Squares\n\nType I (Sequential) sums of squares: Variation explained by adding each term in sequence.\nType III sums of squares: Variation explained by each term after adjusting for all other terms in the model.\n\nRequires options(contrasts = c(\"contr.sum\", \"contr.poly\")) for balanced constraints."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#fitting-the-cell-means-model",
    "href": "lectures/week-05_factorial-anova.html#fitting-the-cell-means-model",
    "title": "Factorial ANOVA",
    "section": "Fitting the Cell-Means Model",
    "text": "Fitting the Cell-Means Model\nAn alternative is the cell-means model, which estimates each \\(\\tau_{ij}\\) directly:\nmodelTC = aov(y ~ fTC, data = react_df)\nwhere fTC is a factor that indicates each treatment combination (i,j). This approach is ideal for more direct comparisons of the individual cell means."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#example-reaction-time-experiment",
    "href": "lectures/week-05_factorial-anova.html#example-reaction-time-experiment",
    "title": "Factorial ANOVA",
    "section": "Example: Reaction Time Experiment",
    "text": "Example: Reaction Time Experiment\nIn the pasted notes, an experiment with Factor A = “Cue Stimulus” (a=2a=2 levels), Factor B = “Cue Time” (b=3b=3 levels), and unequal replicates is demonstrated. Output from anova(modelAB), drop1(modelAB, ~., test=\"F\"), and anova(modelTC):\n\nInteraction not significant,\nFactor A is significant,\nFactor B is not significant."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#main-effect-contrasts",
    "href": "lectures/week-05_factorial-anova.html#main-effect-contrasts",
    "title": "Factorial ANOVA",
    "section": "Main-Effect Contrasts",
    "text": "Main-Effect Contrasts\n\nlsmB  &lt;-  emmeans::emmeans(modelAB, ~ fB)\nsummary(contrast(lsmB, list(B12 = c(1, -1, 0))), infer = c(TRUE, TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nB12\n0.0076667\n0.0098206\n12\n-0.0137306\n0.0290639\n0.7806709\n0.4501222\n\n\n\n\n\n\nThis obtains the contrast \\(\\beta_1 - \\beta_2\\) for Factor B. We can similarly construct more complex contrasts or even interaction contrasts:\n\nlsmAB = emmeans::emmeans(modelAB, ~ fB:fA)\nsummary(contrast(lsmAB, list(AB = c(1, 0, -1, -1, 0, 1))), infer=c(TRUE,TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nAB\n-0.0303333\n0.0196412\n12\n-0.0731279\n0.0124612\n-1.544371\n0.1484504"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#multiple-comparison-procedures",
    "href": "lectures/week-05_factorial-anova.html#multiple-comparison-procedures",
    "title": "Factorial ANOVA",
    "section": "Multiple Comparison Procedures",
    "text": "Multiple Comparison Procedures\nSeveral methods adjust for familywise error:\n\nTukey’s HSD\nBonferroni\nScheffé\nDunnett (comparing all treatments to a control)\n\n\nTukey Example\n\nsummary(\n  contrast(lsmB, method=\"pairwise\", adjust=\"tukey\"),\n  infer = c(TRUE, TRUE),\n  level = 0.99\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nfB1 - fB2\n0.0076667\n0.0098206\n12\n-0.0273734\n0.0427068\n0.7806709\n0.7214544\n\n\nfB1 - fB3\n-0.0118333\n0.0098206\n12\n-0.0468734\n0.0232068\n-1.2049485\n0.4727382\n\n\nfB2 - fB3\n-0.0195000\n0.0098206\n12\n-0.0545401\n0.0155401\n-1.9856194\n0.1581646\n\n\n\n\n\n\nGenerates pairwise comparisons among the levels of B with Tukey’s adjustment at 99% confidence."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#dunnett-example",
    "href": "lectures/week-05_factorial-anova.html#dunnett-example",
    "title": "Factorial ANOVA",
    "section": "Dunnett Example",
    "text": "Dunnett Example\n\nsummary(\n  contrast(lsmB, method=\"trt.vs.ctrl\", adj=\"mvt\", ref=1),\n  infer = c(TRUE, TRUE),\n  level = 0.99\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nfB2 - fB1\n-0.0076667\n0.0098206\n12\n-0.0409348\n0.0256015\n-0.7806709\n0.6611256\n\n\nfB3 - fB1\n0.0118333\n0.0098206\n12\n-0.0214348\n0.0451015\n1.2049485\n0.4032893\n\n\n\n\n\n\nCompares each level of B to the control (the first factor level by default)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#intuitive-understanding",
    "href": "lectures/week-05_factorial-anova.html#intuitive-understanding",
    "title": "Factorial ANOVA",
    "section": "Intuitive Understanding",
    "text": "Intuitive Understanding\nIn statistical modeling, especially in Analysis of Variance (ANOVA), the concepts of balanced and unbalanced designs are fundamental:\n\nBalanced Design: Each factor-level combination has the same number of observations, making calculations straightforward and allowing different sums of squares methods (Type I, II, III) to yield the same results.\nUnbalanced Design: Some factor levels have more observations than others, complicating interpretations and requiring careful choice of sums of squares to ensure valid comparisons.\n\nWhy does balance matter?\n\nIn a balanced design, main effects and interaction effects are easier to interpret because all levels contribute equally to variance.\nIn an unbalanced design, estimates depend on the order of factor entry in the model, leading to different results based on the sum of squares method used."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#type-i-sequential-sums-of-squares",
    "href": "lectures/week-05_factorial-anova.html#type-i-sequential-sums-of-squares",
    "title": "Factorial ANOVA",
    "section": "1. Type I (Sequential) Sums of Squares",
    "text": "1. Type I (Sequential) Sums of Squares\n\nType I SS calculates the sum of squares sequentially based on the order in which factors are entered into the model.\nEach factor’s effect is tested before adjusting for other factors.\nThe order of entry matters, meaning later factors are tested after accounting for earlier ones."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#mathematical-formulation",
    "href": "lectures/week-05_factorial-anova.html#mathematical-formulation",
    "title": "Factorial ANOVA",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\nFor a model with two factors, \\(A\\) and \\(B\\), and their interaction:\n\\[\nSS_{\\text{Type I}} = SS(A) + SS(B \\mid A) + SS(A \\times B \\mid A, B)\n\\]\nwhere:\n\n\\(SS(A)\\) tests the effect of A alone.\n\\(SS(B \\mid A)\\) tests B after accounting for A.\n\\(SS(A \\times B \\mid A, B)\\) tests the interaction after both main effects."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#when-to-use-type-i-ss",
    "href": "lectures/week-05_factorial-anova.html#when-to-use-type-i-ss",
    "title": "Factorial ANOVA",
    "section": "When To Use Type I SS?",
    "text": "When To Use Type I SS?\n\nFor hierarchical models where the order of factor entry reflects natural causation.\nFor regression models where variables are added in a meaningful sequence.\nFor unbalanced data, use with caution—it is biased toward factors entered earlier."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#type-ii-adjusted-sums-of-squares",
    "href": "lectures/week-05_factorial-anova.html#type-ii-adjusted-sums-of-squares",
    "title": "Factorial ANOVA",
    "section": "2. Type II (Adjusted) Sums of Squares",
    "text": "2. Type II (Adjusted) Sums of Squares\n\nType II SS tests each main effect after accounting for all other main effects but without interactions.\nAssumes no interaction between factors when computing main effects."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#mathematical-formulation-1",
    "href": "lectures/week-05_factorial-anova.html#mathematical-formulation-1",
    "title": "Factorial ANOVA",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\n\\[\nSS_{\\text{Type II}} = SS(A \\mid B) + SS(B \\mid A) + SS(A \\times B \\mid A, B)\n\\]\nwhere:\n\n\\(SS(A \\mid B)\\) tests A after controlling for B but ignores the interaction.\n\\(SS(B \\mid A)\\) tests B similarly.\nThe interaction term is tested last."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#when-to-use-type-ii-ss",
    "href": "lectures/week-05_factorial-anova.html#when-to-use-type-ii-ss",
    "title": "Factorial ANOVA",
    "section": "When To Use Type II SS?",
    "text": "When To Use Type II SS?\n\nWhen interactions are not significant and main effects are of primary interest.\nFor regression models where predictors are independent.\nFor unbalanced designs, when interactions are weak."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#type-iii-fully-adjusted-sums-of-squares",
    "href": "lectures/week-05_factorial-anova.html#type-iii-fully-adjusted-sums-of-squares",
    "title": "Factorial ANOVA",
    "section": "3. Type III (Fully Adjusted) Sums of Squares",
    "text": "3. Type III (Fully Adjusted) Sums of Squares\n\nType III SS adjusts for all other terms in the model, including interactions.\nIt tests each factor as if it were the last entered, making it the most interpretable for unbalanced designs."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#mathematical-formulation-2",
    "href": "lectures/week-05_factorial-anova.html#mathematical-formulation-2",
    "title": "Factorial ANOVA",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\n\\[\nSS_{\\text{Type III}} = SS(A \\mid B, A \\times B) + SS(B \\mid A, A \\times B) + SS(A \\times B \\mid A, B)\n\\]\nwhere:\n\nEach term is tested after controlling for all other effects, including interactions."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#when-to-use-type-iii-ss",
    "href": "lectures/week-05_factorial-anova.html#when-to-use-type-iii-ss",
    "title": "Factorial ANOVA",
    "section": "When To Use Type III SS?",
    "text": "When To Use Type III SS?\n\nFor unbalanced designs where fair comparisons between factors are needed.\nFor factorial ANOVA models when interactions are present.\nFor categorical variables, especially with contrasts like contr.sum."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#alternative-approach-drop1-for-type-iii-ss",
    "href": "lectures/week-05_factorial-anova.html#alternative-approach-drop1-for-type-iii-ss",
    "title": "Factorial ANOVA",
    "section": "Alternative Approach: drop1() for Type III SS",
    "text": "Alternative Approach: drop1() for Type III SS\nAnother way to obtain Type III SS in R is:\ndrop1(model, ~., test = \"F\")  # Ensures Type III if contrasts are set correctly\nThis method is equivalent to car::Anova() but requires contrast settings to be explicitly defined beforehand."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#comparison-of-sums-of-squares",
    "href": "lectures/week-05_factorial-anova.html#comparison-of-sums-of-squares",
    "title": "Factorial ANOVA",
    "section": "Comparison Of Sums of Squares",
    "text": "Comparison Of Sums of Squares\n\n\n\n\n\n\n\n\n\n\n\nSS Type\nInterpretation\nOrder Matters?\nBest for Balanced?\nBest for Unbalanced?\nInteraction Handling\n\n\n\n\nType I\nSequential\nYes\nYes\nNo\nIgnored at earlier levels\n\n\nType II\nAdjusted (main effects only)\nNo\nYes\nSome cases\nIgnores interactions\n\n\nType III\nFully adjusted\nNo\nYes\nYes (preferred)\nAlways accounts for interactions"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#conceptual-questions",
    "href": "lectures/week-05_factorial-anova.html#conceptual-questions",
    "title": "Factorial ANOVA",
    "section": "Conceptual Questions",
    "text": "Conceptual Questions\n\nWhy do Type I, II, and III sums of squares give different results in unbalanced designs?\nWhen would you use Type II over Type III SS?\nWhat happens if interaction terms are ignored in an unbalanced ANOVA?\nWhy is Type III SS preferred for interpreting main effects in unbalanced designs?\nHow does contrast coding (contr.treatment vs contr.sum) affect Type III SS calculations?"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#computational-questions",
    "href": "lectures/week-05_factorial-anova.html#computational-questions",
    "title": "Factorial ANOVA",
    "section": "Computational Questions",
    "text": "Computational Questions\n\nGiven the dataset below, determine if it is balanced:\nFactorA  FactorB  Response\nA1       B1       10\nA1       B1       12\nA1       B2       15\nA2       B1       9\nA2       B2       11\nA2       B2       14\nIs the dataset balanced? Compute Type III SS using car::Anova().\nSimulate an unbalanced dataset and fit a two-way ANOVA. Compute Type I, II, and III SS in R.\nInterpret the differences between Type I, II, and III SS in a factorial ANOVA.\nModify contrast settings in R and observe their effect on Type III SS.\nExplain why drop1() and car::Anova() may give different results if contrasts are not set correctly."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#detailed-solutions",
    "href": "lectures/week-05_factorial-anova.html#detailed-solutions",
    "title": "Factorial ANOVA",
    "section": "Detailed Solutions",
    "text": "Detailed Solutions\n\nDataset Balance Check\n\nThe dataset is unbalanced since the number of observations per condition is unequal.\nCompute Type III SS:\n\nlibrary(car)\nmodel &lt;- lm(Response ~ FactorA * FactorB, data = dataset)\nAnova(model, type = \"III\")\nSimulating Unbalanced Data and Computing SS\n\n\n\n\n# Set the seed for reproducibility\nset.seed(42)\n\n# Define the levels for each factor\nfactor_A &lt;- c(\"A1\", \"A2\")\nfactor_B &lt;- c(\"B1\", \"B2\")\n\n# Create a data frame with the factor combinations\ndf &lt;- expand.grid(A = factor_A, B = factor_B)\n\n# Repeat each combination 100 times\ndf &lt;- df[rep(1:nrow(df), each = 50), ]\n\n# Introduce imbalance by removing some samples from one of the factor combinations\ndf &lt;- df |&gt;  \n  filter(!(A == \"A1\" & B == \"B2\")) |&gt;  \n  add_row(df |&gt;  filter(A == \"A1\" & B == \"B2\") |&gt;  sample_n(50))\n\n# Add some response variable (e.g., continuous or categorical)\ndf$response &lt;- rnorm(nrow(df))\n\n# Print the first few rows of the data frame\nprint(df |&gt;  head())\n\n     A  B   response\n1   A1 B1 -0.6089264\n1.1 A1 B1  0.5049551\n1.2 A1 B1 -1.7170087\n1.3 A1 B1 -0.7844590\n1.4 A1 B1 -0.8509076\n1.5 A1 B1 -2.4142076\n\n\n\nTable 5: Simulating Unbalanced Data and Computing SS\n\n\n\n\nmodel &lt;- lm(response ~ A * B, data = df)\ncar::Anova(model, type = \"III\") # Type III SS\n\n\n\n\n\n\nSum Sq\nDf\nF value\nPr(&gt;F)\n\n\n\n\n(Intercept)\n0.0122168\n1\n0.0139678\n0.9060417\n\n\nA\n0.4909088\n1\n0.5612696\n0.4546473\n\n\nB\n0.0966028\n1\n0.1104487\n0.7399890\n\n\nA:B\n0.5338829\n1\n0.6104030\n0.4355788\n\n\nResiduals\n171.4294367\n196\nNA\nNA\n\n\n\n\n\ndrop1(model, ~., test = \"F\") # Type III SS\n\n\n\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n\n\nNA\nNA\n171.4294\n-22.82913\nNA\nNA\n\n\nA\n1\n0.4909088\n171.9203\n-24.25722\n0.5612696\n0.4546473\n\n\nB\n1\n0.0966028\n171.5260\n-24.71646\n0.1104487\n0.7399890\n\n\nA:B\n1\n0.5338829\n171.9633\n-24.20723\n0.6104030\n0.4355788\n\n\n\n\n\nanova(model)\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nA\n1\n0.0677010\n0.0677010\n0.0774045\n0.7811394\n\n\nB\n1\n0.0847516\n0.0847516\n0.0968988\n0.7559143\n\n\nA:B\n1\n0.5338829\n0.5338829\n0.6104030\n0.4355788\n\n\nResiduals\n196\n171.4294367\n0.8746400\nNA\nNA\n\n\n\n\n\n\n\nInterpreting SS Differences\n\nType I results will change based on factor order.\nType II ignores interactions.\nType III ensures fair comparisons."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#conclusion",
    "href": "lectures/week-05_factorial-anova.html#conclusion",
    "title": "Factorial ANOVA",
    "section": "Conclusion",
    "text": "Conclusion\n\nUse Type I for hierarchical models where order matters.\nUse Type II when interactions are not of primary interest.\nUse Type III for unbalanced designs or when interactions exist.\n\nBy setting contrasts correctly, car::Anova() ensures valid Type III SS computations, making comparisons fair in real-world ANOVA applications."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#if-interaction-is-significant",
    "href": "lectures/week-05_factorial-anova.html#if-interaction-is-significant",
    "title": "Factorial ANOVA",
    "section": "If Interaction is Significant",
    "text": "If Interaction is Significant\nGiven the battery example suggests a significant interaction, do the following:\n\nCell Mean Comparison: Use emmeans(model_full, ~ BatteryType:Brand) and pairwise tests.\nInterpret if brand difference is larger for Alkaline vs. Heavy Duty, or if battery type difference is larger for one brand.\nPossibly produce an interaction plot to visualize how lines cross or deviate from parallel."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#if-interaction-was-non-significant",
    "href": "lectures/week-05_factorial-anova.html#if-interaction-was-non-significant",
    "title": "Factorial ANOVA",
    "section": "If Interaction Was Non-Significant",
    "text": "If Interaction Was Non-Significant\n\nRefit a simpler model: model_main &lt;- aov(Life ~ BatteryType + Brand, data=battery_data).\nCompare main effects of BatteryType, Brand with TukeyHSD or emmeans."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#practical-implementation-tips",
    "href": "lectures/week-05_factorial-anova.html#practical-implementation-tips",
    "title": "Factorial ANOVA",
    "section": "Practical Implementation Tips",
    "text": "Practical Implementation Tips\n\n# Interaction EMM\nemm_inter &lt;- emmeans(modelAB, ~ fA:fB)\npairs(emm_inter, adjust=\"tukey\")\n\n contrast          estimate     SE df t.ratio p.value\n fA1 fB1 - fA2 fB1 -0.08333 0.0139 12  -6.000  0.0007\n fA1 fB1 - fA1 fB2  0.00633 0.0139 12   0.456  0.9969\n fA1 fB1 - fA2 fB2 -0.07433 0.0139 12  -5.352  0.0018\n fA1 fB1 - fA1 fB3 -0.02700 0.0139 12  -1.944  0.4235\n fA1 fB1 - fA2 fB3 -0.08000 0.0139 12  -5.760  0.0010\n fA2 fB1 - fA1 fB2  0.08967 0.0139 12   6.456  0.0003\n fA2 fB1 - fA2 fB2  0.00900 0.0139 12   0.648  0.9845\n fA2 fB1 - fA1 fB3  0.05633 0.0139 12   4.056  0.0154\n fA2 fB1 - fA2 fB3  0.00333 0.0139 12   0.240  0.9999\n fA1 fB2 - fA2 fB2 -0.08067 0.0139 12  -5.808  0.0009\n fA1 fB2 - fA1 fB3 -0.03333 0.0139 12  -2.400  0.2300\n fA1 fB2 - fA2 fB3 -0.08633 0.0139 12  -6.216  0.0005\n fA2 fB2 - fA1 fB3  0.04733 0.0139 12   3.408  0.0460\n fA2 fB2 - fA2 fB3 -0.00567 0.0139 12  -0.408  0.9982\n fA1 fB3 - fA2 fB3 -0.05300 0.0139 12  -3.816  0.0231\n\nP value adjustment: tukey method for comparing a family of 6 estimates \n\n\n\nAlways test for interaction first.\nIf \\(p &lt; \\alpha\\) for interaction: do not rely on main-effect means.\nUse emmeans for user-friendly post hoc analysis:\n\npairs(emm_inter, adjust=\"tukey\") for pairwise comparisons.\ncontrast(emm_inter, list(...)) for custom contrasts.\n\nDiagnostic plots are crucial to validate assumptions.\nUnbalanced data: consider Type III SS for interpretative parity.\nInteraction plots can help visualize the interaction effect.\nCustom contrasts can be useful for specific comparisons."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#one-observation-per-cell-small-experiments",
    "href": "lectures/week-05_factorial-anova.html#one-observation-per-cell-small-experiments",
    "title": "Factorial ANOVA",
    "section": "One Observation per Cell (Small Experiments)",
    "text": "One Observation per Cell (Small Experiments)\nWhen r=1r=1 per cell, there is no direct error estimate under the full two-way complete model—the error df is zero. One can:\n\nAssume interaction is negligible (use main-effects model only),\nKnow \\(\\sigma^2\\) a priori,\nUse orthogonal contrasts to identify “negligible” components of the interaction, thereby pooling them to form an error estimate."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#orthogonal-contrasts",
    "href": "lectures/week-05_factorial-anova.html#orthogonal-contrasts",
    "title": "Factorial ANOVA",
    "section": "Orthogonal Contrasts",
    "text": "Orthogonal Contrasts\nTwo contrasts are orthogonal if their corresponding sums of squares are uncorrelated. Equivalently, for equal \\(r_{ij}=r\\), the sum of products of coefficients must be zero."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#model-building-and-type-i-ss",
    "href": "lectures/week-05_factorial-anova.html#model-building-and-type-i-ss",
    "title": "Factorial ANOVA",
    "section": "Model Building and Type I SS",
    "text": "Model Building and Type I SS\nFor building or refining a model, Type I SS can be used to see how adding each effect in sequence explains residual variation."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#r-code-examples-for-plotting",
    "href": "lectures/week-05_factorial-anova.html#r-code-examples-for-plotting",
    "title": "Factorial ANOVA",
    "section": "R Code Examples for Plotting",
    "text": "R Code Examples for Plotting\n\nResidual plots:\n\n\nres &lt;- residuals(modelAB)\nfits &lt;- fitted(modelAB)\nplot(res ~ fits)\n\n\n\n\n\n\n\nqqnorm(res); qqline(res)\n\n\n\n\n\n\n\n\n\nInteraction plots:\n\ninteraction.plot(x.factor = react.data$fA, \n                 trace.factor = react.data$fB,\n                 response = react.data$y, \n                 type=\"b\", \n                 xlab=\"A\", \n                 trace.label=\"B\", \n                 ylab=\"Mean of y\")"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#conclusion-1",
    "href": "lectures/week-05_factorial-anova.html#conclusion-1",
    "title": "Factorial ANOVA",
    "section": "Conclusion",
    "text": "Conclusion\nIn these lecture notes, we have explored:\n\nTwo-way ANOVA models (complete, main-effects) and their key assumptions.\nModel checking via residual diagnostics to confirm normality, equal variances, and independence.\nContrasts and multiple comparisons in R using lsmeans (or emmeans), including Type I vs. Type III sums of squares for unbalanced designs.\nHandling special cases, such as one observation per cell, by leveraging orthogonal contrasts or known variance assumptions.\n\nThese methods allow us to make rigorous inferences about factor main effects and interactions in a two-factor experimental setup. In practice, thorough model checking (especially when cell counts are unequal) ensures valid scientific conclusions and properly calibrated multiple comparisons."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#assignments",
    "href": "lectures/week-05_factorial-anova.html#assignments",
    "title": "Factorial ANOVA",
    "section": "Assignments",
    "text": "Assignments\n\nResidual Diagnostics\n\nSimulate a two-way ANOVA dataset in R, fit the complete model, and produce diagnostic plots for normality, equal variances, and independence.\n\nInteraction Plot Interpretation\n\nProvide a small dataset with potential interaction. Plot the interaction and propose how you would test the no-interaction hypothesis.\n\nOrthogonal Contrasts\n\nFor a \\(2\\times 3\\) experiment, list a complete set of orthogonal contrasts for the interaction. Verify their orthogonality.\n\nPower Calculation\n\nAssume a \\(2\\times 2\\) design with equal sample size \\(r\\) per cell and desired power 0.80 to detect a main-effect difference of 2 (with \\(\\sigma^2\\) estimated at 3). Use the power-based formula to find \\(r\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#exams",
    "href": "lectures/week-05_factorial-anova.html#exams",
    "title": "Factorial ANOVA",
    "section": "Exams",
    "text": "Exams\nConsider an exam scenario with both conceptual and computational questions:\n\nConceptual\n\nDefine the assumptions of the two-way complete model. How do you test them?\nExplain why we typically test interaction first before examining main effects.\n\nComputational\n\nGiven raw data for a \\(3 \\times 4\\) design, compute and fill in an ANOVA table. Perform an \\(F\\)-test for interaction at \\(\\alpha=0.05\\).\n\nShort Essay\n\nCompare and contrast the two-way complete model and the two-way main-effects model. Include real-world examples."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#visual-aids",
    "href": "lectures/week-05_factorial-anova.html#visual-aids",
    "title": "Factorial ANOVA",
    "section": "Visual Aids",
    "text": "Visual Aids\n\nFigure: Interaction Plots for typical data sets showing:\n\nParallel lines (no interaction).\nClearly crossing lines (significant interaction).\n\nFigure: Residual Plots illustrating typical patterns (equal vs. unequal variances, outliers).\nTable Templates for summarizing sample means, sum of squares, and mean squares."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#appendix-advanced-proofs-and-derivations",
    "href": "lectures/week-05_factorial-anova.html#appendix-advanced-proofs-and-derivations",
    "title": "Factorial ANOVA",
    "section": "Appendix: Advanced Proofs and Derivations",
    "text": "Appendix: Advanced Proofs and Derivations\n\nProof of Orthogonality Condition\n\nShow that two contrasts are uncorrelated \\(\\iff \\sum (d_{ij} , h_{ij} / r_{ij}) = 0\\).\n\nDerivation of LSE under Main-Effects Model (Unequal \\(r\\))\n\nSolve the normal equations for \\(\\hat{\\alpha}_i, \\hat{\\beta}_j\\) when \\(r_{ij}\\) differ."
  },
  {
    "objectID": "lectures/week-07_midterm-review.html",
    "href": "lectures/week-07_midterm-review.html",
    "title": "Midterm Review",
    "section": "",
    "text": "What is the primary difference between an observational study and a designed experiment?\n\n\nObservational studies actively manipulate variables, while designed experiments do not.\n\n\n\nObservational studies establish causation, while designed experiments establish correlation.\n\n\n\nObservational studies do not impose treatments, while designed experiments actively manipulate variables.\n\n\n\nObservational studies always involve randomization, while designed experiments do not.\n\n\nExplain why randomization is a key principle in the design of experiments.\nWhat are the three basic principles of experimental design? Provide a brief explanation of each.\n\n\n\n\n\nSuppose a study is conducted on the effect of a new fertilizer on crop yield. There are two fertilizer types (A, B) and three different soil conditions (1, 2, 3). If each combination is tested on four plots, how many experimental units are there?\nA completely randomized design is conducted with 4 treatments and 5 replicates per treatment. Construct an appropriate ANOVA table with sources of variation and degrees of freedom.\n\n\n\n\n\n\n\n\nDefine the following terms:\n\nPopulation vs. Sample\n\nParameter vs. Statistic\n\nSampling Distribution\n\nWhy is it important for a sample to be random when conducting statistical inference?\nWhich of the following is NOT a property of the normal distribution?\n\n\nIt is symmetric about its mean.\n\n\nIt has a mean of 0 and a standard deviation of 1 in all cases.\n\n\nThe total area under the curve is 1.\n\n\nThe mean, median, and mode are equal.\n\n\n\n\n\n\n\nThe lifespan of a type of light bulb follows a normal distribution with a mean of 1200 hours and a standard deviation of 100 hours. What is the probability that a randomly selected bulb lasts more than 1300 hours?\nA random sample of 40 observations is drawn from a normal population with μ = 50 and σ = 10. Compute the probability that the sample mean is greater than 52.\n\n\n\n\n\n\n\n\nWhat is the purpose of blocking in an experiment? When is it appropriate?\nWhich of the following statements about randomized complete block designs (RCBD) is true?\n\n\nIt is used to remove known sources of variability.\n\n\nIt requires an equal number of observations for each treatment.\n\n\nIt assumes treatments and blocks are independent.\n\n\nAll of the above.\n\n\nDescribe the difference between fixed effects and random effects models.\n\n\n\n\n\nAn experiment tests the effect of three diets on pig weight gain using 15 pigs (5 per diet). Given:\n\nSST = 400\n\nSSTr = 300\n\nSSE = 100\nConstruct the ANOVA table and test significance at α = 0.05.\n\nIn an experiment with four treatment levels (6 observations per treatment), compute the degrees of freedom for:\n\nTreatment\n\nError\n\nTotal\n\n\n\n\n\n\n\n\n\nWhat is the main advantage of factorial designs over one-factor-at-a-time experiments?\nIn a \\(2^2\\) factorial design with two factors at two levels, how many treatment combinations exist?\nHow do you interpret the main effect and interaction effect in a factorial design?\n\n\n\n\n\nIn a \\(2^2\\) factorial experiment (Factors A and B with levels A1, A2 and B1, B2), given the treatment means:\n\n\n\nTreatment\nMean Response\n\n\n\n\nA1B1\n20\n\n\nA1B2\n30\n\n\nA2B1\n25\n\n\nA2B2\n35\n\n\n\nCalculate:\n\nThe main effect of A\n\nThe main effect of B\n\nThe interaction effect\n\nA \\(2^3\\) factorial experiment with factors A, B, and C is replicated twice per treatment. How many total observations are required?\n\n\n\n\n\n\n\n\nWhat assumptions must be fulfilled for a one-way ANOVA?\nIn a two-way ANOVA, how many hypotheses are tested and what do they represent?\nWhich ANOVA model is used to compare the effectiveness of three fertilizers on crop yield?\n\n\n\n\n\nA one-way ANOVA is run on three groups (\\(n_1 = 8, n_2 = 8, n_3 = 8\\)) with:\n\nSST = 90\n\nSSTr = 60\n\nSSE = 30\nConstruct the ANOVA table and test significance at α = 0.05.\n\nIn a two-way ANOVA with the following sums of squares:\n\nSST = 180, SSA = 60, SSB = 40, SSAB = 30, SSE = 50\nCompute the F-ratios for Factor A, Factor B, and the interaction term. Assume a total sample size of 30."
  },
  {
    "objectID": "lectures/week-07_midterm-review.html#chapter-1-introduction-to-design-and-analysis-of-experiments",
    "href": "lectures/week-07_midterm-review.html#chapter-1-introduction-to-design-and-analysis-of-experiments",
    "title": "Midterm Review",
    "section": "",
    "text": "What is the primary difference between an observational study and a designed experiment?\n\n\nObservational studies actively manipulate variables, while designed experiments do not.\n\n\n\nObservational studies establish causation, while designed experiments establish correlation.\n\n\n\nObservational studies do not impose treatments, while designed experiments actively manipulate variables.\n\n\n\nObservational studies always involve randomization, while designed experiments do not.\n\n\nExplain why randomization is a key principle in the design of experiments.\nWhat are the three basic principles of experimental design? Provide a brief explanation of each.\n\n\n\n\n\nSuppose a study is conducted on the effect of a new fertilizer on crop yield. There are two fertilizer types (A, B) and three different soil conditions (1, 2, 3). If each combination is tested on four plots, how many experimental units are there?\nA completely randomized design is conducted with 4 treatments and 5 replicates per treatment. Construct an appropriate ANOVA table with sources of variation and degrees of freedom."
  },
  {
    "objectID": "lectures/week-07_midterm-review.html#chapter-2-basic-statistical-concepts-in-experimental-design",
    "href": "lectures/week-07_midterm-review.html#chapter-2-basic-statistical-concepts-in-experimental-design",
    "title": "Midterm Review",
    "section": "",
    "text": "Define the following terms:\n\nPopulation vs. Sample\n\nParameter vs. Statistic\n\nSampling Distribution\n\nWhy is it important for a sample to be random when conducting statistical inference?\nWhich of the following is NOT a property of the normal distribution?\n\n\nIt is symmetric about its mean.\n\n\nIt has a mean of 0 and a standard deviation of 1 in all cases.\n\n\nThe total area under the curve is 1.\n\n\nThe mean, median, and mode are equal.\n\n\n\n\n\n\n\nThe lifespan of a type of light bulb follows a normal distribution with a mean of 1200 hours and a standard deviation of 100 hours. What is the probability that a randomly selected bulb lasts more than 1300 hours?\nA random sample of 40 observations is drawn from a normal population with μ = 50 and σ = 10. Compute the probability that the sample mean is greater than 52."
  },
  {
    "objectID": "lectures/week-07_midterm-review.html#chapter-3-completely-randomized-designs",
    "href": "lectures/week-07_midterm-review.html#chapter-3-completely-randomized-designs",
    "title": "Midterm Review",
    "section": "",
    "text": "What is the purpose of blocking in an experiment? When is it appropriate?\nWhich of the following statements about randomized complete block designs (RCBD) is true?\n\n\nIt is used to remove known sources of variability.\n\n\nIt requires an equal number of observations for each treatment.\n\n\nIt assumes treatments and blocks are independent.\n\n\nAll of the above.\n\n\nDescribe the difference between fixed effects and random effects models.\n\n\n\n\n\nAn experiment tests the effect of three diets on pig weight gain using 15 pigs (5 per diet). Given:\n\nSST = 400\n\nSSTr = 300\n\nSSE = 100\nConstruct the ANOVA table and test significance at α = 0.05.\n\nIn an experiment with four treatment levels (6 observations per treatment), compute the degrees of freedom for:\n\nTreatment\n\nError\n\nTotal"
  },
  {
    "objectID": "lectures/week-07_midterm-review.html#chapter-4-factorial-designs",
    "href": "lectures/week-07_midterm-review.html#chapter-4-factorial-designs",
    "title": "Midterm Review",
    "section": "",
    "text": "What is the main advantage of factorial designs over one-factor-at-a-time experiments?\nIn a \\(2^2\\) factorial design with two factors at two levels, how many treatment combinations exist?\nHow do you interpret the main effect and interaction effect in a factorial design?\n\n\n\n\n\nIn a \\(2^2\\) factorial experiment (Factors A and B with levels A1, A2 and B1, B2), given the treatment means:\n\n\n\nTreatment\nMean Response\n\n\n\n\nA1B1\n20\n\n\nA1B2\n30\n\n\nA2B1\n25\n\n\nA2B2\n35\n\n\n\nCalculate:\n\nThe main effect of A\n\nThe main effect of B\n\nThe interaction effect\n\nA \\(2^3\\) factorial experiment with factors A, B, and C is replicated twice per treatment. How many total observations are required?"
  },
  {
    "objectID": "lectures/week-07_midterm-review.html#chapter-5-analysis-of-variance-anova",
    "href": "lectures/week-07_midterm-review.html#chapter-5-analysis-of-variance-anova",
    "title": "Midterm Review",
    "section": "",
    "text": "What assumptions must be fulfilled for a one-way ANOVA?\nIn a two-way ANOVA, how many hypotheses are tested and what do they represent?\nWhich ANOVA model is used to compare the effectiveness of three fertilizers on crop yield?\n\n\n\n\n\nA one-way ANOVA is run on three groups (\\(n_1 = 8, n_2 = 8, n_3 = 8\\)) with:\n\nSST = 90\n\nSSTr = 60\n\nSSE = 30\nConstruct the ANOVA table and test significance at α = 0.05.\n\nIn a two-way ANOVA with the following sums of squares:\n\nSST = 180, SSA = 60, SSB = 40, SSAB = 30, SSE = 50\nCompute the F-ratios for Factor A, Factor B, and the interaction term. Assume a total sample size of 30."
  },
  {
    "objectID": "lectures/week-07_midterm-review.html#exam-1-review-questions---part-1",
    "href": "lectures/week-07_midterm-review.html#exam-1-review-questions---part-1",
    "title": "Midterm Review",
    "section": "Exam 1: Review Questions - Part 1",
    "text": "Exam 1: Review Questions - Part 1\n\nQuestion 14: ANOVA Table for Three Diets\nAn experiment tests the effect of three different diets on the weight gain of pigs. A total of 15 pigs are used, with five pigs randomly assigned to each diet. The following sum of squares are obtained:\n\nTotal Sum of Squares (SST) = 400\nSum of Squares for Treatment (SSTr) = 300\nSum of Squares for Error (SSE) = 100\n\nConstruct the ANOVA table and test whether diet has a significant effect at α = 0.05.\nSolution\n\n\n\nSource\nSS\ndf\nMS\nF\np-value\n\n\n\n\nTreatment\n300\n2\n150.000\n18.0\n0.000244\n\n\nError\n100\n12\n8.333\n-\n-\n\n\nTotal\n400\n14\n-\n-\n-\n\n\n\n\nThe F-statistic F = 18.0, and the p-value 0.000244 is much less than 0.05.\nConclusion: Reject \\(H_0\\); at least one diet significantly affects weight gain.\n\n\n\n\nQuestion 15: Degrees of Freedom\nSuppose an experiment consists of four treatment levels with six observations per treatment. Calculate the degrees of freedom for the following:\n\nTreatment\nError\nTotal\n\nSolution\n\nTreatment df = 4 - 1 = 3\n\nError df = (4 × 6) - 4 = 20\n\nTotal df = 3 + 20 = 23\n\nAnswer: (3, 20, 23)\n\n\n\nQuestion 19: Main and Interaction Effects\nConsider a \\(2^2\\) factorial experiment where Factor A has levels A1 and A2, Factor B has levels B1 and B2, and the treatment means below:\n\n\n\nTreatment\nMean Response\n\n\n\n\nA1B1\n20\n\n\nA1B2\n30\n\n\nA2B1\n25\n\n\nA2B2\n35\n\n\n\nCompute the main effects and the interaction effect.\nSolution\n\nMain Effect of A = 5\n\nMain Effect of B = 10\n\nInteraction = 0\n\nAnswer: (5, 10, 0)\n\n\n\nQuestion 20: Total Observations in a \\(2^3\\) Factorial Design\nA researcher conducts a \\(2^3\\) factorial experiment with factors A, B, and C, replicated twice. How many total observations?\nSolution\n(2^3) × 2 = 16\nAnswer: 16 observations\n\n\n\nQuestion 24: One-Way ANOVA Table\nA one-way ANOVA is conducted with three groups (\\(n_1 = 8, n_2 = 8, n_3 = 8\\)) and the following sum of squares:\n\nSST = 90\nSSTr = 60\nSSE = 30\n\nConstruct the ANOVA table and test the significance at α = 0.05.\nSolution\n\n\n\nSource\nSS\ndf\nMS\nF\np-value\n\n\n\n\nTreatment\n60\n2\n30.000\n21.0\n0.00001\n\n\nError\n30\n21\n1.428\n-\n-\n\n\nTotal\n90\n23\n-\n-\n-\n\n\n\n\nThe F-statistic F = 21.0, and the p-value 0.00001 is much smaller than 0.05.\nConclusion: Reject \\(H_0\\); there is a significant effect of treatment on response.\n\n\n\n\nQuestion 25: F-ratios for Two-Way ANOVA\nIn a two-way ANOVA, the sum of squares values are given:\n\nSST = 180\nSSA = 60\nSSB = 40\nSSAB = 30\nSSE = 50\n\nCompute the F-ratios for Factor A, Factor B, and the interaction. Assume total sample size is 30.\nSolution\n- \\(F_A\\) (Factor A)\n\\[\n\\frac{MS_A}{MSE} = \\frac{60}{2.083} = 28.8\n\\]\n\n\\(F_B\\) (Factor B)\n\n\\[\n\\frac{MS_B}{MSE} = \\frac{40}{4.167} = 9.6\n\\]\n\n\\(F_{AB}\\) (Interaction)\n\n\\[\n\\frac{MS_{AB}}{MSE} = \\frac{30}{4.167} = 7.2\n\\]\nAnswer: (28.8, 9.6, 7.2)"
  },
  {
    "objectID": "lectures/week-07_midterm-review.html#chapter-2-basic-statistical-concepts-in-experimental-design-2",
    "href": "lectures/week-07_midterm-review.html#chapter-2-basic-statistical-concepts-in-experimental-design-2",
    "title": "Midterm Review",
    "section": "Chapter 2: Basic Statistical Concepts in Experimental Design",
    "text": "Chapter 2: Basic Statistical Concepts in Experimental Design\n\nQ9: Probability of Survival beyond 22 months\n\nGiven: \\(X \\sim N(18, 4^2)\\)\nStandardizing:\n\n\\[\nZ = \\frac{22 - 18}{4} = 1\n\\]\n\nUsing the standard normal table:\n\n\\[\nP(Z &gt; 1) = 1 - P(Z \\leq 1) = 1 - 0.8413 = 0.1587\n\\]\n\nAnswer: 0.1587 (approximately 15.87%)\n\n\n\n\nQ10: R Output Interpretation (One-Way ANOVA)\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nTreatment\n3\n24.6\n8.2\n5.4\n0.009\n\n\nResiduals\n16\n24.3\n1.5\n\n\n\n\n\n\nTotal observations:\n\n\\[\ndf_{Total} = df_{Treatment} + df_{Residual} + 1 = 3 + 16 + 1 = 20\n\\]\n\nDecision at \\(\\alpha = 0.05\\)\n\nSince \\(p = 0.009 &lt; 0.05\\), reject \\(H_0\\) → The treatment effect is significant.\n\nTotal SS\n\n\\[\nSS_{Total} = SS_{Treatment} + SS_{Residual} = 24.6 + 24.3 = 48.9\n\\]"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html",
    "href": "lectures/week-08_rcbd-blocking.html",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "",
    "text": "In the realm of experimental design, Complete Block Designs (CBDs) play a pivotal role in controlling the effects of nuisance factors, thereby enhancing the precision of treatment comparisons. By organizing experimental units into blocks that share similar characteristics, CBDs effectively reduce variability stemming from these extraneous factors. This structured approach not only refines the efficiency of experiments but also ensures more accurate and reliable analyses.\n\n\nImagine you are testing different fertilizers on crops spread across a large farm. The farm has sections with varying soil types, sunlight exposure, and moisture levels. Without controlling for these variations, it would be challenging to discern the true effect of each fertilizer. By grouping plots with similar soil types into blocks, you can more precisely compare the performance of fertilizers within each homogeneous group, minimizing the confounding influence of soil variability.\nThis lecture delves into the structure, applications, and analysis of Complete Block Designs, with a focus on Randomized Complete Block Designs (RCBDs) and their generalizations. We will explore theoretical underpinnings, practical implementations using R, and advanced topics to provide a comprehensive understanding tailored for graduate statistics students."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#introduction",
    "href": "lectures/week-08_rcbd-blocking.html#introduction",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "",
    "text": "In the realm of experimental design, Complete Block Designs (CBDs) play a pivotal role in controlling the effects of nuisance factors, thereby enhancing the precision of treatment comparisons. By organizing experimental units into blocks that share similar characteristics, CBDs effectively reduce variability stemming from these extraneous factors. This structured approach not only refines the efficiency of experiments but also ensures more accurate and reliable analyses.\n\n\nImagine you are testing different fertilizers on crops spread across a large farm. The farm has sections with varying soil types, sunlight exposure, and moisture levels. Without controlling for these variations, it would be challenging to discern the true effect of each fertilizer. By grouping plots with similar soil types into blocks, you can more precisely compare the performance of fertilizers within each homogeneous group, minimizing the confounding influence of soil variability.\nThis lecture delves into the structure, applications, and analysis of Complete Block Designs, with a focus on Randomized Complete Block Designs (RCBDs) and their generalizations. We will explore theoretical underpinnings, practical implementations using R, and advanced topics to provide a comprehensive understanding tailored for graduate statistics students."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#objectives",
    "href": "lectures/week-08_rcbd-blocking.html#objectives",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Objectives",
    "text": "Objectives\n\nDefine and distinguish between complete and incomplete block designs.\nExplore the structure and application of Randomized Complete Block Designs (RCBDs).\nPerform Analysis of Variance (ANOVA) for RCBDs and assess the efficiency of blocking.\nUnderstand general complete block designs and their applications.\nImplement complete block designs in R and interpret the results.\nReinforce concepts with exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#r-packages",
    "href": "lectures/week-08_rcbd-blocking.html#r-packages",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "R Packages",
    "text": "R Packages\nTo facilitate the analysis of Randomized Complete Block Designs (RCBDs) and contrasts, we will utilize the following R packages:\n\n# Required R Packages\npacman::p_load(dplyr, ggplot2, emmeans, ggpubr, here, readr)\ntheme_set(ggpubr::theme_pubr())\n\n\nemmeans: For estimating marginal means and conducting pairwise comparisons.\nggplot2: For creating visualizations and diagnostic plots.\ndplyr: For data manipulation and summarization.\n\nEnsure that these packages are installed in your R environment to follow along with the examples and exercises."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#what-are-complete-block-designs",
    "href": "lectures/week-08_rcbd-blocking.html#what-are-complete-block-designs",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "What Are Complete Block Designs?",
    "text": "What Are Complete Block Designs?\nComplete Block Designs (CBDs) are experimental designs where each block contains all the treatments being tested. This ensures that every treatment is applied in each block, allowing for direct comparisons under similar conditions.\n\nBlock: A group of experimental units that are similar in some aspect and expected to yield comparable responses.\nComplete Block: Each block includes every treatment exactly once.\n\n\nExample\nIn a clinical trial comparing drug treatments across different hospitals, each hospital represents a block. By administering all treatments in each hospital, you account for hospital-specific factors, such as patient demographics and care protocols, ensuring that treatment effects are evaluated consistently."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#randomized-complete-block-designs-rcbds",
    "href": "lectures/week-08_rcbd-blocking.html#randomized-complete-block-designs-rcbds",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Randomized Complete Block Designs (RCBDs)",
    "text": "Randomized Complete Block Designs (RCBDs)\nRCBDs are a specific type of CBD where treatments are randomly assigned within each block. This randomness ensures that the assignment of treatments is free from bias and that any differences observed are attributable to the treatments rather than systematic variations.\nCharacteristics of RCBDs:\n\nThe number of experimental units in each block equals the number of treatments.\nEach treatment appears exactly once per block.\nTreatments are assigned randomly within each block.\n\nAnalogy: Think of RCBDs as seating arrangements in a classroom where each row (block) must contain one student from each house (treatments). By randomly assigning students to seats within rows, you ensure that no particular house has an advantage in any row."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#general-complete-block-designs",
    "href": "lectures/week-08_rcbd-blocking.html#general-complete-block-designs",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "General Complete Block Designs",
    "text": "General Complete Block Designs\nBeyond RCBDs, General Complete Block Designs allow for multiple observations or replicates of treatments within each block. This is particularly useful when interaction effects between treatments and blocks are anticipated.\nFeatures:\n\nA treatment is replicated \\(s &gt; 1\\) times within a block.\nFacilitates the detection of interactions between treatments and blocking factors.\n\nExample: In a clinical trial, if a treatment needs to be administered multiple times within each hospital ward (block) to account for ward-specific factors, a general complete block design would be appropriate."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#model-and-assumptions",
    "href": "lectures/week-08_rcbd-blocking.html#model-and-assumptions",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Model and Assumptions",
    "text": "Model and Assumptions\nThe statistical model for an RCBD can be expressed as:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\epsilon_{hi}\n\\]\nWhere:\n\n\\(Y_{hi}\\): Response variable for treatment \\(i\\) in block \\(h\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hi} \\sim N(0, \\sigma^2)\\): Random error term."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#key-assumptions",
    "href": "lectures/week-08_rcbd-blocking.html#key-assumptions",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Key Assumptions",
    "text": "Key Assumptions\n\nNormality: The residuals (\\(\\epsilon_{hi}\\)) are normally distributed.\nHomogeneity of Variance (Homoscedasticity): The variance of residuals is constant across all treatments and blocks.\nIndependence: Observations are independent of each other.\nNo Interaction: There is no interaction between blocks and treatments, i.e., the effect of a treatment is consistent across all blocks.\n\nDeeper Insight: Assumption of no interaction is critical. If this assumption is violated, the RCBD model may not adequately capture the variability, leading to biased estimates of treatment effects. In such cases, more complex designs that account for interactions, like general complete block designs, are necessary."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#anova-for-rcbds",
    "href": "lectures/week-08_rcbd-blocking.html#anova-for-rcbds",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "ANOVA for RCBDs",
    "text": "ANOVA for RCBDs\nTo analyze RCBDs, we employ Analysis of Variance (ANOVA), which decomposes the total variability in the data into components attributable to blocks, treatments, and random error.\n\nVariance Decomposition\n\\[\n\\text{Total SS} = \\text{Block SS} + \\text{Treatment SS} + \\text{Error SS}\n\\]"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#anova-table",
    "href": "lectures/week-08_rcbd-blocking.html#anova-table",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "ANOVA Table",
    "text": "ANOVA Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegrees of Freedom (DF)\nSum of Squares (SS)\nMean Square (MS)\nF-ratio\n\n\n\n\nBlocks\n\\(b - 1\\)\n\\(SS_\\text{Block}\\)\n\\(MS_\\text{Block} = SS_\\text{Block} / (b-1)\\)\n\n\n\nTreatments\n\\(\\nu - 1\\)\n\\(SS_\\text{Treatment}\\)\n\\(MS_\\text{Treatment} = SS_\\text{Treatment} / (\\nu-1)\\)\n\\(\\frac{MS_\\text{Treatment}}{MS_\\text{Error}}\\)\n\n\nError\n\\((b-1)(\\nu-1)\\)\n\\(SS_\\text{Error}\\)\n\\(MS_\\text{Error} = SS_\\text{Error} / ((b-1)(\\nu-1))\\)\n\n\n\nTotal\n\\(b\\nu - 1\\)\n\\(SS_\\text{Total}\\)\n\n\n\n\n\n\n\nTable 1: ANOVA Table for Randomized Complete Block Designs\n\n\n\n\nF-Ratio and Hypothesis Testing\n\nHypothesis Testing:\n\nNull Hypothesis (\\(H_0\\)): All treatment means are equal (\\(\\tau_i = 0\\) for all \\(i\\)).\nAlternative Hypothesis (\\(H_A\\)): At least one treatment mean is different.\n\nDecision Rule: Reject \\(H_0\\) if the F-ratio exceeds the critical value from the F-distribution with degrees of freedom \\((\\nu-1, (b-1)(\\nu-1))\\) at the chosen significance level (\\(\\alpha\\)).\n\nIntuitive Explanation: The F-ratio compares the variability due to treatments to the variability due to random error. A higher F-ratio indicates that treatments explain a significant portion of the variability in responses, suggesting that treatment effects are meaningful."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#multiple-comparisons",
    "href": "lectures/week-08_rcbd-blocking.html#multiple-comparisons",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Multiple Comparisons",
    "text": "Multiple Comparisons\nAfter establishing the significance of treatment effects, we often conduct multiple comparisons to identify specific differences between treatments. This post-hoc analysis helps pinpoint which treatments significantly differ from each other.\nBonferroni, Scheffé, Tukey, and Dunnett tests are common methods for multiple comparisons in RCBDs for a contrast, \\(C=\\sum c_i \\tau_i\\)\n\\[\n\\sum c_i \\hat\\tau_i \\pm w \\; \\sqrt{\\frac{MSE}{b} \\sum c_i^2}\n\\]\nwhere \\(w\\) is determined by the method used for multiple comparisons.\n\nBonferroni Test: \\(w_B = t_{\\alpha/(2m);\\, (\\nu-1)(b-1)}\\), where \\(m\\) is the number of contrasts.\nScheffé Test: \\(w_S = \\sqrt{(\\nu-1) F_{\\alpha;\\, \\nu-1, (b-1)(\\nu-1)}}\\)\nTukey Test: \\(w_T = q_{\\alpha;\\, \\nu, (b-1)(\\nu-1)}/\\sqrt{2}\\), where \\(q_{\\alpha;\\, \\nu, (b-1)(\\nu-1)}\\) is the critical value from the studentized range distribution."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#example-bread-baking-experiment",
    "href": "lectures/week-08_rcbd-blocking.html#example-bread-baking-experiment",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Example: Bread-Baking Experiment",
    "text": "Example: Bread-Baking Experiment\n\nScenario\nAn experimenter aims to compare four bread recipes (treatments) across three oven shelves (blocks). Each shelf is expected to have similar conditions, making it an ideal blocking factor.\nHere is step by step guide to analyze this Randomized Complete Block Design (RCBD) experiment in R."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#load-data-and-prepare-factors",
    "href": "lectures/week-08_rcbd-blocking.html#load-data-and-prepare-factors",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "1. Load Data and Prepare Factors",
    "text": "1. Load Data and Prepare Factors\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Define blocks and treatments\nblocks &lt;- factor(rep(1:3, each = 4))\ntreatments &lt;- factor(rep(1:4, times = 3))\n\n# Simulate response variable (e.g., loaf weight in grams)\nresponse &lt;- c(\n    rnorm(4, mean = 500, sd = 5), # Block 1\n    rnorm(4, mean = 508, sd = 5), # Block 2\n    rnorm(4, mean = 515, sd = 5) # Block 3\n)\n\n\n# Add treatment-specific effects using case_when function\nresponse &lt;- response +\n    dplyr::case_when(\n        treatments == 1 ~ -2,\n        treatments == 2 ~ 0,\n        treatments == 3 ~ 4,\n        treatments == 4 ~ 6\n    )\n# Create data frame\ndata &lt;- data.frame(blocks, treatments, response)\n\n# View the data\nprint(data)\n\n   blocks treatments response\n1       1          1 504.8548\n2       1          2 497.1765\n3       1          3 505.8156\n4       1          4 509.1643\n5       2          1 508.0213\n6       2          2 507.4694\n7       2          3 519.5576\n8       2          4 513.5267\n9       3          1 523.0921\n10      3          2 514.6864\n11      3          3 525.5243\n12      3          4 532.4332\n\n\nOutput Interpretation: The dataset comprises three blocks, each containing all four treatments. The response variable represents the loaf weight, simulated with slight block-specific means to reflect potential block effects."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#fit-the-rcbd-model",
    "href": "lectures/week-08_rcbd-blocking.html#fit-the-rcbd-model",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "2. Fit the RCBD Model",
    "text": "2. Fit the RCBD Model\n\n# Fit the RCBD model using ANOVA\nmodel &lt;- aov(response ~ blocks + treatments, data = data)\n\n# Summary of the model\nsummary(model)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nblocks       2  784.8   392.4  33.528 0.000554 ***\ntreatments   3  263.5    87.8   7.504 0.018702 *  \nResiduals    6   70.2    11.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation:\n\nBlocks: Capture the variability due to different oven shelves.\nTreatments: Assess the effect of different bread recipes after accounting for block effects.\nError: Represents unexplained variability."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#assess-blocking-efficiency",
    "href": "lectures/week-08_rcbd-blocking.html#assess-blocking-efficiency",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "3. Assess Blocking Efficiency",
    "text": "3. Assess Blocking Efficiency\nCompare the Mean Square for Blocks (\\(MS_\\text{Block}\\)) with Mean Square for Error (\\(MS_\\text{Error}\\)). A higher \\(MS_\\text{Block}\\) indicates substantial block effects, enhancing the precision of treatment comparisons."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#diagnostic-plots",
    "href": "lectures/week-08_rcbd-blocking.html#diagnostic-plots",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "4. Diagnostic Plots",
    "text": "4. Diagnostic Plots\n\n# Diagnostic plots for the ANOVA model\npar(mfrow = c(2, 2))\nplot(model)\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nFigure 1: Diagnostic Plots for RCBD\n\n\n\n\n\nInterpretation:\n\nResiduals vs Fitted: Check for homoscedasticity and non-linearity.\nNormal Q-Q: Assess normality of residuals.\nScale-Location: Evaluate homogeneity of variance.\nResiduals vs Leverage: Identify influential observations."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#multiple-comparisons-post-hoc-analysis",
    "href": "lectures/week-08_rcbd-blocking.html#multiple-comparisons-post-hoc-analysis",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "5. Multiple Comparisons (Post-Hoc Analysis)",
    "text": "5. Multiple Comparisons (Post-Hoc Analysis)\n\n# Estimated marginal means for treatments\nemm &lt;- emmeans(model, ~treatments)\n\n# Pairwise comparisons\npairwise_comparisons &lt;- contrast(emm, method = \"pairwise\")\nsummary(pairwise_comparisons)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\ntreatments1 - treatments2\n5.545312\n2.793356\n6\n1.9851793\n0.2898990\n\n\ntreatments1 - treatments3\n-4.976449\n2.793356\n6\n-1.7815307\n0.3651722\n\n\ntreatments1 - treatments4\n-6.385331\n2.793356\n6\n-2.2858995\n0.2033221\n\n\ntreatments2 - treatments3\n-10.521761\n2.793356\n6\n-3.7667100\n0.0353463\n\n\ntreatments2 - treatments4\n-11.930643\n2.793356\n6\n-4.2710788\n0.0204025\n\n\ntreatments3 - treatments4\n-1.408881\n2.793356\n6\n-0.5043688\n0.9550838\n\n\n\n\n\n\n\nTable 2: Pairwise Comparisons for Treatments\n\n\n\n\nInterpretation: Identify which bread recipes significantly differ in loaf weight after adjusting for oven shelf effects."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#plot-of-multiple-comparisons",
    "href": "lectures/week-08_rcbd-blocking.html#plot-of-multiple-comparisons",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Plot of Multiple Comparisons",
    "text": "Plot of Multiple Comparisons\nplot(emm, comparisons = TRUE) +\n    theme(legend.position = \"top\") +\n    ggtitle(\"Estimated Marginal Means for Treatments\") +\n    theme(plot.title = element_text(hjust = 0.5))\nplot(pairwise_comparisons) +\n    ggtitle(\"Pairwise Comparisons for Treatments\") +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nFigure 2: Estimated Marginal Means for Treatments\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Pairwise Comparisons for Treatments"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#scenario-1",
    "href": "lectures/week-08_rcbd-blocking.html#scenario-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Scenario",
    "text": "Scenario\nA researcher compares three fertilizers (\\(A\\), \\(B\\), \\(C\\)) on plant growth across four fields (blocks). Each field receives all three fertilizers, randomized within the block.\nData:\n\n\n\n\n\n\nBlock (Field)\nFertilizer A\nFertilizer B\nFertilizer C\n\n\n\n\n1\n20\n25\n22\n\n\n2\n18\n21\n19\n\n\n3\n22\n24\n23\n\n\n4\n19\n20\n21\n\n\n\n\n\nTable 3: Plant Growth Data for Fertilizer Experiment\n\n\n\nHere are the step-by-step procedures to analyze this Randomized Complete Block Design (RCBD) experiment."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-1-compute-means",
    "href": "lectures/week-08_rcbd-blocking.html#step-1-compute-means",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 1: Compute Means",
    "text": "Step 1: Compute Means\n\nOverall Mean (\\(\\bar{Y}\\)):\n\n\\[\n\\bar{Y} = \\frac{\\text{Sum of all observations}}{\\text{Total number of observations}} = \\frac{20 + 25 + \\dots + 21}{12} = 21.33\n\\]\n\nTreatment Means (\\(\\bar{Y}_{i\\cdot}\\)):\n\nFertilizer \\(A\\): \\(\\bar{Y}_A = \\frac{20 + 18 + 22 + 19}{4} = 19.75\\)\nFertilizer \\(B\\): \\(\\bar{Y}_B = \\frac{25 + 21 + 24 + 20}{4} = 22.50\\)\nFertilizer \\(C\\): \\(\\bar{Y}_C = \\frac{22 + 19 + 23 + 21}{4} = 21.25\\)\n\nBlock Means (\\(\\bar{Y}_{\\cdot j}\\)):\n\nBlock 1: \\(\\bar{Y}_{\\cdot 1} = \\frac{20 + 25 + 22}{3} = 22.33\\)\nBlock 2: \\(\\bar{Y}_{\\cdot 2} = \\frac{18 + 21 + 19}{3} = 19.33\\)\nBlock 3: \\(\\bar{Y}_{\\cdot 3} = \\frac{22 + 24 + 23}{3} = 23.00\\)\nBlock 4: \\(\\bar{Y}_{\\cdot 4} = \\frac{19 + 20 + 21}{3} = 20.00\\)"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-2-calculate-sums-of-squares",
    "href": "lectures/week-08_rcbd-blocking.html#step-2-calculate-sums-of-squares",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 2: Calculate Sums of Squares",
    "text": "Step 2: Calculate Sums of Squares\n\nTotal Sum of Squares (\\(SS_{\\text{Total}}\\)):\n\n\\[\nSS_{\\text{Total}} = \\sum_{i,j} (Y_{ij} - \\bar{Y})^2 = 35.33\n\\]\n\nTreatment Sum of Squares (\\(SS_{\\text{Treatment}}\\)):\n\n\\[\nSS_{\\text{Treatment}} = b \\sum_i (\\bar{Y}_{i\\cdot} - \\bar{Y})^2 = 4[(19.75 - 21.33)^2 + (22.50 - 21.33)^2 + (21.25 - 21.33)^2] = 12.67\n\\]\n\nBlock Sum of Squares (\\(SS_{\\text{Block}}\\)):\n\n\\[\nSS_{\\text{Block}} = t \\sum_j (\\bar{Y}_{\\cdot j} - \\bar{Y})^2 = 3[(22.33 - 21.33)^2 + \\dots + (20.00 - 21.33)^2] = 21.33\n\\]\n\nError Sum of Squares (\\(SS_{\\text{Error}}\\)):\n\n\\[\nSS_{\\text{Error}} = SS_{\\text{Total}} - SS_{\\text{Treatment}} - SS_{\\text{Block}} = 35.33 - 12.67 - 21.33 = 1.33\n\\]"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-3-construct-anova-table",
    "href": "lectures/week-08_rcbd-blocking.html#step-3-construct-anova-table",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 3: Construct ANOVA Table",
    "text": "Step 3: Construct ANOVA Table\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nTreatment\n2\n12.67\n6.33\n19.02\n\n\nBlock\n3\n21.33\n7.11\n\n\n\nError\n6\n1.33\n0.22\n\n\n\nTotal\n11\n35.33\n\n\n\n\n\n\n\nTable 4: ANOVA Table for Fertilizer Experiment"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-4-interpret-results",
    "href": "lectures/week-08_rcbd-blocking.html#step-4-interpret-results",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 4: Interpret Results",
    "text": "Step 4: Interpret Results\n\nTreatment Effect:\n\nThe \\(F\\)-statistic for treatments is \\(19.02\\), with a p-value \\(&lt; 0.01\\). This indicates a significant difference in plant growth between fertilizers.\nFertilizer \\(B\\) (\\(\\bar{Y}_B = 22.50\\)) performed the best on average.\n\nBlock Effect:\n\n\\(SS_{\\text{Block}} = 21.33\\) indicates considerable variability between fields, confirming that blocking was necessary to control this variability."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#scenario-2",
    "href": "lectures/week-08_rcbd-blocking.html#scenario-2",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Scenario",
    "text": "Scenario\nA school tests three teaching methods (\\(A\\), \\(B\\), \\(C\\)) on student performance across four classes (blocks). Each class is taught using all three methods, randomly assigned.\nData:\n\n\n\n\n\n\nClass\nMethod A\nMethod B\nMethod C\n\n\n\n\n1\n85\n88\n84\n\n\n2\n82\n83\n80\n\n\n3\n90\n89\n92\n\n\n4\n88\n86\n87\n\n\n\n\n\nTable 5: Student Performance Data for Teaching Methods Experiment\n\n\n\nHere’s a step-by-step guide to analyze this Randomized Complete Block Design (RCBD) experiment."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-1-compute-means-1",
    "href": "lectures/week-08_rcbd-blocking.html#step-1-compute-means-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 1: Compute Means",
    "text": "Step 1: Compute Means\n\nOverall Mean: \\(\\bar{Y} = \\frac{\\sum Y_{ij}}{12} = 86.42\\)\nMethod Means:\n\n\\(\\bar{Y}_A = 86.25, \\; \\bar{Y}_B = 86.50, \\; \\bar{Y}_C = 85.50\\)\n\nClass Means:\n\n\\(\\bar{Y}_{\\cdot 1} = 85.67, \\; \\bar{Y}_{\\cdot 2} = 81.67, \\; \\bar{Y}_{\\cdot 3} = 90.33, \\; \\bar{Y}_{\\cdot 4} = 87.00\\)"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-2-sums-of-squares",
    "href": "lectures/week-08_rcbd-blocking.html#step-2-sums-of-squares",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 2: Sums of Squares",
    "text": "Step 2: Sums of Squares\n\n\\(SS_{\\text{Total}} = 57.17\\)\n\\(SS_{\\text{Treatment}} = 6.17\\)\n\\(SS_{\\text{Block}} = 46.83\\)\n\\(SS_{\\text{Error}} = 4.17\\)"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-3-anova-table",
    "href": "lectures/week-08_rcbd-blocking.html#step-3-anova-table",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 3: ANOVA Table",
    "text": "Step 3: ANOVA Table\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nTreatment\n2\n6.17\n3.08\n4.44\n\n\nBlock\n3\n46.83\n15.61\n\n\n\nError\n6\n4.17\n0.69\n\n\n\nTotal\n11\n57.17\n\n\n\n\n\n\n\nTable 6: ANOVA Table for Teaching Methods Experiment"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-4-interpret-results-1",
    "href": "lectures/week-08_rcbd-blocking.html#step-4-interpret-results-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 4: Interpret Results",
    "text": "Step 4: Interpret Results\n\nTreatment Effect:\n\n\\(F = 4.44\\), p-value \\(= 0.067\\). The teaching methods do not differ significantly.\n\nBlock Effect:\n\nLarge \\(SS_{\\text{Block}}\\) shows variability between classes, justifying the use of blocks."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#key-insights",
    "href": "lectures/week-08_rcbd-blocking.html#key-insights",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Key Insights",
    "text": "Key Insights\n\nBlocking Matters:\n\nIgnoring blocks increases residual variability, reducing statistical power.\n\nPractical Use:\n\nRCBD is invaluable when variability between experimental units can be controlled through blocking."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#definition",
    "href": "lectures/week-08_rcbd-blocking.html#definition",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "2.1 Definition",
    "text": "2.1 Definition\nA contrast is a linear combination of treatment means:\n\\[\nC = \\sum_i c_i \\mu_i\n\\]\nwhere:\n\n\\(c_i\\) are coefficients such that \\(\\sum_i c_i = 0\\),\n\\(\\mu_i\\) are the treatment means."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#examples-of-contrasts",
    "href": "lectures/week-08_rcbd-blocking.html#examples-of-contrasts",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "2.2 Examples of Contrasts",
    "text": "2.2 Examples of Contrasts\n\nComparing Two Treatments (\\(A\\) vs \\(B\\)):\n\n\\[\nC = \\mu_A - \\mu_B \\quad \\text{(coefficients: $1, -1, 0$)}.\n\\]\n\nComparing One Treatment to the Average of Others (\\(A\\) vs \\((B+C)/2\\)):\n\n\\[\nC = \\mu_A - \\frac{\\mu_B + \\mu_C}{2} \\quad \\text{(coefficients: $1, -0.5, -0.5$)}.\n\\]\n\nTesting for Linear Trends (e.g., in dose-response studies): Assign weights like \\(-1, 0, 1\\) for increasing levels of a factor."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#scenario-3",
    "href": "lectures/week-08_rcbd-blocking.html#scenario-3",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "3.1 Scenario",
    "text": "3.1 Scenario\nA researcher tests three fertilizers (\\(A\\), \\(B\\), \\(C\\)) on plant growth across four fields. The goal is to:\n\nCompare \\(A\\) vs \\(B\\),\nCompare \\(A\\) vs the average of \\(B\\) and \\(C\\).\n\nData:\n\n\n\n\n\n\nBlock (Field)\nFertilizer A\nFertilizer B\nFertilizer C\n\n\n\n\n1\n20\n25\n22\n\n\n2\n18\n21\n19\n\n\n3\n22\n24\n23\n\n\n4\n19\n20\n21\n\n\n\n\n\nTable 7: Plant Growth Data for Fertilizer Experiment"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#anova-analysis",
    "href": "lectures/week-08_rcbd-blocking.html#anova-analysis",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "3.2 ANOVA Analysis",
    "text": "3.2 ANOVA Analysis\nStep 1: Compute Sums of Squares (as in prior example)\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nTreatment\n2\n12.67\n6.33\n19.02\n\n\nBlock\n3\n21.33\n7.11\n\n\n\nError\n6\n1.33\n0.22\n\n\n\nTotal\n11\n35.33\n\n\n\n\n\n\n\nTable 8: ANOVA Table for Fertilizer Experiment"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-by-step-contrast-computations",
    "href": "lectures/week-08_rcbd-blocking.html#step-by-step-contrast-computations",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "3.3 Step-by-Step Contrast Computations",
    "text": "3.3 Step-by-Step Contrast Computations\n\nTreatment Means:\n\n\\(\\bar{Y}_A = 19.75, \\; \\bar{Y}_B = 22.50, \\; \\bar{Y}_C = 21.25\\).\n\nContrast 1 (\\(A\\) vs \\(B\\)):\n\n\\[\nC_1 = \\bar{Y}_A - \\bar{Y}_B = 19.75 - 22.50 = -2.75\n\\]\n\nContrast 2 (\\(A\\) vs Average of \\(B\\) and \\(C\\)):\n\n\\[\nC_2 = \\bar{Y}_A - \\frac{\\bar{Y}_B + \\bar{Y}_C}{2} = 19.75 - \\frac{22.50 + 21.25}{2} = -2.63\n\\]\n\nStandard Error of Contrasts:\n\n\\[\nSE(C) = \\sqrt{\\frac{\\sigma^2}{b} \\sum_i c_i^2},\n\\]\nwhere:\n\n\\(\\sigma^2 = MS_{\\text{Error}} = 0.22\\),\n\\(b = 4\\) (number of blocks).\n\nFor \\(C_1\\) (\\(1, -1, 0\\)):\n\\[\nSE(C_1) = \\sqrt{\\frac{0.22}{4} \\cdot (1^2 + (-1)^2 + 0^2)} = 0.33\n\\]\nFor \\(C_2\\) (\\(1, -0.5, -0.5\\)):\n\\[\nSE(C_2) = \\sqrt{\\frac{0.22}{4} \\cdot (1^2 + (-0.5)^2 + (-0.5)^2)} = 0.28\n\\]\n\nt-Statistics:\n\n\\[\nt = \\frac{C}{SE(C)}.\n\\]\n\nFor \\(C_1\\): \\(t_1 = \\frac{-2.75}{0.33} = -8.33\\).\nFor \\(C_2\\): \\(t_2 = \\frac{-2.63}{0.28} = -9.39\\).\n\n\np-Values: With 6 degrees of freedom (\\(DF_{\\text{Error}}\\)):\n\n\\(p_1 &lt; 0.001\\), \\(p_2 &lt; 0.001\\)."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#r-code-to-analyze-contrasts",
    "href": "lectures/week-08_rcbd-blocking.html#r-code-to-analyze-contrasts",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "3.4 R Code to Analyze Contrasts",
    "text": "3.4 R Code to Analyze Contrasts\n\n# Data\ndata &lt;- data.frame(\n    Block = factor(rep(1:4, each = 3)),\n    Fertilizer = factor(rep(c(\"A\", \"B\", \"C\"), times = 4)),\n    Response = c(20, 25, 22, 18, 21, 19, 22, 24, 23, 19, 20, 21)\n)\n\n# Fit ANOVA Model\nmodel &lt;- aov(Response ~ Fertilizer + Block, data = data)\nsummary(model)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nFertilizer   2 15.167   7.583   7.378 0.0242 *\nBlock        3 28.333   9.444   9.189 0.0116 *\nResiduals    6  6.167   1.028                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Contrasts\nem &lt;- emmeans(model, \"Fertilizer\")\n\n# Contrast 1: A vs B\ncontrast(em, list(\"A vs B\" = c(1, -1, 0)))\n\n contrast estimate    SE df t.ratio p.value\n A vs B      -2.75 0.717  6  -3.836  0.0086\n\nResults are averaged over the levels of: Block \n\n# Contrast 2: A vs (B + C)/2\ncontrast(em, list(\"A vs Avg(B, C)\" = c(1, -0.5, -0.5)))\n\n contrast       estimate    SE df t.ratio p.value\n A vs Avg(B, C)    -2.12 0.621  6  -3.423  0.0141\n\nResults are averaged over the levels of: Block"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#explanation-of-r-output",
    "href": "lectures/week-08_rcbd-blocking.html#explanation-of-r-output",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "3.5 Explanation of R Output",
    "text": "3.5 Explanation of R Output\n\nANOVA Table:\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nFertilizer   2 15.167   7.583   7.378 0.0242 *\nBlock        3 28.333   9.444   9.189 0.0116 *\nResiduals    6  6.167   1.028                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTreatments are significant (\\(p = 0.002\\)).\n\n\nContrast Results:\n\n\n\\(A\\) vs \\(B\\):\n\n contrast estimate    SE df t.ratio p.value\n A vs B      -2.75 0.717  6  -3.836  0.0086\n\nResults are averaged over the levels of: Block\n\nFertilizer \\(B\\) significantly outperforms \\(A\\).\n\\(A\\) vs \\((B+C)/2\\):\n\n contrast       estimate    SE df t.ratio p.value\n A vs Avg(B, C)    -2.12 0.621  6  -3.423  0.0141\n\nResults are averaged over the levels of: Block \n\nThe average of \\(B\\) and \\(C\\) significantly outperforms \\(A\\)."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#interpretation",
    "href": "lectures/week-08_rcbd-blocking.html#interpretation",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "3.6 Interpretation",
    "text": "3.6 Interpretation\n\nANOVA Results:\n\nTreatments significantly affect plant growth (\\(p = 0.002\\)).\nBlocking controls variability due to fields.\n\nContrasts:\n\nFertilizer \\(B\\) is significantly better than \\(A\\) (\\(t = -8.33, p &lt; 0.001\\)).\nThe average of \\(B\\) and \\(C\\) also significantly outperforms \\(A\\) (\\(t = -9.39, p &lt; 0.001\\)).\n\nPractical Implication:\n\nFertilizer \\(B\\) is the most effective. The contrast analysis provides detailed insights beyond ANOVA."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#summary",
    "href": "lectures/week-08_rcbd-blocking.html#summary",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Summary",
    "text": "Summary\n\nRCBD Benefits:\n\nControls variability from blocks (e.g., fields, classrooms).\nProvides more precise estimates of treatment effects.\n\nContrasts:\n\nUseful for testing specific hypotheses about treatment effects.\nAdd depth to the ANOVA by allowing targeted comparisons.\n\nR Analysis:\n\nANOVA identifies significant differences.\nContrasts pinpoint specific treatment relationships."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#r-packages-used",
    "href": "lectures/week-08_rcbd-blocking.html#r-packages-used",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "R Packages Used",
    "text": "R Packages Used\n\n# Load required packages using pacman for convenience.\npacman::p_load(dplyr, ggplot2, tidyr, knitr, readr, here, ggpubr)\ntheme_set(theme_pubr())\n\nExplanation:\n\ndplyr: data manipulation,\nggplot2: visualizations,\ntidyr: data reshaping,\nknitr: producing tables,\nreadr: reading in external data files. # Mathematical Foundations from Chapter 10"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#model-without-interaction",
    "href": "lectures/week-08_rcbd-blocking.html#model-without-interaction",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Model without Interaction",
    "text": "Model without Interaction\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + \\varepsilon_{hit}, \\quad \\varepsilon_{hit} \\sim N(0,\\sigma^2)\n\\]\n\n\\(\\mu\\) is the overall mean.\n\\(\\theta_h\\) is the effect of the \\(h\\)th block.\n\\(\\tau_i\\) is the effect of the \\(i\\)th treatment.\n\\(\\varepsilon_{hit}\\) is the random error.\n\\(Y_{hit}\\) is the response for treatment \\(i\\) in block \\(h\\) and replicate \\(t\\).\n\\(\\nu\\) is the number of treatments.\n\\(b\\) is the number of blocks.\n\\(s\\) is the number of replicates per treatment per block."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#sums-of-squares-computation",
    "href": "lectures/week-08_rcbd-blocking.html#sums-of-squares-computation",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Sums of Squares Computation",
    "text": "Sums of Squares Computation\n\nTotal Sum of Squares:\n\n\\[\nSS_{\\text{Total}} = \\sum_{h=1}^{b} \\sum_{i=1}^{v} \\sum_{t=1}^{s} (Y_{hit} - \\bar{Y}_{\\cdots})^2\n\\]\n\nBlock Sum of Squares:\n\n\\[\nSS_{\\theta} = vs\\sum_{h=1}^{b} (\\bar{Y}_{h..} - \\bar{Y}_{\\cdots})^2\n\\]\n\nTreatment Sum of Squares:\n\n\\[\nSS_{Treatment} = bs\\sum_{i=1}^{v} (\\bar{Y}_{.i.} - \\bar{Y}_{\\cdots})^2\n\\]\n\nError Sum of Squares:\n\n\\[\nSS_{Error} = SS_{\\text{Total}} - SS_{\\theta} - SS_{Treatment}\n\\]\n\nDegrees of Freedom:\n\nBlocks: \\(b - 1\\)\nTreatments: \\(v - 1\\)\nError: \\(bvs - b - v + 1\\)\nTotal: \\(bvs - 1\\)"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#multiple-comparisons-1",
    "href": "lectures/week-08_rcbd-blocking.html#multiple-comparisons-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Multiple Comparisons",
    "text": "Multiple Comparisons\n\nContrast Estimation: For any contrast \\(\\sum c_i \\tau_i\\) (with \\(\\sum c_i=0\\)):\n\n\\[\n\\sum c_i \\tau_i \\in \\left(\\sum c_i \\bar{Y}_{.i.} \\pm w \\sqrt{\\frac{MS_{Error} \\sum c_i^2}{bs}}\\right)\n\\]\n\n\\(w\\) is the critical value from the chosen method (Bonferroni, Scheffé, Tukey, etc.).\nInterpretation: This provides a simultaneous confidence interval for contrasts, ensuring control over the familywise error rate."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#experiment-context",
    "href": "lectures/week-08_rcbd-blocking.html#experiment-context",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Experiment Context",
    "text": "Experiment Context\n\nBackground: Bananas are stored using two methods:\n\nCounter Storage: Conventional storage on a counter.\nHanging Storage: Using a banana hanging device to slow ripening.\n\nMeasured Outcome: The response variable \\(y\\) represents the percentage of black color on the banana peel (an indicator of ripeness).\nExperimental Factors:\n\nBlock: Grouping factor (e.g., repeated measures over time or location).\nLight: Level of illumination (affecting ripening).\nStorage: Storage method (1 = counter, 2 = hanging)."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#data-overview",
    "href": "lectures/week-08_rcbd-blocking.html#data-overview",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Data Overview",
    "text": "Data Overview\nThe banana.txt file has the following columns:\n\nblock: Block identifier.\nlight: Light condition (1 or 2).\nstorage: Storage method (1 = counter, 2 = hanging).\ny: Measured percentage of black color."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#computational-formulae",
    "href": "lectures/week-08_rcbd-blocking.html#computational-formulae",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "“Computational” Formulae",
    "text": "“Computational” Formulae\n\nBlocks are indexed by \\(h\\) (from 1 to \\(v\\)).\nTreatments indexed by \\(i\\).\nReplicates indexed by \\(t\\).\n\\(s\\) replicates per block-treatment combination.\nObserved response: \\(y_{hit}\\).\nDot notation represents summation over the omitted index.\n\nThen, the corrected equations are:\n\nTreatment Sum of Squares (Blocks), \\(SS_{\\theta}\\)\n\\[\nSS_{\\theta} = v s \\sum_{h=1}^{b} \\bar{y}_{h..}^2 - b v s \\bar{y}_{…}^2\n\\]\n\n\nTotal Sum of Squares, \\(SS_{\\text{Total}}\\)\n\\[\nSS_{\\text{Total}} = \\sum_{h=1}^{b}\\sum_{i=1}^{v}\\sum_{t=1}^{s} y_{hit}^2 - b v s \\bar{y}_{…}^2\n\\]\n\n\nTreatment Sum of Squares, \\(SS_{Treatment}\\)\n\\[\nSS_{Treatment} = b s \\sum_{i=1}^{v} \\bar{y}_{.i.}^2 - b v s \\bar{y}_{…}^2\n\\]\n\n\nError Sum of Squares, \\(SS_{Error}\\)\n\\[\nSS_{Error} = SS_{\\text{Total}} - SS_{\\theta} - SS_{Treatment}\n\\]\n\n\nDefinitions of Means Used\n\nOverall mean (\\(\\bar{y}_{…}\\)):\n\n\\[\n\\bar{y}_{…} = \\frac{1}{b v s}\\sum_{h=1}^{b}\\sum_{i=1}^{v}\\sum_{t=1}^{s}y_{hit}\n\\]\n\nBlock Means (\\(\\bar{y}_{h..}\\)):\n\n\\[\n\\bar{y}_{h..} = \\frac{1}{v s}\\sum_{i=1}^{v}\\sum_{t=1}^{s}y_{hit}\n\\]\n\nTreatment Means (\\(\\bar{y}_{.i.}\\)):\n\n\\[\n\\bar{y}_{.i.} = \\frac{1}{b s}\\sum_{h=1}^{b}\\sum_{t=1}^{s}y_{hit}\n\\]\n\n\nError Sum of Squares, \\(SS_{Error}\\)\nDefined as:\n\\[\nSS_{Error} = SS_{\\text{Total}} - SS_{\\theta} - SS_{Treatment}\n\\]\n\n\nExplanation of Terms\n\n\\(b\\): Number of blocks\n\\(v\\): Number of treatment levels\n\\(s\\): Number of replicates per block-treatment combination\n\\(y_{hit}\\): Observed response at block \\(h\\), treatment \\(i\\), and replicate \\(t\\)\n\n\n\nInterpretation\n\n\\(SS_{\\theta}\\): Measures the variability due to differences between blocks.\n\\(SS_{Treatment}\\): Measures the variability due to treatment differences.\n\\(SS_{Error}\\): Captures residual (unexplained) variability after accounting for block and treatment effects."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#anova-decision-rule",
    "href": "lectures/week-08_rcbd-blocking.html#anova-decision-rule",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "3. ANOVA Decision Rule",
    "text": "3. ANOVA Decision Rule\nThe F-test for treatments in a complete block design is:\n\\[\nF = \\frac{MS_{Treatment}}{MS_{Error}}\n\\]\nwhere:\n\n\\(MS_{Treatment} = \\frac{SS_{Treatment}}{v-1}\\)\n\\(MS_{Error} = \\frac{SS_{Error}}{bvs - b - v + 1}\\)\n\nReject the null hypothesis \\(H_0: \\tau_1 = \\tau_2\\) if:\n\\[\nF &gt; F_{\\alpha; \\, \\nu-1, b\\nu s-b-\\nu+1}\n\\]"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#data-import-and-preparation",
    "href": "lectures/week-08_rcbd-blocking.html#data-import-and-preparation",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Data Import and Preparation",
    "text": "Data Import and Preparation\n\nbanana_data &lt;- read.table(\n    here(\"data\", \"dean2017\", \"banana.txt\"),\n    header = TRUE\n) |&gt;\n    mutate(\n        block = factor(block),\n        storage = factor(\n            storage,\n            levels = c(1, 2),\n            labels = c(\"Counter\", \"Hanging\")\n        )\n    )\nbanana_data |&gt; head()\n\n\n\n\n\n\n\n\nblock\nlight\nstorage\ny\n\n\n\n\n1\n1\nCounter\n30\n\n\n1\n1\nCounter\n30\n\n\n1\n1\nCounter\n17\n\n\n1\n1\nCounter\n43\n\n\n1\n1\nHanging\n43\n\n\n1\n1\nHanging\n35\n\n\n\n\n\n\n\nTable 9: First Few Rows of Banana Data"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#descriptive-statistics-and-treatment-means",
    "href": "lectures/week-08_rcbd-blocking.html#descriptive-statistics-and-treatment-means",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Descriptive Statistics and Treatment Means",
    "text": "Descriptive Statistics and Treatment Means\n# Summary statistics for marginal means and cell means\nstorage_cell_means &lt;- banana_data |&gt;\n    group_by(block, storage) |&gt;\n    summarize(Mean = mean(y), n = n()) |&gt;\n    ungroup()\n\nstorage_cell_means\n## marginal means for storage\nstorage_margin_means &lt;- banana_data |&gt;\n    group_by(storage) |&gt;\n    summarize(Mean = mean(y), n = n()) |&gt;\n    ungroup()\nstorage_margin_means\n## marginal means for block\nstorage_block_means &lt;- banana_data |&gt;\n    group_by(block) |&gt;\n    summarize(Mean = mean(y), n = n()) |&gt;\n    ungroup()\nstorage_block_means\n\n\n\n\n\n\n\n\nblock\nstorage\nMean\nn\n\n\n\n\n1\nCounter\n33.875\n8\n\n\n1\nHanging\n37.875\n8\n\n\n2\nCounter\n49.000\n8\n\n\n2\nHanging\n44.875\n8\n\n\n3\nCounter\n41.625\n8\n\n\n3\nHanging\n31.000\n8\n\n\n\n\n\n\n\n\nstorage\nMean\nn\n\n\n\n\nCounter\n41.50000\n24\n\n\nHanging\n37.91667\n24\n\n\n\n\n\n\n\n\nblock\nMean\nn\n\n\n\n\n1\n35.8750\n16\n\n\n2\n46.9375\n16\n\n\n3\n36.3125\n16\n\n\n\n\n\n\n\nTable 10: [“Summary Statistics for Banana Data”,“Marginal Means for Storage Method”,“Marginal Means for Block”]"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#calculating-sums-of-squares",
    "href": "lectures/week-08_rcbd-blocking.html#calculating-sums-of-squares",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Calculating Sums of Squares",
    "text": "Calculating Sums of Squares\n\n# Compute the sums of squares\nb &lt;- length(unique(banana_data$block))\nv &lt;- length(unique(banana_data$storage))\ns &lt;- storage_cell_means$n[1]\nn &lt;- nrow(banana_data)\n\n# Overall mean\ny_bar &lt;- mean(banana_data$y)\n\n# Block sum of squares\nSS_theta &lt;- v *\n    s *\n    sum((storage_block_means$Mean - y_bar)^2) -\n    b * v * s * (mean(storage_block_means$Mean) - y_bar)^2\n\n# Treatment sum of squares\nSS_Treatment &lt;- b *\n    s *\n    sum((storage_margin_means$Mean - y_bar)^2) -\n    b * v * s * (mean(storage_margin_means$Mean) - y_bar)^2\n\n# Total sum of squares\nSS_Total &lt;- sum(banana_data$y^2) - b * v * s * y_bar^2\n\n# Error sum of squares\nSS_Error &lt;- SS_Total - SS_theta - SS_Treatment\n\n# Degrees of freedom\ndf_theta &lt;- b - 1\ndf_Treatment &lt;- v - 1\ndf_Error &lt;- b * v * s - b - v + 1\ndf_Total &lt;- b * v * s - 1\n\n# MS for Treatment and Error\nMS_Treatment &lt;- SS_Treatment / df_Treatment\nMS_Error &lt;- SS_Error / df_Error\n\n# F-statistic\nF_stat &lt;- MS_Treatment / MS_Error\n\nres_aov_storage_block &lt;- data.frame(\n    Component = c(\"Block\", \"Treatment\", \"Error\", \"Total\"),\n    SS = c(SS_theta, SS_Treatment, SS_Error, SS_Total),\n    DF = c(df_theta, df_Treatment, df_Error, df_Total),\n    MS = c(SS_theta / df_theta, MS_Treatment, MS_Error, NA),\n    F = c(NA, F_stat, NA, NA),\n    p_value = c(\n        NA,\n        pf(F_stat, df_Treatment, df_Error, lower.tail = FALSE),\n        NA,\n        NA\n    )\n)\n\nres_aov_storage_block |&gt;\n    knitr::kable(\n        caption = \"ANOVA Components for Banana Experiment\"\n    )\n\n\n\n\n\n\n\nComponent\nSS\nDF\nMS\nF\np_value\n\n\n\n\nBlock\n1255.7917\n2\n627.8958\nNA\nNA\n\n\nTreatment\n154.0833\n1\n154.0833\n0.8302268\n0.367172\n\n\nError\n8166.0417\n44\n185.5919\nNA\nNA\n\n\nTotal\n9575.9167\n47\nNA\nNA\nNA\n\n\n\n\n\n\nTable 11: ANOVA Components for Banana Experiment"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#fitting-the-block-design-model",
    "href": "lectures/week-08_rcbd-blocking.html#fitting-the-block-design-model",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Fitting the Block Design Model",
    "text": "Fitting the Block Design Model\nAssuming ‘block’ is our blocking factor and ‘storage’ is the treatment factor, we fit the model:\n\nfit_banana &lt;- aov(y ~ block + storage, data = banana_data)\nanova_banana &lt;- summary(fit_banana)[[1]]\nknitr::kable(anova_banana, caption = \"ANOVA Table for Banana Experiment\")\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nblock\n2\n1255.7917\n627.8958\n3.3832079\n0.0429809\n\n\nstorage\n1\n154.0833\n154.0833\n0.8302268\n0.3671720\n\n\nResiduals\n44\n8166.0417\n185.5919\nNA\nNA\n\n\n\n\n\n\nTable 12: ANOVA Table for Banana Experiment\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCompare the results from the ANOVA table with the computed Anova components from the previous code chunk."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#derivation-of-treatment-contrast-variance",
    "href": "lectures/week-08_rcbd-blocking.html#derivation-of-treatment-contrast-variance",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Derivation of Treatment Contrast Variance",
    "text": "Derivation of Treatment Contrast Variance\nFor a contrast \\(L = \\sum c_i \\tau_i\\), the estimated contrast is:\n\\[\n\\hat{L} = \\sum c_i \\bar{Y}_{.i.}\n\\]\nwith variance given by:\n\\[\n\\text{Var}(\\hat{L}) = \\sigma^2 \\sum \\frac{c_i^2}{bs}\n\\]\nThus, a \\(100(1-\\alpha)\\%\\) confidence interval for \\(L\\) is:\n\\[\n\\hat{L} \\pm w \\sqrt{MS_{Error} \\sum \\frac{c_i^2}{bs}}\n\\]\nwhere \\(w\\) is the critical value from the chosen method:\n\nTukey’s method: \\(w_T = q_{\\alpha;\\, \\nu, bvs-b-v+1}/ \\sqrt{2}\\)\nScheffé’s method: \\(w_S = \\sqrt{(\\nu-1) F_{\\alpha; \\,\\nu-1, bvs-b-v+1}}\\)\nBonferroni’s method: \\(w_B = t_{\\alpha/2m;\\, bvs-b-v+1}\\), where \\(m\\) is the number of contrasts."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#model-overview",
    "href": "lectures/week-08_rcbd-blocking.html#model-overview",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Model Overview",
    "text": "Model Overview\nFor experiments with replicated treatments within blocks, the model extends to include block × treatment interaction:\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)_{hi} + \\epsilon_{hit}\n\\]\nWhere:\n\n\\(Y_{hit}\\): Response for treatment \\(i\\) in block \\(h\\) on replicate \\(t\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\((\\theta \\tau)_{hi}\\): Interaction effect between block \\(h\\) and treatment \\(i\\).\n\\(\\epsilon_{hit}\\): Random error, assumed \\(N(0,\\sigma^2)\\)."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#component-explanation",
    "href": "lectures/week-08_rcbd-blocking.html#component-explanation",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Component Explanation",
    "text": "Component Explanation\n\n1. Overall Mean and Block Effects\n\nOverall Mean \\(\\mu\\): The grand average of all observations.\nBlock Effect \\(\\theta_h\\): Adjusts for systematic differences among blocks (e.g., different environmental conditions).\n\n\n\n2. Treatment Effect and Interaction\n\nTreatment Effect \\(\\tau_i\\): The deviation of treatment \\(i\\)’s mean from \\(\\mu\\).\nInteraction \\((\\theta \\tau)_{hi}\\): Captures how treatment \\(i\\)’s effect varies in block \\(h\\). Example: A storage method might perform differently in Block 1 than in Block 2 due to unobserved factors.\n\n\n\n3. Random Error \\(\\epsilon_{hit}\\)\n\nAccounts for unexplained variation.\nAssumed to be independent and normally distributed."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#data-overview-1",
    "href": "lectures/week-08_rcbd-blocking.html#data-overview-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Data Overview",
    "text": "Data Overview\nThe banana data includes:\n\nblock: Block identifier (e.g., location or time).\nlight: Light condition (1 = low light, 2 = high light).\nstorage: Storage method (1 = Counter, 2 = Hanging).\ny: Percentage of black color on banana peel (measure of ripeness).\n\nWe will focus on “storage” as the treatment and “block” as the blocking factor, and illustrate the block-by-treatment interaction."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-1.-compute-means",
    "href": "lectures/week-08_rcbd-blocking.html#step-1.-compute-means",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 1. Compute Means",
    "text": "Step 1. Compute Means\n\nOverall Mean:\n\n\\[\n\\bar{Y}_{…} = \\frac{1}{n}\\sum y\n\\]\n\nTreatment (Storage) Mean:\n\n\\[\n\\bar{Y}_{.i.} = \\frac{1}{n_i}\\sum_{\\text{storage}=i} y\n\\]\n\nBlock Mean:\n\n\\[\n\\bar{Y}_{h..} = \\frac{1}{n_h}\\sum_{\\text{block}=h} y\n\\]"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#step-2.-sums-of-squares",
    "href": "lectures/week-08_rcbd-blocking.html#step-2.-sums-of-squares",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Step 2. Sums of Squares",
    "text": "Step 2. Sums of Squares\n\nTotal Sum of Squares:\n\n\\[\nSS_{\\text{Total}} = \\sum (y - \\bar{Y}_{…})^2\n\\]\n\nBlock Sum of Squares:\n\n\\[\nSS_{\\theta} = vs\\sum_{h} (\\bar{Y}_{h..} - \\bar{Y}_{…})^2\n\\]\n\nTreatment Sum of Squares:\n\n\\[\nSS_{Treatment} = bs\\sum_{i} (\\bar{Y}_{.i.} - \\bar{Y}_{…})^2\n\\]\n\nInteraction Sum of Squares:\n\n\\[\nSS_{\\theta\\tau} = \\sum_{h}\\sum_{i} \\left(\\bar{Y}_{hi.} - \\bar{Y}_{h..} - \\bar{Y}_{.i.} + \\bar{Y}_{…}\\right)^2\n\\]\n\nError Sum of Squares:\n\n\\[\nSS_{Error} = SS_{\\text{Total}} - SS_{\\theta} - SS_{Treatment} - SS_{\\theta\\tau}\n\\]\nHere, \\(b\\) is the number of blocks, \\(v\\) the number of treatments, and \\(s\\) the number of replicates per treatment per block."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#fitting-the-general-complete-block-design-model",
    "href": "lectures/week-08_rcbd-blocking.html#fitting-the-general-complete-block-design-model",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Fitting the General Complete Block Design Model",
    "text": "Fitting the General Complete Block Design Model\nWe now fit the model that includes the block, treatment (storage), and their interaction.\n\n\n\n# including block:storage interaction.\nfit_banana_int &lt;- aov(y ~ block * storage, data = banana_data)\nsummary(fit_banana_int)\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)  \nblock          2   1256   627.9   3.409 0.0425 *\nstorage        1    154   154.1   0.836 0.3656  \nblock:storage  2    430   214.8   1.166 0.3215  \nResiduals     42   7736   184.2                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTable 13: Treatment Means by Storage Method\n\n\n\nInterpretation:\n\nBlock: Variation among blocks.\nStorage (Treatment): Main effect of storage method.\nBlock:Storage: Interaction term, indicating if storage method performance varies by block.\nResiduals: Unexplained variability."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#detailed-interpretation-of-anova-results",
    "href": "lectures/week-08_rcbd-blocking.html#detailed-interpretation-of-anova-results",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Detailed Interpretation of ANOVA Results",
    "text": "Detailed Interpretation of ANOVA Results\n\nF-statistics: Compare the mean squares (MS) for storage and the interaction against the error MS.\np-values: Determine if storage method and/or the interaction is statistically significant.\n\nIf the interaction term is significant, it suggests that the effect of the storage method on ripening (percentage black) differs among blocks."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#deriving-a-contrast-for-storage-methods",
    "href": "lectures/week-08_rcbd-blocking.html#deriving-a-contrast-for-storage-methods",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Deriving a Contrast for Storage Methods",
    "text": "Deriving a Contrast for Storage Methods\nAssume we want to compare:\n\\[\nL = \\tau_1 - \\tau_2\n\\]\nwith:\n\\[\n\\hat{L} = \\bar{Y}_{.1.} - \\bar{Y}_{.2.}\n\\]\nThe estimated variance of \\(\\hat{L}\\) is:\n\\[\n\\text{Var}(\\hat{L}) = \\sigma^2 \\left(\\frac{1}{bs} + \\frac{1}{bs}\\right) = \\frac{2\\sigma^2}{bs}\n\\]\nThus, a 95% confidence interval for \\(L\\) is given by:\n\\[\n\\hat{L} \\pm t_{df,0.025} \\sqrt{\\frac{2MS_{Error} }{bs}}\n\\]\nwhere \\(MS_{Error}\\) is the error mean square from the ANOVA."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#r-code-for-contrast-and-confidence-interval",
    "href": "lectures/week-08_rcbd-blocking.html#r-code-for-contrast-and-confidence-interval",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "R Code for Contrast and Confidence Interval",
    "text": "R Code for Contrast and Confidence Interval\n\n\n\nstorage_means &lt;- banana_data |&gt;\n    group_by(storage) |&gt;\n    summarize(Mean = mean(y))\n\n# Extract the error mean square from the ANOVA table\nmse &lt;- anova_banana[\"Residuals\", \"Mean Sq\"]\n# Define number of blocks (b) and replicates per cell (s)\nb &lt;- length(unique(banana_data$block))\n# For simplicity, assume equal replication: s = n / (b * number_of_treatments)\ns &lt;- nrow(banana_data) / (b * 2)\n\n# Compute contrast: difference between storage method 1 and 2\nL_hat &lt;- storage_means$Mean[1] - storage_means$Mean[2]\n# Degrees of freedom for error from ANOVA table\ndf_error &lt;- anova_banana[\"Residuals\", \"Df\"]\n\n# Calculate standard error of the contrast\nSE_L &lt;- sqrt(2 * mse / (b * s))\n\n# Critical t-value for 95% CI\nt_val &lt;- qt(0.975, df = df_error)\n\n# Confidence Interval for the contrast\nCI_lower &lt;- L_hat - t_val * SE_L\nCI_upper &lt;- L_hat + t_val * SE_L\n\ncat(\"Contrast (Storage 1 - Storage 2):\", L_hat, \"\\n\")\n\nContrast (Storage 1 - Storage 2): 3.583333 \n\ncat(\"95% CI: [\", CI_lower, \",\", CI_upper, \"]\\n\")\n\n95% CI: [ -4.342467 , 11.50913 ]\n\n# Conform with emmeans package\n\nlibrary(emmeans)\nemm &lt;- emmeans(fit_banana, ~storage)\ncontrast(emm, list(\"1 - 2\" = c(1, -1)))\n\n contrast estimate   SE df t.ratio p.value\n 1 - 2        3.58 3.93 44   0.911  0.3672\n\nResults are averaged over the levels of: block \n\n\n\nTable 14: Treatment Means by Storage Method\n\n\n\nInterpretation: This code computes the difference in means between storage methods, its standard error, and a 95% confidence interval using the error variance from the ANOVA table."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#summary-and-takeaways",
    "href": "lectures/week-08_rcbd-blocking.html#summary-and-takeaways",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Summary and Takeaways",
    "text": "Summary and Takeaways\n\nWe extended the basic block design model to include a block-by-treatment interaction.\nDetailed computations:\n\nMeans, Sums of Squares, and Variance Components are essential for constructing the ANOVA.\nContrast and Confidence Intervals: Demonstrated using storage method differences.\n\nThe R code illustrates how to read data, fit the model, and interpret the outputs.\nAlways check the significance of the interaction to decide whether to interpret the main effects directly."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html",
    "href": "lectures/week-10_ancova-causal-inference_part1.html",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "",
    "text": "By the end of this lesson, you will be able to:\n\nDefine potential outcomes and causal effects: Understand what each outcome represents and why we cannot observe both for a single unit.\nDescribe the assumptions required for causal inference: Learn the key conditions needed to draw causal conclusions.\nConnect potential outcomes to classical methods: See how ANOVA and ANCOVA relate to causal inference.\nInterpret numerical examples: Use examples to visualize and calculate treatment effects.\nPractice with problems: Build your skills through structured practice."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#objectives",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#objectives",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "",
    "text": "By the end of this lesson, you will be able to:\n\nDefine potential outcomes and causal effects: Understand what each outcome represents and why we cannot observe both for a single unit.\nDescribe the assumptions required for causal inference: Learn the key conditions needed to draw causal conclusions.\nConnect potential outcomes to classical methods: See how ANOVA and ANCOVA relate to causal inference.\nInterpret numerical examples: Use examples to visualize and calculate treatment effects.\nPractice with problems: Build your skills through structured practice."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#the-big-question",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#the-big-question",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧠 The Big Question",
    "text": "🧠 The Big Question\nImagine you want to know if a treatment—whether it’s a tutoring session, a new medication, or a teaching method—leads to better outcomes. The key challenge is: How do we know if the treatment actually causes the improvement?"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#understanding-potential-outcomes",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#understanding-potential-outcomes",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🔍 Understanding Potential Outcomes",
    "text": "🔍 Understanding Potential Outcomes\nEvery unit (like a student, patient, or plant) in our study has two potential outcomes:\n\n\\(Y_i(1)\\): The outcome if unit i receives the treatment.\n\\(Y_i(0)\\): The outcome if unit i does not receive the treatment (control condition).\n\nKey Intuition: Each unit has a “what if” scenario for both conditions. However, in reality, we only observe one of these scenarios—the one that actually happens. The unobserved outcome is called the counterfactual.\n\nWhy is this tricky? If we can only see one outcome per unit, comparing outcomes directly might mix up the treatment effect with differences inherent to the units."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#scaffolding-example-two-students",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#scaffolding-example-two-students",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧾 Scaffolding Example: Two Students",
    "text": "🧾 Scaffolding Example: Two Students\nConsider two students:\n\nAlice: If she receives a special tutoring program, her test score might be 90. Without tutoring, it might be 80.\nBen: If he receives tutoring, his score might be 85; without it, perhaps it remains 85.\n\nWe can only observe one outcome per student. For instance, if Alice is tutored, we see her score as 90. We never get to see what her score would have been without tutoring. This missing information (80 for Alice) is our counterfactual.\n\n🧮 Calculating Causal Effects\n\nIndividual Causal Effect:\n\n\\[\n\\text{Effect}_i = Y_i(1) - Y_i(0)\n\\]\nFor Alice, the causal effect would be \\(90 - 80 = 10\\).\n\nAverage Treatment Effect (ATE): This is the average of the individual effects over all units:\n\n\\[\n\\text{ATE} = \\mathbb{E}[Y(1) - Y(0)]\n\\]\nIt tells us the overall expected improvement if everyone were to receive the treatment.\n\nAverage Treatment on the Treated (ATT): Sometimes we care only about those who actually received the treatment:\n\n\\[\n\\text{ATT} = \\mathbb{E}[Y(1) - Y(0) \\mid T = 1]\n\\]\nThis average is calculated only for the treated group.\n\nNote: Since we can’t observe both outcomes for a single unit, we use various strategies (like randomization) to estimate these averages."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#mathematical-justification",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#mathematical-justification",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🔍 Mathematical Justification",
    "text": "🔍 Mathematical Justification\nWith random assignment:\n\\[\n\\mathbb{E}[Y(1) \\mid T = 1] = \\mathbb{E}[Y(1)]\n\\]\n\\[\n\\mathbb{E}[Y(0) \\mid T = 0] = \\mathbb{E}[Y(0)]\n\\]\nThus, the Average Treatment Effect (ATE) can be estimated as:\n\\[\n\\text{ATE} \\approx \\mathbb{E}[Y \\mid T = 1] - \\mathbb{E}[Y \\mid T = 0]\n\\]"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#detailed-numerical-example",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#detailed-numerical-example",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧾 Detailed Numerical Example",
    "text": "🧾 Detailed Numerical Example\nImagine we have two groups:\n\nGroup A (Control): Scores = [70, 75, 80]\nGroup B (Treatment): Scores = [85, 88, 90]\n\nStep 1: Compute the group means: - Mean for Group A:\n\\[\n\\text{Mean A} = \\frac{70 + 75 + 80}{3} = 75\n\\]\n\nMean for Group B:\n\n\\[\n\\text{Mean B} = \\frac{85 + 88 + 90}{3} \\approx 87.67\n\\]\nStep 2: Estimate the ATE:\n\\[\n\\text{Estimated ATE} = 87.67 - 75 \\approx 12.67\n\\]\nThis means that, on average, the treatment is associated with a score increase of about 12.67 points."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#causal-use-of-anova",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#causal-use-of-anova",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "✅ Causal Use of ANOVA",
    "text": "✅ Causal Use of ANOVA\nANOVA Model:\n\\[\nY_{ij} = \\mu + \\tau_j + \\epsilon_{ij}\n\\]\nWhen groups are formed via random assignment, any observed difference in means can be attributed to the treatment effect. Specifically, for group \\(j\\):\n\\[\n\\tau_j = \\mathbb{E}[Y \\mid T = j] - \\mu\n\\]"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#worked-example",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#worked-example",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧾 Worked Example",
    "text": "🧾 Worked Example\nConsider the two groups from our numerical example:\n\nGroup A (Control): Mean = 75\nGroup B (Treatment): Mean = 87.67\n\nThe treatment effect for Group B can be seen as:\n\\[\n\\tau_B = 87.67 - 75 \\approx 12.67\n\\]\nThis confirms that the treatment (Group B) is associated with a 12.67 point increase in the outcome compared to the overall mean."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#what-is-ancova",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#what-is-ancova",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🔎 What is ANCOVA?",
    "text": "🔎 What is ANCOVA?\nANCOVA (Analysis of Covariance) extends ANOVA by adjusting for one or more covariates—variables that might influence the outcome. This method helps us control for differences that existed before the treatment, allowing a clearer picture of the treatment’s effect.\nBasic ANCOVA Model:\n\\[\nY_i = \\alpha + \\tau T_i + \\beta X_i + \\epsilon_i\n\\]\nWhere:\n\n\\(T_i\\) is a binary indicator for treatment (1 if treated, 0 if control).\n\\(X_i\\) is a covariate, such as a pre-test score.\n\\(\\tau\\) represents the treatment effect after controlling for the covariate.\n\\(\\beta\\) measures how the covariate influences the outcome."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#detailed-example-with-covariates",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#detailed-example-with-covariates",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧾 Detailed Example with Covariates",
    "text": "🧾 Detailed Example with Covariates\nSuppose we have the following data:\n\n\n\n\n\n\n\n\n\nPerson\nPre-Test Score (\\(X_i\\))\nTreatment (\\(T_i\\))\nPost-Test Score (\\(Y_i\\))\n\n\n\n\nAlice\n80\n1\n90\n\n\nBen\n70\n0\n75\n\n\nCarla\n85\n1\n95\n\n\nDave\n65\n0\n70"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#without-adjustment",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#without-adjustment",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "Without Adjustment",
    "text": "Without Adjustment\n\nThe treated group’s mean post-test might appear higher simply because they had higher pre-test scores.\nTreated group post-test mean: \\((90 + 95)/2 = 92.5\\)\nControl group post-test mean: \\((75 + 70)/2 = 72.5\\)\nDifference = 20\n\nWhy Adjust? The difference of 20 could be inflated because the treated group was already more prepared (higher pre-tests). ANCOVA helps us adjust the post-test scores by accounting for pre-test differences, thereby estimating the treatment effect (\\(\\tau\\)) more accurately.\nHow It Works: - The model “removes” the part of the outcome variation that is explained by the covariate (the pre-test score). - What remains is a cleaner comparison between the treatment and control groups."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#sutva-stable-unit-treatment-value-assumption",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#sutva-stable-unit-treatment-value-assumption",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "✅ 1. SUTVA (Stable Unit Treatment Value Assumption)",
    "text": "✅ 1. SUTVA (Stable Unit Treatment Value Assumption)\n\nMeaning: Each unit’s potential outcomes are unaffected by the treatments given to other units, and the treatment itself is clearly defined.\nIntuition: Imagine a classroom: if one student’s treatment (like receiving extra tutoring) somehow affects another student’s performance, then the assumption is violated."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#ignorability-or-unconfoundedness",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#ignorability-or-unconfoundedness",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "✅ 2. Ignorability (or Unconfoundedness)",
    "text": "✅ 2. Ignorability (or Unconfoundedness)\n\nMeaning: Given the covariates (and with randomization, naturally), the treatment assignment is independent of the potential outcomes.\nIntuition: This is like saying that there are no hidden factors that both affect who gets the treatment and the outcomes. If we control for observed factors, the treatment assignment should be “as good as random.”"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#overlap-common-support",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#overlap-common-support",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "✅ 3. Overlap (Common Support)",
    "text": "✅ 3. Overlap (Common Support)\n\nMeaning: Every unit has a positive probability of receiving either the treatment or the control.\nIntuition: If some individuals are never eligible for treatment, then comparing them to those who can be treated can lead to biased estimates."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#final-thoughts-and-practice",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#final-thoughts-and-practice",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "Final Thoughts and Practice",
    "text": "Final Thoughts and Practice\n\nWhy All This Matters: Understanding these concepts helps you rigorously think about what it means to say one thing causes another. Whether you’re running an experiment or analyzing observational data, these ideas guide your interpretation.\nScaffolding Your Learning:\n\nStart Simple: Work through individual examples like the ones above.\nBuild Complexity: Gradually introduce additional variables (like covariates in ANCOVA) and consider their impact.\nPractice Problems: Apply these concepts in simulated data scenarios to see how the methods work in practice."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-1-heres-the-full-table-of-potential-outcomes-both-y1-and-y0",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-1-heres-the-full-table-of-potential-outcomes-both-y1-and-y0",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🔷 Step 1: Here’s the Full Table of Potential Outcomes (both Y(1) and Y(0))",
    "text": "🔷 Step 1: Here’s the Full Table of Potential Outcomes (both Y(1) and Y(0))\n\n\n\nStudent\n\\(Y(1)\\) (if treated)\n\\(Y(0)\\) (if control)\n\n\n\n\nMaya\n88\n82\n\n\nZoe\n92\n92\n\n\nLiam\n85\n75\n\n\nRay\n90\n83"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-2-treatment-assignment",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-2-treatment-assignment",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧷 Step 2: Treatment Assignment",
    "text": "🧷 Step 2: Treatment Assignment\nNow let’s say this is how treatment was assigned:\n\nMaya: ✅ Treated\nZoe: ❌ Control\nLiam: ✅ Treated\nRay: ❌ Control"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-3-can-you-fill-in-the-table-below",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-3-can-you-fill-in-the-table-below",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🔷 Step 3: Can You Fill in the Table Below?",
    "text": "🔷 Step 3: Can You Fill in the Table Below?\n➡️ Use what we know about \\(Y(1), Y(0)\\), and treatment assignment to complete the observed outcome for each student. You’ll also indicate which potential outcome is unobserved.\n\n\n\n\n\n\n\n\n\nStudent\nTreatment Given? (T)\nOutcome \\(Y^{\\text{obs}}\\)\nUnobserved Counterfactual\n\n\n\n\nMaya\n\n\n\n\n\nZoe\n\n\n\n\n\nLiam\n\n\n\n\n\nRay\n\n\n\n\n\n\nTry filling this in! You only need to:\n\nChoose the observed outcome based on treatment\nLabel the counterfactual (the outcome we don’t get to see)\n\nGreat effort! Let’s fill in the table step-by-step together and then move forward. 😊"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#completed-table-observed-and-unobserved-outcomes",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#completed-table-observed-and-unobserved-outcomes",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "✅ Completed Table: Observed and Unobserved Outcomes",
    "text": "✅ Completed Table: Observed and Unobserved Outcomes\n\n\n\n\n\n\n\n\n\nStudent\nTreatment Given? (\\(T\\))\nObserved Outcome \\(Y^{\\text{obs}}\\)\nUnobserved Counterfactual\n\n\n\n\nMaya\n✅ 1 (Treated)\n88\n\\(Y(0) = 82\\)\n\n\nZoe\n❌ 0 (Control)\n92\n\\(Y(1) = 92\\)\n\n\nLiam\n✅ 1 (Treated)\n85\n\\(Y(0) = 75\\)\n\n\nRay\n❌ 0 (Control)\n83\n\\(Y(1) = 90\\)"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#interpretation",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#interpretation",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧠 Interpretation",
    "text": "🧠 Interpretation\nWe observe:\n\n\\(Y(1)\\) for Maya and Liam (treated)\n\\(Y(0)\\) for Zoe and Ray (control)\n\nWe never observe both for any student!"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#next-step-estimate-the-ate",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#next-step-estimate-the-ate",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🔷 Next Step: Estimate the ATE",
    "text": "🔷 Next Step: Estimate the ATE\nLet’s calculate:\n\nThe mean outcome for treated students\nThe mean outcome for control students\nEstimate the Average Treatment Effect (ATE) using observed data"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#try-this",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#try-this",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "✏️ Try This",
    "text": "✏️ Try This\n\n\n\nGroup\nStudents\nObserved Scores\nMean Outcome\n\n\n\n\nTreated\nMaya, Liam\n[88, 85]\n?\n\n\nControl\nZoe, Ray\n[92, 83]\n?\n\n\n\n\\[\n\\text{Estimated ATE} = \\text{Mean Treated} - \\text{Mean Control}\n\\]\nCan you fill in the mean scores and calculate the estimated ATE? Give it a try!\nAwesome! Let’s solve it together. ✅"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-1-compute-group-means",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-1-compute-group-means",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "✅ Step 1: Compute Group Means",
    "text": "✅ Step 1: Compute Group Means\n\nTreated Group: Maya, Liam\nObserved scores: [88, 85]\n\\[\n\\text{Mean Treated} = \\frac{88 + 85}{2} = \\frac{173}{2} = 86.5\n\\]\n\n\nControl Group: Zoe, Ray\nObserved scores: [92, 83]\n\\[\n\\text{Mean Control} = \\frac{92 + 83}{2} = \\frac{175}{2} = 87.5\n\\]"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-2-estimate-ate",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-2-estimate-ate",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "✅ Step 2: Estimate ATE",
    "text": "✅ Step 2: Estimate ATE\n\\[\n\\text{Estimated ATE} = \\text{Mean Treated} - \\text{Mean Control} = 86.5 - 87.5 = -1.0\n\\]\n\n\n🎯 Interpretation\n\nThe observed estimate suggests the treatment reduced scores by 1 point.\nBut we know from the true potential outcomes (from earlier) that the treatment actually helped all treated students!\n\nHmm… something’s off. Why?"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#next-step-whats-the-problem-confounding",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#next-step-whats-the-problem-confounding",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🔎 Next Step: What’s the Problem? (Confounding!)",
    "text": "🔎 Next Step: What’s the Problem? (Confounding!)\nLet’s go deeper. Look again:\n\n\n\nStudent\nPre-Treatment Score\nTreated?\n\\(Y(1)\\)\n\\(Y(0)\\)\n\n\n\n\nMaya\n88\n✅\n88\n82\n\n\nZoe\n92\n❌\n92\n92\n\n\nLiam\n85\n✅\n85\n75\n\n\nRay\n83\n❌\n90\n83\n\n\n\nZoe and Ray were already stronger readers before treatment.\nSo even though the treatment helped, it looks harmful if we don’t adjust for the pre-treatment scores!"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#lets-fix-it-with-ancova",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#lets-fix-it-with-ancova",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🎯 Let’s Fix It with ANCOVA",
    "text": "🎯 Let’s Fix It with ANCOVA\nANCOVA helps us adjust for pre-treatment differences.\nLet’s write the model:\n\\[\n\\text{Post-Test} = \\alpha + \\tau \\cdot \\text{Treatment} + \\beta \\cdot \\text{Pre-Test} + \\epsilon\n\\]\nHere:\n\n\\(\\tau\\): treatment effect after adjusting for pre-test\n\\(\\beta\\): effect of pre-test score"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#data-recap",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#data-recap",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧾 Data Recap",
    "text": "🧾 Data Recap\n\n\n\nStudent\nPre-Test (X)\nTreatment (T)\nPost-Test (Y)\n\n\n\n\nMaya\n88\n1\n88\n\n\nZoe\n92\n0\n92\n\n\nLiam\n85\n1\n85\n\n\nRay\n83\n0\n83"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#ancova-model-recap",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#ancova-model-recap",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🎯 ANCOVA Model (recap)",
    "text": "🎯 ANCOVA Model (recap)\nWe want to fit a linear model:\n\\[\nY_i = \\alpha + \\tau T_i + \\beta X_i + \\epsilon_i\n\\]\nWhere:\n\n\\(Y_i\\): post-test score\n\\(T_i\\): treatment (1 = yes, 0 = no)\n\\(X_i\\): pre-test score\n\\(\\tau\\): adjusted treatment effect\n\\(\\beta\\): effect of pre-test on post-test"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-by-step-ancova-table",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-by-step-ancova-table",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "📋 Step-by-Step ANCOVA Table",
    "text": "📋 Step-by-Step ANCOVA Table\nLet’s compute a few key quantities.\n\n\n\n\n\n\n\n\n\n\nStudent\nPre-Test \\(X\\)\nTreatment \\(T\\)\nPost-Test \\(Y\\)\nContribution to Regression\n\n\n\n\nMaya\n88\n1\n88\nHelps estimate \\(\\tau\\) & \\(\\beta\\)\n\n\nZoe\n92\n0\n92\nHelps estimate \\(\\beta\\)\n\n\nLiam\n85\n1\n85\nHelps estimate \\(\\tau\\) & \\(\\beta\\)\n\n\nRay\n83\n0\n83\nHelps estimate \\(\\beta\\)\n\n\n\nWe can already spot something:\n🔹 Students with higher pre-tests also have higher post-tests, regardless of treatment.\nSo without controlling for \\(X\\), the treatment might look worse than it is."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#want-to-estimate-tau-the-adjusted-treatment-effect",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#want-to-estimate-tau-the-adjusted-treatment-effect",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🔢 Want to Estimate \\(\\tau\\) (the Adjusted Treatment effect)?",
    "text": "🔢 Want to Estimate \\(\\tau\\) (the Adjusted Treatment effect)?\nNice choice! 😊 Using group averages adjusted by covariates is a very intuitive way to see how ANCOVA works without full regression formulas. Let’s walk through it step by step."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-1-data-summary",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-1-data-summary",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧾 Step 1: Data Summary",
    "text": "🧾 Step 1: Data Summary\nHere’s our data again:\n\n\n\nStudent\nPre-Test (X)\nTreatment (T)\nPost-Test (Y)\n\n\n\n\nMaya\n88\n1\n88\n\n\nZoe\n92\n0\n92\n\n\nLiam\n85\n1\n85\n\n\nRay\n83\n0\n83"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-2-compute-group-means",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-2-compute-group-means",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "📊 Step 2: Compute Group Means",
    "text": "📊 Step 2: Compute Group Means\n\n✅ Treated Group (Maya, Liam)\n\nPre-test avg: \\(\\frac{88 + 85}{2} = 86.5\\)\nPost-test avg: \\(\\frac{88 + 85}{2} = 86.5\\)\n\n\n\n❌ Control Group (Zoe, Ray)\n\nPre-test avg: \\(\\frac{92 + 83}{2} = 87.5\\)\nPost-test avg: \\(\\frac{92 + 83}{2} = 87.5\\)"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-3-problem-with-simple-comparison",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-3-problem-with-simple-comparison",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🤔 Step 3: Problem with Simple Comparison",
    "text": "🤔 Step 3: Problem with Simple Comparison\n\\[\n\\text{Post-test difference} = 86.5 - 87.5 = -1.0\n\\]\nIt looks like the treatment made things worse. But look again:\n\nThe control group started with higher pre-test scores!\nWe need to adjust for this."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-4-adjust-post-test-means-to-a-common-pre-test",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-4-adjust-post-test-means-to-a-common-pre-test",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧮 Step 4: Adjust Post-Test Means to a Common Pre-Test",
    "text": "🧮 Step 4: Adjust Post-Test Means to a Common Pre-Test\nLet’s adjust both groups to a common pre-test level, like the overall mean pre-test:\n\\[\n\\text{Overall pre-test mean} = \\frac{88 + 92 + 85 + 83}{4} = \\frac{348}{4} = 87\n\\]\nNow assume post-test scores rise linearly with pre-test (using a slope we’ll estimate soon)."
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-5-estimate-the-slope-beta",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-5-estimate-the-slope-beta",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🪜 Step 5: Estimate the Slope \\(\\beta\\)",
    "text": "🪜 Step 5: Estimate the Slope \\(\\beta\\)\nLet’s look at control group only:\n\nZoe: X = 92, Y = 92\nRay: X = 83, Y = 83\n\nSo for controls, Post = Pre → slope \\(\\beta = 1\\)"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-6-adjust-means-using-slope",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-6-adjust-means-using-slope",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🧮 Step 6: Adjust Means Using Slope",
    "text": "🧮 Step 6: Adjust Means Using Slope\n\nControl Group\nTheir pre-test avg = 87.5, but we want to adjust to 87.\n\\[\n\\begin{align*}\n\\text{Adjust by } \\beta (87 - 87.5) &= 1 \\cdot (-0.5) = -0.5 \\\\\n\\text{Adjusted mean} &= 87.5 - 0.5 = 87.0\n\\end{align*}\n\\]\n\n\nTreated Group\nPre-test avg = 86.5 → adjust by:\n\\[\n\\begin{align*}\n1 \\cdot (87 - 86.5) &= 1 \\cdot (+0.5) \\\\\n\\text{Adjusted mean} &= 86.5 + 0.5 = 87.0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/week-10_ancova-causal-inference_part1.html#step-7-estimate-adjusted-treatment-effect",
    "href": "lectures/week-10_ancova-causal-inference_part1.html#step-7-estimate-adjusted-treatment-effect",
    "title": "ANCOVA and Causal Inference Using the Potential Outcomes Framework",
    "section": "🎉 Step 7: Estimate Adjusted Treatment Effect",
    "text": "🎉 Step 7: Estimate Adjusted Treatment Effect\n\\[\n\\text{Adjusted ATE} = 87.0 - 87.0 = 0\n\\]\n\n🧠 Interpretation\nBefore adjustment:\n📉 It looked like the treatment had a negative effect (-1).\nAfter adjustment:\n📏 The treatment effect is 0 → students improved at the same rate once we control for starting point.\n✅ ANCOVA lets us level the playing field, so we don’t mistake “high scorers” for “treatment success.”"
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html",
    "href": "lectures/week-11_row-column-youden.html",
    "title": "Row and Column Designs",
    "section": "",
    "text": "In the realm of experimental design, Row–Column Designs serve as an extension of traditional blocking techniques by incorporating two independent blocking factors: rows and columns. These designs are instrumental in controlling multiple sources of variability, thereby enhancing the precision and efficiency of treatment comparisons. By organizing experimental units into a matrix of rows and columns, Row–Column Designs mitigate the influence of two distinct nuisance factors, enabling clearer insights into the effects of the primary treatments under investigation.\nIntuitive Example: Imagine conducting an agricultural experiment to test different crop varieties. The field is naturally divided into rows and columns due to varying sunlight and soil moisture gradients. By applying each variety once per row and column, you control for these environmental gradients, ensuring that differences in crop performance are attributable to the varieties themselves rather than external factors.\nThis lecture delves into the structure, analysis, and practical applications of Row–Column Designs, with a focus on specific examples like Latin Square Designs and Youden Designs. We will explore theoretical foundations, implement analyses using R, and engage with advanced topics to provide a comprehensive understanding suitable for graduate-level studies.\n\n\n\nUnderstand the principles and utility of Row–Column Designs.\nExplore specific examples such as Latin Square and Youden Designs.\nAnalyze data using ANOVA tailored for Row–Column Designs.\nExtend designs to accommodate factorial experiments.\nImplement Row–Column Designs in R with step-by-step guidance.\nReinforce concepts through exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#objectives",
    "href": "lectures/week-11_row-column-youden.html#objectives",
    "title": "Row and Column Designs",
    "section": "",
    "text": "Understand the principles and utility of Row–Column Designs.\nExplore specific examples such as Latin Square and Youden Designs.\nAnalyze data using ANOVA tailored for Row–Column Designs.\nExtend designs to accommodate factorial experiments.\nImplement Row–Column Designs in R with step-by-step guidance.\nReinforce concepts through exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#rowcolumn-design-structure",
    "href": "lectures/week-11_row-column-youden.html#rowcolumn-design-structure",
    "title": "Row and Column Designs",
    "section": "Row–Column Design Structure",
    "text": "Row–Column Design Structure\nA Row–Column Design arranges experimental units in a two-dimensional grid comprising rows and columns. Each cell in the grid represents an experimental unit to which a treatment is applied. This design incorporates two independent blocking factors:\n\nRows: Control for variability along the horizontal dimension.\nColumns: Control for variability along the vertical dimension.\n\nDiagram of a 3x3 Latin Square:\n\n\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\n\nRow 1\nA\nB\nC\n\n\nRow 2\nB\nC\nA\n\n\nRow 3\nC\nA\nB"
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#blocking-factors",
    "href": "lectures/week-11_row-column-youden.html#blocking-factors",
    "title": "Row and Column Designs",
    "section": "Blocking Factors",
    "text": "Blocking Factors\n\nRow Blocks: Address systematic variability across rows, such as gradients in environmental conditions.\nColumn Blocks: Address systematic variability across columns, such as variations in soil composition.\n\nIntuitive Explanation: Consider a greenhouse experiment where temperature gradients run horizontally and light intensity gradients run vertically. By arranging treatments in a row–column grid, you control for these two sources of environmental variability, ensuring that treatment effects are not confounded with spatial gradients."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#advantages",
    "href": "lectures/week-11_row-column-youden.html#advantages",
    "title": "Row and Column Designs",
    "section": "Advantages",
    "text": "Advantages\n\nReduced Variability: By controlling two sources of variability, Row–Column Designs offer greater precision in estimating treatment effects.\nEfficiency: These designs make efficient use of experimental units, especially in small-scale experiments with inherent multidimensional variability.\nFlexibility: They can be extended to accommodate factorial experiments and multiple interactions.\nEnhanced Analytical Power: ANOVA tailored for Row–Column Designs can effectively isolate treatment effects from block effects.\n\nReference: Montgomery, D. C., Peck, E. A., & Vining, G. G. (2020). Introduction to Linear Regression Analysis. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#definition",
    "href": "lectures/week-11_row-column-youden.html#definition",
    "title": "Row and Column Designs",
    "section": "Definition",
    "text": "Definition\nA Latin Square Design is a specialized type of Row–Column Design where:\n\nEach treatment appears exactly once in each row.\nEach treatment appears exactly once in each column.\n\nThis ensures that the design is balanced and controls for both row and column effects effectively."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#construction",
    "href": "lectures/week-11_row-column-youden.html#construction",
    "title": "Row and Column Designs",
    "section": "Construction",
    "text": "Construction\n\nStandard Latin Squares:\nFor \\(v = 3\\) treatments, a Latin Square can be constructed as follows:\n\n\\[\n\\begin{array}{|c|c|c|}\n   \\hline\n   A & B & C \\\\\n   \\hline\n   B & C & A \\\\\n   \\hline\n   C & A & B \\\\\n   \\hline\n   \\end{array}\n\\]\n\nRows and Columns: Each treatment \\(A\\), \\(B\\), and \\(C\\) appears once per row and column.\n\n\nRandomization:\nTo prevent systematic bias, randomize the order of treatments within rows and columns. This can be achieved by randomly permuting the treatment labels for rows and columns."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#model",
    "href": "lectures/week-11_row-column-youden.html#model",
    "title": "Row and Column Designs",
    "section": "Model",
    "text": "Model\nFor a Row–Column Design with \\(b\\) rows, \\(c\\) columns, and \\(v\\) treatments, the ANOVA model is defined as:\n\\[\nY_{hqi} = \\mu + \\theta_h + \\phi_q + \\tau_i + \\epsilon_{hqi},\n\\]\nwhere:\n\n\\(Y_{hqi}\\): Response for treatment \\(i\\) in row \\(h\\) and column \\(q\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of row \\(h\\).\n\\(\\phi_q\\): Effect of column \\(q\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hqi} \\sim N(0, \\sigma^2)\\): Random error term.\n\n\nKey Assumptions\n\nNormality: The residuals (\\(\\epsilon_{hqi}\\)) are normally distributed.\nHomogeneity of Variance (Homoscedasticity): The variance of residuals is constant across all treatments, rows, and columns.\nIndependence: Observations are independent of each other.\nNo Interaction: There is no interaction between rows and treatments or columns and treatments, i.e., the effects of treatments are consistent across rows and columns.\n\n\n\nANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegrees of Freedom (DF)\nSum of Squares (SS)\nMean Square (MS)\nF-ratio\n\n\n\n\nRows\n\\(b - 1\\)\n\\(SS_{\\text{Rows}}\\)\n\\(MS_{\\text{Rows}} = \\frac{SS_{\\text{Rows}}}{b - 1}\\)\n\\(F = \\frac{MS_{\\text{Rows}}}{MS_{\\text{Error}}}\\)\n\n\nColumns\n\\(c - 1\\)\n\\(SS_{\\text{Columns}}\\)\n\\(MS_{\\text{Columns}} = \\frac{SS_{\\text{Columns}}}{c - 1}\\)\n\\(F = \\frac{MS_{\\text{Columns}}}{MS_{\\text{Error}}}\\)\n\n\nTreatments adj\n\\(v - 1\\)\n\\(SS_{\\text{Trt adj}}\\)\n\\(MS_{\\text{Trt adj}} = \\frac{SS_{\\text{Trt adj}}}{v - 1}\\)\n\\(F = \\frac{MS_{\\text{Treatments}}}{MS_{\\text{Error}}}\\)\n\n\nError\n\\(bc - b - c - v + 2\\)\n\\(SS_{\\text{Error}}\\)\n\\(MS_{\\text{Error}} = \\frac{SS_{\\text{Error}}}{bc - b - c - v + 2}\\)\n-\n\n\nTotal\n\\(b c - 1\\)\n\\(SS_{\\text{Total}}\\)\n-\n-"
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#hypothesis-testing",
    "href": "lectures/week-11_row-column-youden.html#hypothesis-testing",
    "title": "Row and Column Designs",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): All treatment means are equal (\\(\\tau_i = 0\\) for all \\(i\\)).\nAlternative Hypothesis (\\(H_A\\)): At least one treatment mean differs.\n\nDecision Rule: Reject \\(H_0\\) if the F-ratio for treatments exceeds the critical value from the F-distribution with degrees of freedom \\((v - 1, bc - b - c - v + 2)\\) at the chosen significance level (\\(\\alpha\\))."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#example-interpretation",
    "href": "lectures/week-11_row-column-youden.html#example-interpretation",
    "title": "Row and Column Designs",
    "section": "Example Interpretation",
    "text": "Example Interpretation\n\nRows and Columns: Assess the significance of row and column effects. Significant row or column effects indicate that these blocking factors explain a substantial portion of the variability in the response.\nTreatments: Determine if treatments have a significant impact on the response variable after accounting for row and column effects.\nError: Represents unexplained variability, serving as the denominator for F-ratios."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#computational-formula",
    "href": "lectures/week-11_row-column-youden.html#computational-formula",
    "title": "Row and Column Designs",
    "section": "Computational Formula",
    "text": "Computational Formula\nThese formulas typically apply to the Analysis of Variance (ANOVA) for experimental designs involving treatments laid out in a structure with rows and columns, aiming to control for variability from these two sources (e.g., Latin Squares, Youden Squares, Row-Column Designs).\n\n\\(b\\): Number of rows\n\\(c\\): Number of columns\n\\(v\\): Number of treatments\n\\(y_{hq}\\): Observation in row \\(h\\) and column \\(q\\)\n\\(t(h,q)\\): Treatment applied in cell \\((h,q)\\)\n\\(N = bc\\): Total number of observations (assuming a complete layout)\n\\(B_h = \\sum_{q=1}^{c} y_{hq}\\): Total for row \\(h\\) (sum of \\(c\\) observations)\n\\(C_q = \\sum_{h=1}^{b} y_{hq}\\): Total for column \\(q\\) (sum of \\(b\\) observations)\n\\(T_i = \\sum_{(h,q): t(h,q)=i} y_{hq}\\): Total for treatment \\(i\\)\n\\(G = \\sum_{h=1}^{b} \\sum_{q=1}^{c} y_{hq}\\): Grand total (sum of \\(N=bc\\) observations)\n\\(r\\): Number of replications per treatment (assumed constant in some provided formulas, but could be \\(r_i\\))\n\\(n_{h.i}\\): Number of times treatment \\(i\\) appears in row \\(h\\)\n\\(n_{.qi}\\): Number of times treatment \\(i\\) appears in column \\(q\\)\n\\(CF = \\frac{G^2}{N} = \\frac{G^2}{bc}\\): Correction Factor\n\nNote. We have \\(r\\) replications per treatment in a complete design, but in some cases, \\(r_i\\) may vary. The formulas provided assume a complete layout with equal replication. ## Unadjusted Row Effect Sum of Squares (\\(SS_\\theta\\))\n\\[\nSS_{Rows} = \\frac{1}{c} \\sum_{h=1}^{b} B_h^2 - \\frac{2 G^2}{bc} + \\frac{b G^2}{bc} = \\frac{1}{c} \\sum_{h=1}^{b} B_h^2 - \\frac{G^2}{bc}\n\\]"
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#derivation-of-row-effect-sum-of-squares",
    "href": "lectures/week-11_row-column-youden.html#derivation-of-row-effect-sum-of-squares",
    "title": "Row and Column Designs",
    "section": "Derivation of Row Effect Sum of Squares",
    "text": "Derivation of Row Effect Sum of Squares\nThe unadjusted sum of squares for rows measures the variability between row means.\n\nRow mean \\(h\\): \\(\\bar{y}_{h.} = B_h / c\\)\nGrand mean: \\(\\bar{y}_{..} = G / N = G / (bc)\\)\nThe sum of squares is the sum of squared deviations from the grand mean, weighted by the number of observations per row:\n\n\\[\nSS_{Rows} = \\sum_{h=1}^{b} \\sum_{q=1}^{c} (\\bar{y}_{h.} - \\bar{y}_{..})^2 = \\sum_{h=1}^{b} c (\\bar{y}_{h.} - \\bar{y}_{..})^2\n\\]\n\nSubstitute the means:\n\n\\[\nSS_{Rows} = \\sum_{h=1}^{b} c \\left(\\frac{B_h}{c} - \\frac{G}{bc}\\right)^2 = \\sum_{h=1}^{b} c \\left[\\frac{B_h^2}{c^2} - \\frac{2 B_h G}{bc^2} + \\frac{G^2}{(bc)^2}\\right]\n\\]\n\nDistribute the sum and simplify:\n\n\\[\nSS_{Rows} = \\sum_{h=1}^{b} \\left[\\frac{B_h^2}{c} - \\frac{2 B_h G}{bc} + \\frac{c G^2}{b^2 c^2}\\right]\n\\]\n\\[\nSS_{Rows} = \\left(\\frac{1}{c} \\sum_{h=1}^{b} B_h^2\\right) - \\frac{2 G}{bc} \\left(\\sum_{h=1}^{b} B_h\\right) + \\sum_{h=1}^{b} \\frac{G^2}{bc}\n\\]\n\nSince \\(\\sum B_h = G\\) and the last term sums \\(b\\) times:\n\n\\[\nSS_{Rows} = \\frac{1}{c} \\sum_{h=1}^{b} B_h^2 - \\frac{2 G^2}{bc} + \\frac{b G^2}{bc} = \\frac{1}{c} \\sum_{h=1}^{b} B_h^2 - \\frac{G^2}{bc}\n\\]\n\nThe formula’s \\(1/c\\) factor arises naturally because each \\(B_h\\) is the sum of \\(c\\) observations. Dividing the squared total \\(B_h^2\\) by \\(c\\) scales it appropriately for the sum of squares calculation based on means."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#unadjusted-column-effect-sum-of-squares-ss_phi",
    "href": "lectures/week-11_row-column-youden.html#unadjusted-column-effect-sum-of-squares-ss_phi",
    "title": "Row and Column Designs",
    "section": "Unadjusted Column Effect Sum of Squares (\\(ss_\\phi\\))",
    "text": "Unadjusted Column Effect Sum of Squares (\\(ss_\\phi\\))\n\\[\nSS_{Cols} = \\frac{1}{b} \\sum_{q=1}^{c} C_q^2 - \\frac{G^2}{bc}\n\\]\n\nDerivation of Standard Formula:\n\nThe derivation is analogous to the Row SS derivation, swapping roles of rows and columns.\n\nColumn mean \\(q\\): \\(\\bar{y}_{.q} = C_q / b\\)\nGrand mean: \\(\\bar{y}_{..} = G / (bc)\\)\nSum of squares:\n\n\\[\nSS_{Cols} = \\sum_{q=1}^{c} \\sum_{h=1}^{b} (\\bar{y}_{.q} - \\bar{y}_{..})^2 = \\sum_{q=1}^{c} b (\\bar{y}_{.q} - \\bar{y}_{..})^2\n\\]\n\nFollowing similar algebraic steps as for \\(SS_{Rows}\\) leads to:\n\n\\[\nSS_{Cols} = \\frac{1}{b} \\sum_{q=1}^{c} C_q^2 - \\frac{G^2}{bc}\n\\]\n\nTreatment Effect (Adjusted for Rows and Columns)\nAdjustment is needed because treatments are generally not orthogonal to rows and columns. We first find adjusted treatment totals (\\(Q_i\\)) and then use them to calculate the adjusted treatment sum of squares (\\(SS_\\text{Trt adju}\\)).\n\nAdjusted Treatment Totals (\\(Q_i\\))\n\\[\nQ_i = T_i - \\frac{1}{c} \\sum_{h=1}^{b} n_{h.i} B_h - \\frac{1}{b} \\sum_{q=1}^{c} n_{.qi} C_q + \\frac{b}{c} r_i G\n\\]\nwhere\n\n\\(T_i\\): Total for treatment \\(i\\)\n\\(B_h\\): Total for row \\(h\\)\n\\(C_q\\): Total for column \\(q\\)\n\\(G\\): Grand total\n\\(h\\): Row index\n\\(q\\): Column index\n\\(n_{h.i}\\): Number of times treatment \\(i\\) appears in row \\(h\\)\n\\(n_{.qi}\\): Number of times treatment \\(i\\) appears in column \\(q\\)\n\nNote. This uses potentially variable replication \\(r_i\\). If \\(r\\) is constant, \\(r_i\\) becomes \\(r\\). This formula assumes \\(r_i\\) replications per treatment."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#explanation-of-standard-formula",
    "href": "lectures/week-11_row-column-youden.html#explanation-of-standard-formula",
    "title": "Row and Column Designs",
    "section": "Explanation of Standard Formula:",
    "text": "Explanation of Standard Formula:\n\nThe adjusted total \\(Q_i\\) is the raw treatment total$ T_i$ corrected for the effects of the specific rows and columns where treatment \\(i\\) appeared.\nThe adjustment subtracts the estimated contribution of row means (\\(B_h/c\\)) and column means (\\(C_q/b\\)) for the units receiving treatment i and adds back the overall mean effect (\\(G/bc\\)) scaled by the number of replications (\\(r_i\\)).\nThis quantity arises naturally from the normal equations for estimating treatment effects (\\(\\tau_i\\)) in the linear model \\(y_{hq(i)} = \\mu + \\theta_h + \\phi_q + \\tau_i + \\epsilon_{hq(i)}\\) after eliminating row and column parameters. The \\(Q_i\\) form the right-hand side of the reduced normal equations for the treatment effects."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#adjusted-treatment-sum-of-squares-ss_texttrt-adj",
    "href": "lectures/week-11_row-column-youden.html#adjusted-treatment-sum-of-squares-ss_texttrt-adj",
    "title": "Row and Column Designs",
    "section": "Adjusted Treatment Sum of Squares (\\(SS_\\text{Trt adj}\\))",
    "text": "Adjusted Treatment Sum of Squares (\\(SS_\\text{Trt adj}\\))\n\\[\nSS_\\text{Trt adj} = \\sum_{i=1}^{v} Q_i \\hat{\\tau}_i\n\\]\nwhere:\n\n\\(ss_{Tadj}\\): Adjusted treatment sum of squares\n\\(Q_i\\): Adjusted treatment total for treatment \\(i\\)\n\\(\\hat{\\tau}_i\\): Estimated treatment effect for treatment \\(i\\)\n\\(v\\): Number of treatments\n\\(\\hat{\\tau}_i\\): Estimated treatment effect (solution to the reduced normal equations)"
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#explanation-of-standard-formula-1",
    "href": "lectures/week-11_row-column-youden.html#explanation-of-standard-formula-1",
    "title": "Row and Column Designs",
    "section": "Explanation of Standard Formula:",
    "text": "Explanation of Standard Formula:\n\nThis formula arises directly from the theory of linear models.\nThe adjusted sum of squares for a factor (here, treatments) can be expressed as the quadratic form \\(\\hat{\\tau}' Q\\), where \\(\\hat{\\tau}\\) is the vector of estimated treatment effects (solutions to the reduced normal equations \\(C \\hat{\\tau} = Q\\)) and \\(Q\\) is the vector of adjusted totals.\nExpanding \\(\\hat{\\tau}' Q\\) gives the summation \\(\\sum Q_i \\hat{\\tau}_i\\).\nAlternative computational formulas often exist, especially for balanced designs (e.g., involving \\(\\sum Q_i^2\\)), but this form is theoretically fundamental."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#analysis-of-variance-anova",
    "href": "lectures/week-11_row-column-youden.html#analysis-of-variance-anova",
    "title": "Row and Column Designs",
    "section": "Analysis of Variance (ANOVA)",
    "text": "Analysis of Variance (ANOVA)\nFor a Latin Square Design with \\(v\\) treatments, the ANOVA model includes:\n\\[\nY_{hqi} = \\mu + \\theta_h + \\phi_q + \\tau_i + \\epsilon_{hqi},\n\\]\nwhere:\n\n\\(Y_{hqi}\\): Response for treatment \\(i\\) in row \\(h\\) and column \\(q\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of row \\(h\\).\n\\(\\phi_q\\): Effect of column \\(q\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hqi} \\sim N(0, \\sigma^2)\\): Random error term.\n\nIn a classical (fully balanced) Latin Square, the treatment sum of squares is the same whether you view it as “adjusted” (after removing row and column effects) or “unadjusted” (looking at treatments alone). In other words, they do not differ."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#why-they-are-the-same",
    "href": "lectures/week-11_row-column-youden.html#why-they-are-the-same",
    "title": "Row and Column Designs",
    "section": "Why They Are the Same",
    "text": "Why They Are the Same\n\nOrthogonality\nIn a Latin Square, the design is constructed so that treatments are orthogonal (statistically independent) to the row and column factors. Orthogonality means that when you estimate the effect of treatments, you are not “borrowing” or “confounding” variation that belongs to the row or column blocks.\nBalanced Cells\nEach treatment occurs exactly once in every row and once in every column, so there is no overlap in the sum of squares partition:\n\\[\n\\text{SSTotal} \\;=\\; \\text{SSRows} \\;+\\; \\text{SSColumns} \\;+\\; \\text{SSTreatments} \\;+\\; \\text{SSError}.\n\\] Because of this strict balance, the decomposition of sums of squares is clean and unique.\nAdjusted = Unadjusted for Orthogonal Designs\nWhen factors are orthogonal, “partial” or “adjusted” sum of squares for a given factor (e.g. treatments) equals the “unadjusted” sum of squares you would get if you calculated it before removing the other factors. No matter the sequence in which you enter factors into the model, the SS for each orthogonal factor is unchanged."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#when-they-might-differ",
    "href": "lectures/week-11_row-column-youden.html#when-they-might-differ",
    "title": "Row and Column Designs",
    "section": "When They Might Differ",
    "text": "When They Might Differ\n\nMissing Data or Unbalanced Layout\nIf some runs are missing or the design is not balanced, the orthogonality breaks. Then the “adjusted” (Type III or partial) sum of squares for treatments can differ from the “unadjusted” (Type I or sequential) sum of squares.\nNon-orthogonal Blocks/Treatments\nIf blocking factors and treatments are not arranged in an orthogonal way (as in some incomplete-block or more complex designs), you can again see discrepancies.\n\nIn the standard (complete, balanced) Latin Square, however, there is perfect orthogonality among row blocks, column blocks, and treatments—so the treatment SS is invariant to whether you consider it “adjusted” or “unadjusted.”"
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#example-anova-table-for-latin-square",
    "href": "lectures/week-11_row-column-youden.html#example-anova-table-for-latin-square",
    "title": "Row and Column Designs",
    "section": "Example ANOVA Table for Latin Square",
    "text": "Example ANOVA Table for Latin Square\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegrees of Freedom\nSum of Squares\nMean Square\nF-ratio\n\n\n\n\nRows\n\\(v - 1\\)\n\\(SS_\\text{Rows}\\)\n\\(MS_\\text{Rows}\\)\n\\(F = \\frac{MS_\\text{Rows}}{MS_\\text{Error}}\\)\n\n\nColumns\n\\(v - 1\\)\n\\(SS_\\text{Columns}\\)\n\\(MS_\\text{Columns}\\)\n\\(F = \\frac{MS_\\text{Columns}}{MS_\\text{Error}}\\)\n\n\nTreatments Adjusted\n\\(v - 1\\)\n\\(SS_\\text{Trt Adj}\\)\n\\(MS_\\text{Trt adj}\\)\n\\(F = \\frac{MS_\\text{Trt adj}}{MS_\\text{Error}}\\)\n\n\nError\n\\((v-2)(v-1)\\)\n\\(SS_\\text{Error}\\)\n\\(MS_\\text{Error}\\)\n\n\n\nTotal\n\\(v^2 - 1\\)\n\\(SS_\\text{Total}\\)\n\n\n\n\n\nHypothesis Testing:\n\nNull Hypothesis (\\(H_0\\)): All treatment means are equal (\\(\\tau_k = 0\\) for all \\(k\\)).\nAlternative Hypothesis (\\(H_A\\)): At least one treatment mean differs.\n\nDecision Rule: Reject \\(H_0\\) if the F-ratio for treatments exceeds the critical value from the F-distribution with degrees of freedom \\((v - 1, (v-2)(v-1))\\) at the chosen significance level (\\(\\alpha\\))."
  },
  {
    "objectID": "lectures/week-11_row-column-youden.html#example-in-r",
    "href": "lectures/week-11_row-column-youden.html#example-in-r",
    "title": "Row and Column Designs",
    "section": "Example in R",
    "text": "Example in R\n\n# Simulated Data for Latin Square Design\nset.seed(123)\ndata &lt;- data.frame(\n    row = factor(rep(1:3, each = 3)),\n    column = factor(rep(1:3, times = 3)),\n    treatment = factor(c(\"A\", \"B\", \"C\", \"B\", \"C\", \"A\", \"C\", \"A\", \"B\")),\n    response = c(10.2, 15.3, 20.1, 14.8, 19.5, 9.7, 19.0, 9.9, 14.2)\n)\n\n# Fit ANOVA Model\nfit &lt;- lm(response ~ row + column + treatment, data = data)\nanova(fit)\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nrow\n2\n1.0688889\n0.5344444\n6.5890411\n0.1317690\n\n\ncolumn\n2\n0.1088889\n0.0544444\n0.6712329\n0.5983607\n\n\ntreatment\n2\n138.2422222\n69.1211111\n852.1780822\n0.0011721\n\n\nResiduals\n2\n0.1622222\n0.0811111\nNA\nNA\n\n\n\n\n\n\nInterpretation: - Rows and Columns: Control for row and column effects, ensuring that treatment comparisons are not confounded by these blocking factors. - Treatments: Assess the main effect of treatments on the response variable after accounting for row and column effects. - Error: Represents variability not explained by rows, columns, or treatments."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html",
    "href": "lectures/week-12_mixed-models.html",
    "title": "Mixed Models",
    "section": "",
    "text": "In the landscape of statistical modeling, Random-Effects and Mixed-Effects Models play a pivotal role in analyzing data that exhibit hierarchical or grouped structures. Unlike fixed-effects models, which assume that the levels of a factor are the only ones of interest, random-effects models consider factors as random samples from a larger population. This distinction allows for the assessment of variability not just within treatments but also across different groups or clusters.\nIntuitive Analogy: Imagine conducting a study on the effectiveness of a new teaching method across various schools. If we treat schools as fixed effects, we’re only interested in those specific schools. However, if we consider schools as random effects, we acknowledge that these schools are a random sample from a broader population, enabling us to generalize our findings beyond the studied institutions.\nThis week’s focus encompasses:\n\nDifferentiating between fixed-effects and random-effects models.\nDelving into single and multiple random-effects models.\nExploring mixed-effects models that integrate both fixed and random components.\nImplementing these models in R with practical examples.\nEngaging with advanced topics through detailed proofs and mathematical derivations.\nReinforcing learning through targeted exercises.\n\n\n\nBy the end of this lecture, you will be able to:\n\nDifferentiate between fixed-effects and random-effects models.\nApply single random-effects models and interpret their results.\nAnalyze mixed-effects models, combining fixed and random components.\nEvaluate model assumptions and perform diagnostic checks.\nCompute confidence intervals and test hypotheses for random effects.\nEngage with advanced mathematical concepts through proofs and derivations.\nEnhance your understanding through practical R examples and exercises."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#objectives",
    "href": "lectures/week-12_mixed-models.html#objectives",
    "title": "Mixed Models",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDifferentiate between fixed-effects and random-effects models.\nApply single random-effects models and interpret their results.\nAnalyze mixed-effects models, combining fixed and random components.\nEvaluate model assumptions and perform diagnostic checks.\nCompute confidence intervals and test hypotheses for random effects.\nEngage with advanced mathematical concepts through proofs and derivations.\nEnhance your understanding through practical R examples and exercises."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#fixed-effects",
    "href": "lectures/week-12_mixed-models.html#fixed-effects",
    "title": "Mixed Models",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nDefinition\nFixed Effects are factors in a model where the levels are specifically chosen and of primary interest. The goal is to make inferences about these particular levels.\n\n\nExamples\n\nComparing the effectiveness of three specific fertilizers (A, B, C).\nEvaluating the performance of particular teaching methods.\n\n\n\nModeling Assumptions\n\nFactor levels are constants and not random.\nEffects are modeled as fixed parameters.\n\nIntuitive Example: Assessing the impact of three distinct fertilizers on plant growth where only these three fertilizers are of interest."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#random-effects",
    "href": "lectures/week-12_mixed-models.html#random-effects",
    "title": "Mixed Models",
    "section": "Random Effects",
    "text": "Random Effects\n\nDefinition\nRandom Effects are factors in a model where the levels are randomly sampled from a larger population. The focus is on understanding the variability across these random levels rather than the levels themselves.\n\n\nExamples\n\nVariability among machine operators selected randomly from a factory.\nBatch-to-batch variability in a manufacturing process.\n\n\n\nModeling Assumptions\n\nFactor levels are random variables, typically assumed to follow a normal distribution.\nEffects are modeled as random variables with a specified distribution.\n\nIntuitive Example: Studying the effect of different classrooms (randomly selected from all classrooms in a district) on student performance, aiming to generalize findings beyond the sampled classrooms."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#random-effects-one-way-model",
    "href": "lectures/week-12_mixed-models.html#random-effects-one-way-model",
    "title": "Mixed Models",
    "section": "Random-Effects One-Way Model",
    "text": "Random-Effects One-Way Model\nThe Random-Effects One-Way Model is suitable when a single random factor influences the response variable. This model accounts for variability due to the random factor and the inherent experimental error.\n\nModel Specification\n\\[\nY_{it} = \\mu + T_i + \\varepsilon_{it}, \\quad T_i \\sim N(0, \\sigma_T^2), \\quad \\varepsilon_{it} \\sim N(0, \\sigma^2)\n\\]\nWhere:\n\n\\(Y_{it}\\): Response for the \\(t\\)-th observation in the \\(i\\)-th group.\n\\(\\mu\\): Overall mean.\n\\(T_i\\): Random effect for the \\(i\\)-th group.\n\\(\\varepsilon_{it}\\): Random error term.\n\n\n\nKey Properties\n\nExpectation: \\(E[Y_{it}] = \\mu\\)\nVariance: \\(Var(Y_{it}) = \\sigma_T^2 + \\sigma^2\\)\nCovariance: \\(Cov(Y_{it}, Y_{is}) = \\sigma_T^2\\) for \\(t \\neq s\\) within the same group.\n\n\n\nANOVA for Random-Effects Models\nUnlike fixed-effects ANOVA, random-effects ANOVA decomposes the total variability into components attributable to random effects and residual error.\nANOVA Table:\n\n\n\n\n\n\n\n\n\n\nSource\nDegrees of Freedom\nSum of Squares (SS)\nMean Square (MS)\nExpected Mean Square\n\n\n\n\nTreatments\n\\(a - 1\\)\n\\(SS_T\\)\n\\(MS_T = \\frac{SS_T}{a - 1}\\)\n\\(a \\sigma_T^2 + \\sigma^2\\)\n\n\nError\n\\(a(n - 1)\\)\n\\(SS_E\\)\n\\(MS_E = \\frac{SS_E}{a(n - 1)}\\)\n\\(\\sigma^2\\)\n\n\nTotal\n\\(an - 1\\)\n\\(SS_{Total}\\)\n\n\n\n\n\nWhere:\n\n\\(a\\): Number of groups.\n\\(n\\): Number of observations per group.\n\n\n\nEstimation of Variance Components\nUsing Restricted Maximum Likelihood (REML), we estimate:\n\\[\n\\hat{\\sigma}^2_T = \\frac{MS_T - MS_E}{n}, \\quad \\hat{\\sigma}^2 = MS_E\n\\]\nExample in R:\n# Load necessary library\nlibrary(lme4)\n\n# Simulated Data\nset.seed(123)\ndata_single_random &lt;- data.frame(\n  group = factor(rep(1:5, each = 4)),\n  response = c(\n    rnorm(4, mean = 10, sd = 2),\n    rnorm(4, mean = 12, sd = 2),\n    rnorm(4, mean = 11, sd = 2),\n    rnorm(4, mean = 13, sd = 2),\n    rnorm(4, mean = 12, sd = 2)\n  )\n)\n\n# Fit Random-Effects Model\nmodel_random &lt;- lmer(response ~ 1 + (1 | group), data = data_single_random)\nsummary(model_random)\n\n# Extract Variance Components\nVarCorr(model_random)\nStep-by-Step Interpretation: 1. Model Fitting: The lmer function fits a random-intercept model where group is the random effect. 2. Summary Output: Provides estimates of fixed effects (overall mean) and random effects (variance components). 3. Variance Components: Extracted using VarCorr, showing the estimated variances for random effects and residuals."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#model-specification-1",
    "href": "lectures/week-12_mixed-models.html#model-specification-1",
    "title": "Mixed Models",
    "section": "Model Specification",
    "text": "Model Specification\n\\[\nY_{ij} = \\mu + \\alpha_i + T_j + \\varepsilon_{ij}\n\\]\nWhere:\n\n\\(Y_{ij}\\): Response for the \\(i\\)-th fixed effect level and \\(j\\)-th random effect level.\n\\(\\mu\\): Overall mean.\n\\(\\alpha_i\\): Fixed effect for the \\(i\\)-th level of a fixed factor.\n\\(T_j \\sim N(0, \\sigma_T^2)\\): Random effect for the \\(j\\)-th level of a random factor.\n\\(\\varepsilon_{ij} \\sim N(0, \\sigma^2)\\): Random error term.\n\nIntuitive Example: Assessing the impact of different teaching methods (fixed effect) across various schools (random effect) on student performance.\n\nExample: Randomized Block Design with Random Blocks\n\\[\nY_{ij} = \\mu + \\alpha_i + B_j + \\varepsilon_{ij}, \\quad B_j \\sim N(0, \\sigma_B^2)\n\\]\nWhere:\n\nalpha_i: Fixed effect of treatment.\nB_j: Random effect of block (e.g., different classrooms).\n\n\n\nR Implementation\n# Load necessary library\nlibrary(lme4)\n\n# Simulated Data for Mixed-Effects Model\nset.seed(456)\ndata_mixed &lt;- data.frame(\n  treatment = factor(rep(c(\"A\", \"B\", \"C\"), times = 10)),\n  block = factor(rep(1:10, each = 3)),\n  response = c(\n    rnorm(3, mean = 5, sd = 1),\n    rnorm(3, mean = 6, sd = 1),\n    rnorm(3, mean = 7, sd = 1),\n    rnorm(3, mean = 5, sd = 1),\n    rnorm(3, mean = 6, sd = 1),\n    rnorm(3, mean = 7, sd = 1),\n    rnorm(3, mean = 5, sd = 1),\n    rnorm(3, mean = 6, sd = 1),\n    rnorm(3, mean = 7, sd = 1),\n    rnorm(3, mean = 5, sd = 1)\n  )\n)\n\n# Fit Mixed-Effects Model\nmodel_mixed &lt;- lmer(response ~ treatment + (1 | block), data = data_mixed)\nsummary(model_mixed)\n\n# Confidence Intervals for Fixed Effects\nconfint(model_mixed, parm = \"treatment\")\n\n# Diagnostics\npar(mfrow = c(2, 2))\nplot(model_mixed)\nInterpretation: - Fixed Effects (treatment): Assess the impact of different treatments on the response variable. - Random Effects (block): Account for variability across different blocks (e.g., schools, classrooms). - Confidence Intervals: Provide a range of plausible values for the fixed effects. - Diagnostics: Residual plots help verify model assumptions such as normality and homoscedasticity."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#key-assumptions",
    "href": "lectures/week-12_mixed-models.html#key-assumptions",
    "title": "Mixed Models",
    "section": "Key Assumptions",
    "text": "Key Assumptions\n\nNormality: Random effects and residuals are normally distributed.\nIndependence: Observations are independent within and across groups.\nHomogeneity of Variance (Homoscedasticity): Variance is constant across all levels of fixed and random factors.\nNo Correlation: Random effects are uncorrelated with fixed effects."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#diagnostic-procedures",
    "href": "lectures/week-12_mixed-models.html#diagnostic-procedures",
    "title": "Mixed Models",
    "section": "Diagnostic Procedures",
    "text": "Diagnostic Procedures\n\nResidual Analysis\n\nResiduals vs. Fitted Values Plot: Checks for non-linearity, unequal error variances, and outliers.\nQ-Q Plot of Residuals: Assesses the normality of residuals.\nScale-Location Plot: Evaluates homoscedasticity.\nResiduals vs. Random Effects Plot: Detects any patterns that suggest violations of independence or homogeneity.\n\n\n\nVariance Components Estimation\n\nREML (Restricted Maximum Likelihood): Preferred method for estimating variance components as it accounts for the loss of degrees of freedom.\n\n\n\nR Implementation for Diagnostics\n# Fit the mixed-effects model\nmodel &lt;- lmer(response ~ treatment + (1 | block), data = data_mixed)\n\n# Diagnostic Plots\npar(mfrow = c(2, 2))\nplot(model)\n\n# Shapiro-Wilk Test for Normality of Residuals\nshapiro.test(resid(model))\n\n# Check for Homoscedasticity\nlibrary(car)\nleveneTest(response ~ treatment, data = data_mixed)\nInterpretation: - Normality Tests: Non-significant p-values indicate no evidence against normality. - Levene’s Test: Assesses equality of variances across groups; non-significant p-values suggest homoscedasticity. - Residual Plots: Should display random scatter without discernible patterns."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#fitting-random-effects-models",
    "href": "lectures/week-12_mixed-models.html#fitting-random-effects-models",
    "title": "Mixed Models",
    "section": "Fitting Random-Effects Models",
    "text": "Fitting Random-Effects Models\nRandom-effects models are essential when dealing with grouped or hierarchical data. Below is a step-by-step example of fitting a random-effects model using R.\n\nExample: Single Random Effect\nScenario: Assessing the effect of different diets on weight loss across various clinics. Clinics are randomly sampled from a larger population.\n# Load necessary library\nlibrary(lme4)\n\n# Simulated Data\nset.seed(123)\ndata_random &lt;- data.frame(\n  clinic = factor(rep(1:10, each = 5)),\n  diet = factor(rep(c(\"Diet1\", \"Diet2\", \"Diet3\", \"Diet4\", \"Diet5\"), times = 10)),\n  weight_loss = c(\n    rnorm(5, mean = 2, sd = 0.5),\n    rnorm(5, mean = 2.5, sd = 0.5),\n    rnorm(5, mean = 3, sd = 0.5),\n    rnorm(5, mean = 3.5, sd = 0.5),\n    rnorm(5, mean = 4, sd = 0.5),\n    rnorm(5, mean = 2, sd = 0.5),\n    rnorm(5, mean = 2.5, sd = 0.5),\n    rnorm(5, mean = 3, sd = 0.5),\n    rnorm(5, mean = 3.5, sd = 0.5),\n    rnorm(5, mean = 4, sd = 0.5)\n  )\n)\n\n# Fit Random-Effects Model\nmodel_random &lt;- lmer(weight_loss ~ 1 + (1 | clinic), data = data_random)\nsummary(model_random)\n\n# Extract Variance Components\nVarCorr(model_random)\nStep-by-Step Interpretation: 1. Data Preparation: Simulated data includes 10 clinics, each with 5 diet treatments. 2. Model Fitting: lmer fits a model with a random intercept for clinics. 3. Summary Output: Provides estimates for the overall mean, random effects variance, and residual variance. 4. Variance Components: VarCorr displays the estimated variances for the random effects and residuals."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#fitting-mixed-effects-models",
    "href": "lectures/week-12_mixed-models.html#fitting-mixed-effects-models",
    "title": "Mixed Models",
    "section": "Fitting Mixed-Effects Models",
    "text": "Fitting Mixed-Effects Models\nMixed-Effects Models accommodate both fixed and random factors, offering a versatile framework for complex data structures.\n\nExample: Two-Way Factorial with Random Blocks\nScenario: Investigating the effect of two factors (e.g., fertilizer type and watering frequency) on plant growth across randomly selected greenhouses.\n# Load necessary library\nlibrary(lme4)\n\n# Simulated Data\nset.seed(456)\ndata_mixed &lt;- data.frame(\n  greenhouse = factor(rep(1:15, each = 4)),\n  fertilizer = factor(rep(c(\"F1\", \"F2\"), each = 2, times = 15)),\n  watering = factor(rep(c(\"W1\", \"W2\"), times = 30)),\n  growth = c(\n    rnorm(60, mean = 10, sd = 1),\n    rnorm(60, mean = 12, sd = 1)\n  )\n)\n\n# Introduce Interaction Effect\ndata_mixed$growth &lt;- with(data_mixed, growth + ifelse(fertilizer == \"F2\" & watering == \"W2\", 2, 0))\n\n# Fit Mixed-Effects Model with Interaction\nmodel_mixed &lt;- lmer(growth ~ fertilizer * watering + (1 | greenhouse), data = data_mixed)\nsummary(model_mixed)\n\n# Estimated Marginal Means and Pairwise Comparisons\nlibrary(emmeans)\nemm &lt;- emmeans(model_mixed, ~ fertilizer * watering)\npairs(emm)\nInterpretation: - Fixed Effects (fertilizer, watering, fertilizer:watering): Assess the main effects and interaction between fertilizer type and watering frequency. - Random Effects (greenhouse): Account for variability across different greenhouses. - Interaction Effect: Significant interaction indicates that the effect of one factor depends on the level of the other factor. - Pairwise Comparisons: Identify specific group differences after accounting for other factors."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#example-resting-metabolic-rate",
    "href": "lectures/week-12_mixed-models.html#example-resting-metabolic-rate",
    "title": "Mixed Models",
    "section": "Example: Resting Metabolic Rate",
    "text": "Example: Resting Metabolic Rate\n\nScenario\nA study aims to compare three different exercise protocols (Treatments A, B, C) on subjects’ resting metabolic rates. To control for age-related variability, subjects are grouped into four blocks based on age groups.\n\n\nStep-by-Step R Code\n\n1. Load and Prepare Data\n# Set seed for reproducibility\nset.seed(789)\n\n# Define blocks and treatments\nblocks &lt;- factor(rep(1:4, each = 3))\ntreatments &lt;- factor(rep(c(\"A\", \"B\", \"C\"), times = 4))\n\n# Simulate response variable (resting metabolic rate in kcal/day)\nresponse &lt;- c(\n  rnorm(3, mean = 1500, sd = 50),  # Block 1\n  rnorm(3, mean = 1550, sd = 50),  # Block 2\n  rnorm(3, mean = 1480, sd = 50),  # Block 3\n  rnorm(3, mean = 1520, sd = 50)   # Block 4\n)\n\n# Create data frame\ndata_rmr &lt;- data.frame(blocks, treatments, response)\n\n# View the data\nprint(data_rmr)\nOutput Interpretation: The dataset comprises four blocks (age groups), each containing three treatments (A, B, C). The response variable represents the resting metabolic rate, simulated with slight block-specific means to reflect age-related variability.\n\n\n2. Fit the RCBD Model\n# Fit the RCBD model using ANOVA\nmodel_rcbd &lt;- aov(response ~ blocks + treatments, data = data_rmr)\n\n# Summary of the model\nsummary(model_rcbd)\nInterpretation: - Blocks: Capture variability due to different age groups. - Treatments: Assess the effect of exercise protocols after accounting for age-related variability. - Error: Represents unexplained variability in resting metabolic rates.\n\n\n3. Diagnostic Plots\n# Diagnostic plots for the ANOVA model\npar(mfrow = c(2, 2))\nplot(model_rcbd)\nInterpretation: Examine residual plots to ensure that assumptions of normality, homoscedasticity, and independence are met. Look for random scatter in Residuals vs Fitted and Q-Q plots to assess normality.\n\n\n4. Post-Hoc Analysis (Multiple Comparisons)\n# Load emmeans package for estimated marginal means\nlibrary(emmeans)\n\n# Estimated marginal means for treatments\nemm_rmr &lt;- emmeans(model_rcbd, ~ treatments)\n\n# Pairwise comparisons\npairwise_comparisons_rmr &lt;- contrast(emm_rmr, method = \"pairwise\")\nsummary(pairwise_comparisons_rmr)\nInterpretation: Identify which exercise protocols significantly differ in their effect on resting metabolic rates after adjusting for age-related variability."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#variance-component-estimation",
    "href": "lectures/week-12_mixed-models.html#variance-component-estimation",
    "title": "Mixed Models",
    "section": "Variance Component Estimation",
    "text": "Variance Component Estimation\n\nDeriving Expected Mean Squares\nIn random-effects models, the Expected Mean Squares (EMS) are fundamental for estimating variance components.\n\n\nRandom-Effects One-Way Model\n\\[\nY_{it} = \\mu + T_i + \\varepsilon_{it}, \\quad T_i \\sim N(0, \\sigma_T^2), \\quad \\varepsilon_{it} \\sim N(0, \\sigma^2)\n\\]\nExpected Mean Squares:\n\\[\nEMS_T = \\sigma_T^2 + \\sigma^2\n\\]\n\\[\nEMS_E = \\sigma^2\n\\]\nDerivation:\n\nFor Treatments:\n\n\\[\nEMS_T = Var(T_i) + Var(\\varepsilon_{it}) = \\sigma_T^2 + \\sigma^2\n\\]\n\nFor Error:\n\n\\[\nEMS_E = Var(\\varepsilon_{it}) = \\sigma^2\n\\]\nEstimation:\nUsing ANOVA sums of squares, we set up equations based on EMS to solve for variance components.\n\\[\nMS_T = \\sigma_T^2 + \\sigma^2\n\\]\n\\[\nMS_E = \\sigma^2\n\\]\nSolving for \\(\\sigma_T^2\\):\n\\[\n\\hat{\\sigma}_T^2 = MS_T - MS_E\n\\]\nConclusion: This derivation highlights how ANOVA estimates variance components by leveraging the properties of expected mean squares.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#confidence-intervals-for-variance-components",
    "href": "lectures/week-12_mixed-models.html#confidence-intervals-for-variance-components",
    "title": "Mixed Models",
    "section": "Confidence Intervals for Variance Components",
    "text": "Confidence Intervals for Variance Components\n\nSatterthwaite Approximation\nThe Satterthwaite Approximation provides a method to estimate the degrees of freedom for variance components, facilitating the construction of confidence intervals.\n\n\nSteps\n\nEstimate Variance Components: Using REML or other estimation methods.\nCalculate Degrees of Freedom: Approximated based on the variance component estimates.\nConstruct Confidence Intervals:\n\n\\[\nCI = \\hat{\\sigma}^2 \\pm t_{\\alpha/2, df} \\times SE(\\hat{\\sigma}^2)\n\\]\nWhere:\n\n\\(\\hat{\\sigma}^2\\): Estimated variance component.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(SE(\\hat{\\sigma}^2)\\): Standard error of the variance component estimate.\n\nExample in R:\n# Fit the model\nmodel &lt;- lmer(response ~ 1 + (1 | group), data = data_random)\n\n# Extract variance components\nvar_components &lt;- VarCorr(model)\nsigma_T_sq &lt;- as.numeric(var_components$group)\nsigma_sq &lt;- attr(var_components, \"sc\")^2\n\n# Estimate standard errors using the lme4 package's built-in functions\nconfint(model, parm = \"theta_\", method = \"Wald\")\nInterpretation: The confint function provides confidence intervals for the variance components, allowing for inference about the population variability.\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#medium-difficulty",
    "href": "lectures/week-12_mixed-models.html#medium-difficulty",
    "title": "Mixed Models",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\n1. Two-Way ANCOVA with Interaction\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate \\(X\\).\nFit an ANCOVA model adjusting for \\(X\\).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\nset.seed(123)\n\n# Define factors and covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- 5 + \n     ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + \n     0.5 * (X - mean(X)) + \n     rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\nQuestions:\n\nMain Effects:\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate \\(X\\): How does \\(X\\) influence the response variable \\(Y\\)?\n\nSignificance Testing:\n\nAre the treatment effects statistically significant after adjusting for \\(X\\)?\nIs the covariate \\(X\\) a significant predictor of \\(Y\\)?\n\nDiagnostic Plots:\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\nInterpretation of Pairwise Comparisons:\n\nWhich treatments significantly differ from each other after adjusting for \\(X\\)?\nHow do the adjusted means compare across treatments?\n\n\nExpected Answers:\n\nMain Effects:\n\nTreatments B and C have positive effects relative to Treatment A.\nCovariate \\(X\\) has a positive influence on \\(Y\\), indicating that as \\(X\\) increases, \\(Y\\) tends to increase.\n\nSignificance Testing:\n\nIf the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nIf the p-value for \\(X\\) is below the significance level, \\(X\\) is a significant predictor.\n\nDiagnostic Plots:\n\nRandom scatter in Residuals vs Fitted suggests homoscedasticity and linearity.\nQ-Q plot close to the diagonal indicates normality of residuals.\nNo points with high leverage or large residuals indicate no influential observations.\n\nInterpretation of Pairwise Comparisons:\n\nSignificant pairwise differences indicate which specific treatments differ after adjusting for \\(X\\).\nAdjusted means provide a clearer comparison by accounting for the covariate’s effect."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#challenging-problems",
    "href": "lectures/week-12_mixed-models.html#challenging-problems",
    "title": "Mixed Models",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\n1. Unbalanced Two-Factor ANCOVA Analysis\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\nR Code Example:\nlibrary(car)\n\nset.seed(456)\n\n# Simulate unbalanced data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate responses with treatment and covariate effects\nY &lt;- 5 + \n     ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n     ifelse(FactorB == \"B1\", 0, 3) + \n     0.5 * (X - mean(X)) + \n     rnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)\nDiscussion Points:\n\nType II vs. Type III SS:\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\nImpact of Imbalance:\n\nImbalanced designs can lead to different interpretations under Type II and Type III SS.\nType I SS is not appropriate here as it is sequential and depends on the order of factors.\n\nSignificance of Effects:\n\nIdentify which factors and interactions are significant under each type.\nDiscuss how imbalance might inflate or deflate the significance levels of certain effects.\n\nImplications for Experimental Conclusions:\n\nChoosing the appropriate type of sums of squares is crucial for accurate inference.\nMisinterpretation can lead to incorrect conclusions about factor effects and interactions.\n\n\nExpected Insights:\n\nType II SS: Often preferred when interactions are not of primary interest. It provides unbiased estimates of main effects under the assumption of no interaction.\nType III SS: Necessary when interactions are present or when the design is unbalanced. It accounts for all factors simultaneously, providing conditional main effects.\nImbalance Effects: Can distort the Type I SS results, leading to misleading conclusions. Type II and III SS offer more robust alternatives in unbalanced settings.\n\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\n\n\n2. Three-Factor Interaction Interpretation\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\nR Code Example:\nlibrary(emmeans)\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\nY &lt;- 5 + \n     ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n     ifelse(FactorB == \"B1\", 0, 3) + \n     ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + \n     0.5 * (X - mean(X)) + \n     ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, \n            ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + \n     rnorm(72, mean=0, sd=3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\nsummary(fit_three)\n\n# Plot three-way interaction\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ FactorC) +\n  labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") +\n  theme_minimal()\n\n# Estimated marginal means\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)\nInterpretation:\n\nThree-Way Interaction:\n\nThe interaction plot demonstrates how the interaction between Factor A and Factor B varies across different levels of Factor C.\nNon-parallel lines across facets indicate a significant three-way interaction.\n\nImpact on Main and Two-Way Interactions:\n\nSignificant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation.\nThe effect of one factor depends on the combination of other factors, necessitating a comprehensive analysis.\n\nPractical Implications:\n\nUnderstanding three-way interactions provides nuanced insights into how multiple factors jointly influence the response variable.\nThis can inform more sophisticated decision-making and strategy development in experimental settings.\n\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer.\n\n\n3. Mathematical Proof of Interaction Contrasts\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations.\n\nProof Outline:\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)_{ik}\\) and \\((\\beta\\gamma)_{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)_{ij} = (\\alpha\\gamma)_{ik} = (\\beta\\gamma)_{jk} = (\\alpha\\beta\\gamma)_{ijk} = 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model: If any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\n\nConclusion:\nInteraction contrasts (\\((\\alpha\\beta)_{ij}\\), \\((\\alpha\\gamma)_{ik}\\), \\((\\beta\\gamma)_{jk}\\), and \\((\\alpha\\beta\\gamma)_{ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#variance-calculations-for-latin-square-designs",
    "href": "lectures/week-12_mixed-models.html#variance-calculations-for-latin-square-designs",
    "title": "Mixed Models",
    "section": "Variance Calculations for Latin Square Designs",
    "text": "Variance Calculations for Latin Square Designs\nIn a Latin Square Design with \\(v\\) treatments, the variance of the estimated treatment effects is influenced by the design’s structure.\n\nFormula\nFor a Latin Square with \\(v\\) treatments:\n\\[\nVar(\\tau_i) = \\frac{\\sigma^2}{v}\n\\]\n\n\nDerivation\n\nModel Setup:\nThe ANOVA model for a Latin Square is:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\tau_k + \\epsilon_{ijk},\n\\]\nwhere \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\).\n\nEstimator for Treatment Effect (\\(\\tau_k\\)):\nThe treatment effects are estimated as:\n\n\\[\n\\hat{\\tau}_k = \\bar{Y}_{\\cdot \\cdot k} - \\bar{Y}_{\\cdot \\cdot \\cdot},\n\\]\nwhere \\(\\bar{Y}_{\\cdot \\cdot k}\\) is the mean response for treatment \\(k\\) and \\(\\bar{Y}_{\\cdot \\cdot \\cdot}\\) is the grand mean.\n\nVariance of \\(\\hat{\\tau}_k\\):\nSince each treatment is replicated \\(v\\) times (once per row and column):\n\n\\[\nVar(\\hat{\\tau}_k) = \\frac{\\sigma^2}{v}\n\\]\nConclusion: The variance of the treatment effect estimates decreases with increasing number of treatments in the Latin Square, enhancing the precision of the estimates.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#degrees-of-freedom-in-youden-designs",
    "href": "lectures/week-12_mixed-models.html#degrees-of-freedom-in-youden-designs",
    "title": "Mixed Models",
    "section": "Degrees of Freedom in Youden Designs",
    "text": "Degrees of Freedom in Youden Designs\nFor a Youden Design with parameters \\(v = 4\\) treatments, \\(b = 4\\) blocks (rows), and \\(c = 3\\) columns, the degrees of freedom for the error term are calculated as follows:\n\nFormula\n\\[\ndf_{\\text{Error}} = bc - b - c - v + 2\n\\]\n\n\nCalculation\nGiven:\n\n\\(v = 4\\)\n\\(b = 4\\)\n\\(c = 3\\)\n\nPlugging into the formula:\n\\[\ndf_{\\text{Error}} = (4 \\times 3) - 4 - 3 - 4 + 2 = 12 - 4 - 3 - 4 + 2 = 3\n\\]\nInterpretation: The error degrees of freedom indicate the number of independent pieces of information available to estimate the error variance. In this case, with 3 error degrees of freedom, the design is sufficiently constrained to estimate treatment effects after accounting for row and column effects.\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#proof-of-interaction-contrasts",
    "href": "lectures/week-12_mixed-models.html#proof-of-interaction-contrasts",
    "title": "Mixed Models",
    "section": "Proof of Interaction Contrasts",
    "text": "Proof of Interaction Contrasts\n\nInteraction Contrasts Measure Deviations from Additivity\nDefinition: In a three-factor design, interaction contrasts quantify how the combined effect of three factors deviates from the additive effects of each factor individually and their two-way interactions.\n\n\nProof Steps\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)_{ik}\\) and \\((\\beta\\gamma)_{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)_{ij} = (\\alpha\\gamma)_{ik} = (\\beta\\gamma)_{jk} = (\\alpha\\beta\\gamma)_{ijk} = 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model: If any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\n\nConclusion:\nInteraction contrasts (\\((\\alpha\\beta)_{ij}\\), \\((\\alpha\\gamma)_{ik}\\), \\((\\beta\\gamma)_{jk}\\), and \\((\\alpha\\beta\\gamma)_{ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#adjusted-treatment-means",
    "href": "lectures/week-12_mixed-models.html#adjusted-treatment-means",
    "title": "Mixed Models",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe Adjusted Mean for treatment \\(i\\) accounts for the influence of blocking factors, providing a more accurate estimate of the treatment effect.\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment \\(i\\).\n\\(\\bar{Y}_i\\): Mean response for treatment \\(i\\).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment \\(i\\).\n\\(\\bar{X}\\): Overall mean of the covariate.\n\n\nDerivation\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#f-test-for-homogeneity-of-slopes",
    "href": "lectures/week-12_mixed-models.html#f-test-for-homogeneity-of-slopes",
    "title": "Mixed Models",
    "section": "F-Test for Homogeneity of Slopes",
    "text": "F-Test for Homogeneity of Slopes\nTesting the assumption of homogeneity of regression slopes ensures that the relationship between the covariate and the response is consistent across all treatment groups.\n\nModel Comparison\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + \\epsilon_{hi}\n\\]\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + (\\theta \\tau)_{hi} + \\epsilon_{hi}\n\\]\nWhere \\((\\theta \\tau)_{hi}\\) allows the slope of the covariate to vary with treatments.\n\n\nHypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): Homogeneity of slopes (\\(\\beta\\) is the same across treatments).\nAlternative Hypothesis (\\(H_A\\)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta_j\\) for some \\(i, j\\)).\n\n\n\nTest Procedure\n\nFit Both Models:\n\nReduced Model: Without interaction.\nFull Model: With interaction.\n\nCompare Models Using ANOVA:\n# Fit reduced model\nmodel_reduced &lt;- lm(Time ~ fColor + x, data = balloon.data)\n\n# Fit full model with interaction\nmodel_full &lt;- lm(Time ~ fColor * x, data = balloon.data)\n\n# Compare models\nanova(model_reduced, model_full)\nDecision Rule:\n\nIf the interaction term is significant (\\(p &lt; \\alpha\\)), reject \\(H_0\\) and conclude that slopes are not homogeneous.\nIf not significant, accept \\(H_0\\) and proceed with the reduced model.\n\n\nConclusion: A significant interaction term indicates that the effect of the covariate varies across treatments, violating the homogeneity of slopes assumption and necessitating a more complex model.\nReference: Scheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-12_mixed-models.html#confidence-intervals-for-higher-order-contrasts",
    "title": "Mixed Models",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts provide insights into the magnitude and significance of these interactions.\n\nFormula\n\\[\nCI = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\n\n\nSteps in R\n\nEstimate the Contrast:\nUsing the emmeans package to define and estimate contrasts.\nCalculate the Confidence Interval:\nThe emmeans package automatically provides confidence intervals when performing contrasts.\n\nExample:\nlibrary(emmeans)\n\n# Fit the ANCOVA model\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define estimated marginal means\nemm &lt;- emmeans(model, ~ Treatment)\n\n# Define a contrast (e.g., Treatment B vs. Treatment A)\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\nsummary(contrast_AB, infer = TRUE)\nInterpretation: The output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#medium-difficulty-1",
    "href": "lectures/week-12_mixed-models.html#medium-difficulty-1",
    "title": "Mixed Models",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\n1. Two-Way ANCOVA with Interaction\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate \\(X\\).\nFit an ANCOVA model adjusting for \\(X\\).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\nset.seed(123)\n\n# Define factors and covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- 5 + \n     ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + \n     0.5 * (X - mean(X)) + \n     rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\nQuestions:\n\nMain Effects:\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate \\(X\\): How does \\(X\\) influence the response variable \\(Y\\)?\n\nSignificance Testing:\n\nAre the treatment effects statistically significant after adjusting for \\(X\\)?\nIs the covariate \\(X\\) a significant predictor of \\(Y\\)?\n\nDiagnostic Plots:\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\nInterpretation of Pairwise Comparisons:\n\nWhich treatments significantly differ from each other after adjusting for \\(X\\)?\nHow do the adjusted means compare across treatments?\n\n\nExpected Answers:\n\nMain Effects:\n\nTreatments B and C have positive effects relative to Treatment A.\nCovariate \\(X\\) has a positive influence on \\(Y\\), indicating that as \\(X\\) increases, \\(Y\\) tends to increase.\n\nSignificance Testing:\n\nIf the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nIf the p-value for \\(X\\) is below the significance level, \\(X\\) is a significant predictor.\n\nDiagnostic Plots:\n\nRandom scatter in Residuals vs Fitted suggests homoscedasticity and linearity.\nQ-Q plot close to the diagonal indicates normality of residuals.\nNo points with high leverage or large residuals indicate no influential observations.\n\nInterpretation of Pairwise Comparisons:\n\nSignificant pairwise differences indicate which specific treatments differ after adjusting for \\(X\\).\nAdjusted means provide a clearer comparison by accounting for the covariate’s effect."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#challenging-problems-1",
    "href": "lectures/week-12_mixed-models.html#challenging-problems-1",
    "title": "Mixed Models",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\n1. Unbalanced Two-Factor ANCOVA Analysis\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\nR Code Example:\nlibrary(car)\n\nset.seed(456)\n\n# Simulate unbalanced data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate responses with treatment and covariate effects\nY &lt;- 5 + \n     ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n     ifelse(FactorB == \"B1\", 0, 3) + \n     0.5 * (X - mean(X)) + \n     rnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)\nDiscussion Points:\n\nType II vs. Type III SS:\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\nImpact of Imbalance:\n\nImbalanced designs can lead to different interpretations under Type II and Type III SS.\nType I SS is not appropriate here as it is sequential and depends on the order of factors.\n\nSignificance of Effects:\n\nIdentify which factors and interactions are significant under each type.\nDiscuss how imbalance might inflate or deflate the significance levels of certain effects.\n\nImplications for Experimental Conclusions:\n\nChoosing the appropriate type of sums of squares is crucial for accurate inference.\nMisinterpretation can lead to incorrect conclusions about factor effects and interactions.\n\n\nExpected Insights:\n\nType II SS: Often preferred when interactions are not of primary interest. It provides unbiased estimates of main effects under the assumption of no interaction.\nType III SS: Necessary when interactions are present or when the design is unbalanced. It accounts for all factors simultaneously, providing conditional main effects.\nImbalance Effects: Can distort the Type I SS results, leading to misleading conclusions. Type II and III SS offer more robust alternatives in unbalanced settings.\n\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\n\n\n2. Three-Factor Interaction Interpretation\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\nR Code Example:\nlibrary(emmeans)\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\nY &lt;- 5 + \n     ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n     ifelse(FactorB == \"B1\", 0, 3) + \n     ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + \n     0.5 * (X - mean(X)) + \n     ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, \n            ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + \n     rnorm(72, mean=0, sd=3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\nsummary(fit_three)\n\n# Plot three-way interaction\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ FactorC) +\n  labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") +\n  theme_minimal()\n\n# Estimated marginal means\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)\nInterpretation:\n\nThree-Way Interaction:\n\nThe interaction plot demonstrates how the interaction between Factor A and Factor B varies across different levels of Factor C.\nNon-parallel lines across facets indicate a significant three-way interaction.\n\nImpact on Main and Two-Way Interactions:\n\nSignificant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation.\nThe effect of one factor depends on the combination of other factors, necessitating a comprehensive analysis.\n\nPractical Implications:\n\nUnderstanding three-way interactions provides nuanced insights into how multiple factors jointly influence the response variable.\nThis can inform more sophisticated decision-making and strategy development in experimental settings.\n\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer.\n\n\n3. Mathematical Proof of Interaction Contrasts\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations.\n\nProof Outline:\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)_{ik}\\) and \\((\\beta\\gamma)_{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)_{ij} = (\\alpha\\gamma)_{ik} = (\\beta\\gamma)_{jk} = (\\alpha\\beta\\gamma)_{ijk} = 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model: If any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\n\nConclusion:\nInteraction contrasts (\\((\\alpha\\beta)_{ij}\\), \\((\\alpha\\gamma)_{ik}\\), \\((\\beta\\gamma)_{jk}\\), and \\((\\alpha\\beta\\gamma)_{ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#adjusted-treatment-means-1",
    "href": "lectures/week-12_mixed-models.html#adjusted-treatment-means-1",
    "title": "Mixed Models",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe Adjusted Mean for treatment \\(i\\) accounts for the influence of blocking factors, providing a more accurate estimate of the treatment effect.\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment \\(i\\).\n\\(\\bar{Y}_i\\): Mean response for treatment \\(i\\).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment \\(i\\).\n\\(\\bar{X}\\): Overall mean of the covariate.\n\n\nDerivation\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#f-test-for-homogeneity-of-slopes-1",
    "href": "lectures/week-12_mixed-models.html#f-test-for-homogeneity-of-slopes-1",
    "title": "Mixed Models",
    "section": "F-Test for Homogeneity of Slopes",
    "text": "F-Test for Homogeneity of Slopes\nTesting the assumption of homogeneity of regression slopes ensures that the relationship between the covariate and the response is consistent across all treatment groups.\n\nModel Comparison\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + \\epsilon_{hi}\n\\]\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + (\\theta \\tau)_{hi} + \\epsilon_{hi}\n\\]\nWhere \\((\\theta \\tau)_{hi}\\) allows the slope of the covariate to vary with treatments.\n\n\nHypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): Homogeneity of slopes (\\(\\beta\\) is the same across treatments).\nAlternative Hypothesis (\\(H_A\\)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta_j\\) for some \\(i, j\\)).\n\n\n\nTest Procedure\n\nFit Both Models:\n\nReduced Model: Without interaction.\nFull Model: With interaction.\n\nCompare Models Using ANOVA:\n# Fit reduced model\nmodel_reduced &lt;- lm(Time ~ fColor + x, data = balloon.data)\n\n# Fit full model with interaction\nmodel_full &lt;- lm(Time ~ fColor * x, data = balloon.data)\n\n# Compare models\nanova(model_reduced, model_full)\nDecision Rule:\n\nIf the interaction term is significant (\\(p &lt; \\alpha\\)), reject \\(H_0\\) and conclude that slopes are not homogeneous.\nIf not significant, accept \\(H_0\\) and proceed with the reduced model.\n\n\nConclusion: A significant interaction term indicates that the effect of the covariate varies across treatments, violating the homogeneity of slopes assumption and necessitating a more complex model.\nReference: Scheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#confidence-intervals-for-higher-order-contrasts-1",
    "href": "lectures/week-12_mixed-models.html#confidence-intervals-for-higher-order-contrasts-1",
    "title": "Mixed Models",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts provide insights into the magnitude and significance of these interactions.\n\nFormula\n\\[\nCI = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\n\n\nSteps in R\n\nEstimate the Contrast:\nUsing the emmeans package to define and estimate contrasts.\nCalculate the Confidence Interval:\nThe emmeans package automatically provides confidence intervals when performing contrasts.\n\nExample:\nlibrary(emmeans)\n\n# Fit the ANCOVA model\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define estimated marginal means\nemm &lt;- emmeans(model, ~ Treatment)\n\n# Define a contrast (e.g., Treatment B vs. Treatment A)\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\nsummary(contrast_AB, infer = TRUE)\nInterpretation: The output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-14_split-plot.html",
    "href": "lectures/week-14_split-plot.html",
    "title": "Split-Plot ANOVA",
    "section": "",
    "text": "Split-plot designs are a powerful class of experimental designs that originated in agricultural settings but now see extensive use across engineering and scientific fields where certain factors are hard to change and others are easy to change. First formalized by R. A. Fisher at Rothamsted Experimental Station, these designs address practical limitations in experimentation—for instance, when adjusting equipment temperature is costly or time-consuming (Kempthorne, 1977; Scheffé, 1959).\n\n\n\nEarly Development: Split-plot methodology emerged from agricultural research, where entire fields (whole plots) were assigned to certain treatments (e.g., irrigation methods), and sub-portions of each field (split plots) received additional treatments (e.g., fertilizer types).\nModern Applications: Today, split-plot designs are prevalent in industrial experiments (Dean et al., 2017), such as semiconductor manufacturing or paint-curing processes. They enable investigators to manipulate “hard-to-change” variables at the whole-plot level while fine-tuning other variables at the split-plot level.\n\n\n\n\nAfter working through this material, you will be able to:\n\nRecognize when split-plot designs are appropriate.\nFormulate the statistical models underpinning split-plot experiments.\nAnalyze split-plot data using R and interpret the output.\nUnderstand the hierarchical (two-error) structure inherent in split-plot designs.\nConduct diagnostic checks and manage potential pitfalls such as unbalanced data or pseudo-replication."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#historical-context",
    "href": "lectures/week-14_split-plot.html#historical-context",
    "title": "Split-Plot ANOVA",
    "section": "",
    "text": "Early Development: Split-plot methodology emerged from agricultural research, where entire fields (whole plots) were assigned to certain treatments (e.g., irrigation methods), and sub-portions of each field (split plots) received additional treatments (e.g., fertilizer types).\nModern Applications: Today, split-plot designs are prevalent in industrial experiments (Dean et al., 2017), such as semiconductor manufacturing or paint-curing processes. They enable investigators to manipulate “hard-to-change” variables at the whole-plot level while fine-tuning other variables at the split-plot level."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#learning-objectives",
    "href": "lectures/week-14_split-plot.html#learning-objectives",
    "title": "Split-Plot ANOVA",
    "section": "",
    "text": "After working through this material, you will be able to:\n\nRecognize when split-plot designs are appropriate.\nFormulate the statistical models underpinning split-plot experiments.\nAnalyze split-plot data using R and interpret the output.\nUnderstand the hierarchical (two-error) structure inherent in split-plot designs.\nConduct diagnostic checks and manage potential pitfalls such as unbalanced data or pseudo-replication."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#two-error-structure",
    "href": "lectures/week-14_split-plot.html#two-error-structure",
    "title": "Split-Plot ANOVA",
    "section": "2.1 Two-Error Structure",
    "text": "2.1 Two-Error Structure\nA defining feature of split-plot designs is the presence of two distinct experimental error terms (Dean et al., 2017; Montgomery & Runger, 2010; Christensen, 2018):\n\nWhole-Plot Error (\\(\\sigma^2_W\\))\n\nArises from variation among the “hard-to-change” factors (e.g., temperature, irrigation).\nTypically larger due to the cost and complexity of changing these factors.\nExample: Oven-to-oven variability when testing different oven temperatures.\n\nSplit-Plot Error (\\(\\sigma^2_S\\))\n\nCaptures variability within each whole plot for the “easy-to-change” factors.\nUsually smaller since these factors are more straightforward to manipulate.\nExample: Variation among parts painted in the same oven run.\n\n\nThis nested structure means that each whole plot is subdivided into several split plots, and different analysis methods are needed to properly separate the two sources of variability."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#intuitive-analogy-paint-curing-process",
    "href": "lectures/week-14_split-plot.html#intuitive-analogy-paint-curing-process",
    "title": "Split-Plot ANOVA",
    "section": "2.2 Intuitive Analogy: Paint Curing Process",
    "text": "2.2 Intuitive Analogy: Paint Curing Process\nConsider an industrial paint-curing process:\n\nWhole-Plot Factor: Oven Temperature (Low, Medium, High)\nSplit-Plot Factor: Paint Type (A, B, C)\nResponse: Coating Durability\n\nRunning the experiment:\n\nSet the oven to a specific temperature (hard to change).\nPaint several parts (easy to change) in that oven run.\nMeasure the durability of each part’s coating.\n\nBecause the oven temperature remains fixed for a batch of painted parts, any variation among parts at the same temperature is captured by \\(\\sigma^2_S\\) (split-plot error), whereas variation across different oven temperatures is captured by \\(\\sigma^2_W\\) (whole-plot error)."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#manufacturing-example",
    "href": "lectures/week-14_split-plot.html#manufacturing-example",
    "title": "Split-Plot ANOVA",
    "section": "6.1 Manufacturing Example",
    "text": "6.1 Manufacturing Example\nSuppose you have:\n\nWhole-Plot Factor: Furnace Temperature (\\(800^\\circ\\)C vs. \\(900^\\circ\\)C)\nSplit-Plot Factor: Gas Flow Rate (10, 20, 30 sccm)\nBlocks: 4 furnace runs\nResponse: Film Thickness Uniformity\n\n\nset.seed(123)\nsemi_data &lt;- data.frame(\n  run = rep(1:4, each = 6),\n  temp = rep(rep(c(\"800C\", \"900C\"), each = 3), 4),\n  flow = rep(c(\"10\", \"20\", \"30\"), times = 8),\n  uniformity = rnorm(24,\n    mean = rep(c(85, 90), each = 12) +\n      rep(c(0, 2, 4), times = 8),\n    sd = 2\n  )\n)\n\nsemi_model &lt;- lmer(uniformity ~ temp * flow + (1 | run) + (1 | run:temp),\n  data = semi_data\n)\n\nsummary(semi_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: uniformity ~ temp * flow + (1 | run) + (1 | run:temp)\n   Data: semi_data\n\nREML criterion at convergence: 92.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.61892 -0.60661  0.00919  0.54843  1.50653 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n run:temp (Intercept) 0.000    0.000   \n run      (Intercept) 5.452    2.335   \n Residual             4.459    2.112   \nNumber of obs: 24, groups:  run:temp, 8; run, 4\n\nFixed effects:\n                Estimate Std. Error t value\n(Intercept)     88.00128    1.57408  55.906\ntemp900C         0.09561    1.49316   0.064\nflow20           0.57004    1.49316   0.382\nflow30           3.12281    1.49316   2.091\ntemp900C:flow20  1.24567    2.11164   0.590\ntemp900C:flow30 -0.03002    2.11164  -0.014\n\nCorrelation of Fixed Effects:\n            (Intr) tm900C flow20 flow30 t900C:2\ntemp900C    -0.474                             \nflow20      -0.474  0.500                      \nflow30      -0.474  0.500  0.500               \ntmp900C:f20  0.335 -0.707 -0.707 -0.354        \ntmp900C:f30  0.335 -0.707 -0.354 -0.707  0.500 \noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nanova(semi_model)\n\n\n\n\n\n\nnpar\nSum Sq\nMean Sq\nF value\n\n\n\n\ntemp\n1\n1.504964\n1.504964\n0.3375085\n\n\nflow\n2\n39.328867\n19.664433\n4.4100163\n\n\ntemp:flow\n2\n2.119994\n1.059997\n0.2377188\n\n\n\n\n\n\n\nPlotting:\n\n\nlibrary(ggplot2)\nggplot(semi_data, aes(x = flow, y = uniformity, color = temp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~run) +\n  theme_bw() +\n  labs(\n    title = \"Film Uniformity vs. Flow Rate by Temperature\",\n    x = \"Flow Rate (sccm)\",\n    y = \"Uniformity (%)\"\n  )"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#agricultural-example",
    "href": "lectures/week-14_split-plot.html#agricultural-example",
    "title": "Split-Plot ANOVA",
    "section": "6.2 Agricultural Example",
    "text": "6.2 Agricultural Example\n\nWhole-Plot Factor: Irrigation Method (Drip, Sprinkler)\nSplit-Plot Factor: Fertilizer (None, Organic, Chemical)\nBlocks: 6 fields\nResponse: Crop Yield\n\n\nset.seed(456)\nag_data &lt;- data.frame(\n  field = rep(1:6, each = 6),\n  irrigation = rep(rep(c(\"Drip\", \"Sprinkler\"), each = 3), 6),\n  fertilizer = rep(c(\"None\", \"Organic\", \"Chemical\"), times = 12),\n  yield = rnorm(36,\n    mean = rep(c(40, 45), each = 18) +\n      rep(c(0, 5, 8), times = 12),\n    sd = 3\n  )\n)\n\nag_model &lt;- lmer(\n  yield ~ irrigation * fertilizer +\n    (1 | field) + (1 | field:irrigation),\n  data = ag_data\n)\n\nanova(ag_model)\n\n\n\n\n\n\nnpar\nSum Sq\nMean Sq\nF value\n\n\n\n\nirrigation\n1\n0.4184505\n0.4184505\n0.0422611\n\n\nfertilizer\n2\n379.3214623\n189.6607311\n19.1546586\n\n\nirrigation:fertilizer\n2\n35.7991513\n17.8995757\n1.8077557\n\n\n\n\n\nemmeans(ag_model, ~ irrigation | fertilizer)\n\nfertilizer = Chemical:\n irrigation emmean  SE df lower.CL upper.CL\n Drip         49.2 1.7 21     45.7     52.8\n Sprinkler    51.7 1.7 21     48.1     55.2\n\nfertilizer = None:\n irrigation emmean  SE df lower.CL upper.CL\n Drip         43.5 1.7 21     40.0     47.1\n Sprinkler    42.2 1.7 21     38.7     45.7\n\nfertilizer = Organic:\n irrigation emmean  SE df lower.CL upper.CL\n Drip         49.8 1.7 21     46.3     53.3\n Sprinkler    47.7 1.7 21     44.1     51.2\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#power-analysis",
    "href": "lectures/week-14_split-plot.html#power-analysis",
    "title": "Split-Plot ANOVA",
    "section": "7.1 Power Analysis",
    "text": "7.1 Power Analysis\nIn split-plot designs, we have distinct power calculations for the whole-plot factor and the split-plot factor (Dean et al., 2017). Each factor is tested against a different error term.\n\\[\n\\text{Power}^*_{\\text{whole-plot}} \\quad=\\quad P\\bigl(F^*_{a-1,(b-1)(a-1)} &gt; F_{\\alpha;a-1,(b-1)(a-1)} \\;\\bigm|\\; \\delta\\bigr),\n\\]\n\\[\n\\text{Power}^*_{\\text{split-plot}} \\quad=\\quad P\\bigl(F^*_{s-1,a(b-1)(s-1)} &gt; F_{\\alpha;s-1,a(b-1)(s-1)} \\;\\bigm|\\; \\delta\\bigr).\n\\]\n\n\\(\\delta\\) represents the noncentrality parameter linked to the effect size.\nSoftware such as pwr, simr, or custom code can be used to approximate power."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#optimal-design-considerations",
    "href": "lectures/week-14_split-plot.html#optimal-design-considerations",
    "title": "Split-Plot ANOVA",
    "section": "7.2 Optimal Design Considerations",
    "text": "7.2 Optimal Design Considerations\nFor large-scale experiments, cost constraints and logistical feasibility often dictate the chosen design structure (Kempthorne, 1977; Dean et al., 2017). Key elements include:\n\nBalance: Achieving uniform replication across factor levels.\nResource Allocation: The ratio of whole-plot replicates to split-plot replicates.\nVariance Component Estimates: Anticipating or pilot-testing \\(\\sigma_W^2\\) and \\(\\sigma_S^2\\) to inform design decisions."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#pseudo-replication",
    "href": "lectures/week-14_split-plot.html#pseudo-replication",
    "title": "Split-Plot ANOVA",
    "section": "8.1 Pseudo-Replication",
    "text": "8.1 Pseudo-Replication\nA critical error is to treat measurements within a single whole plot as fully independent replicates for the whole-plot factor. This leads to inflated Type I error rates and overly optimistic standard errors.\nIncorrect:\n# Failing to account for hierarchical structure\nlm_model &lt;- lm(yield ~ temp * flow, data = semi_data)\nCorrect:\n# Using a mixed-effects model to capture nested variability\nlmer_model &lt;- lmer(uniformity ~ temp * flow + (1|run) + (1|run:temp),\n                   data = semi_data)"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#unbalanced-data",
    "href": "lectures/week-14_split-plot.html#unbalanced-data",
    "title": "Split-Plot ANOVA",
    "section": "8.2 Unbalanced Data",
    "text": "8.2 Unbalanced Data\nReal-world settings often produce unbalanced data (missing cells, unequal replicates):\n\nUse lmerTest for Satterthwaite or Kenward-Roger df approximations.\nExamine residuals and leverage robust inference methods if necessary.\n\n\nlibrary(lmerTest)\nunbalanced_data &lt;- subset(semi_data, !(run == 4 & flow == \"10\")) # remove some observations\nmodel_unbalanced &lt;- lmer(\n  uniformity ~ temp * flow +\n    (1 | run) + (1 | run:temp),\n  data = unbalanced_data\n)\nanova(model_unbalanced, ddf = \"Satterthwaite\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSum Sq\nMean Sq\nNumDF\nDenDF\nF value\nPr(&gt;F)\n\n\n\n\ntemp\n2.766374\n2.7663745\n1\n13.08924\n0.5870812\n0.4571464\n\n\nflow\n43.247929\n21.6239644\n2\n13.28133\n4.5890470\n0.0305412\n\n\ntemp:flow\n1.628877\n0.8144384\n2\n13.08924\n0.1728405\n0.8431608"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#medium-difficulty-problems",
    "href": "lectures/week-14_split-plot.html#medium-difficulty-problems",
    "title": "Split-Plot ANOVA",
    "section": "9.1 Medium-Difficulty Problems",
    "text": "9.1 Medium-Difficulty Problems\n\nSplit-Plot Degrees of Freedom\n\nA two-factor split-plot experiment has 3 whole-plot factor levels, 4 split-plot factor levels, and 3 blocks.\n\nCalculate the df for the whole-plot factor.\nCalculate the df for the split-plot error.\nExplain why whole-plot and split-plot errors differ.\n\n\nModel Formulation\n\nWrite down the full split-plot model with two factors (A, B) and one blocking factor (r blocks). Identify which terms belong to the whole-plot portion vs. the split-plot portion.\n\nInterpreting lmer Output\n\nFit a split-plot model using the ag_data dataset.\nReport which random effects are estimated and how they impact the F-tests for the main factors."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#challenging-problems",
    "href": "lectures/week-14_split-plot.html#challenging-problems",
    "title": "Split-Plot ANOVA",
    "section": "9.2 Challenging Problems",
    "text": "9.2 Challenging Problems\n\nDeriving Expected Mean Squares\n\nTask: Show that \\(E(MS_{whole}) = \\sigma^2_S + s\\sigma^2_W\\).\nHint: Partition the sum of squares for the whole plot and use properties of the normal distribution.\nReference: See derivations in Scheffé (1959) and Appendix A below.\n\nPower Calculation\n\nSuppose \\(\\sigma_W^2 = 15\\) and \\(\\sigma_S^2 = 5\\). You have 3 levels of a whole-plot factor (A) and 4 levels of a split-plot factor (B), with 3 replicates (blocks).\nTask: Outline how you would simulate data in R to estimate the power to detect a particular effect size \\(\\delta\\) on factor A.\n\nDesign Optimization\n\nYou have a cost function \\(C = c_w n_w + c_s n_s\\), and you want to keep the variance of \\(\\hat{\\tau}*i - \\hat{\\tau}*{i'}\\) below a threshold \\(V_0\\). Show how to derive the ratio \\(n_w^* / n_s^*\\) that minimizes cost subject to the variance constraint (Kempthorne, 1977)."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#a.1-expected-mean-squares",
    "href": "lectures/week-14_split-plot.html#a.1-expected-mean-squares",
    "title": "Split-Plot ANOVA",
    "section": "A.1 Expected Mean Squares",
    "text": "A.1 Expected Mean Squares\nUsing the notation in Dean et al. (2017) and Scheffé (1959):\n\\[\n\\text{For a balanced split-plot design:}\n\\]\n\nWhole-plot error:\n\n\\[\nE(MS_{\\text{whole}}) = \\sigma^2_S + s\\,\\sigma^2_W.\n\\]\n\nSplit-plot error:\n\n\\[\nE(MS_{\\text{split}}) = \\sigma^2_S.\n\\]\nThe derivation partitions sums of squares and leverages properties of normal distributions (see Christensen, 2018, for a modern ANOVA treatment)."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#a.2-power-analysis-for-whole-plot-factor",
    "href": "lectures/week-14_split-plot.html#a.2-power-analysis-for-whole-plot-factor",
    "title": "Split-Plot ANOVA",
    "section": "A.2 Power Analysis for Whole-Plot Factor",
    "text": "A.2 Power Analysis for Whole-Plot Factor\n\\[\n\\lambda \\;=\\; \\frac{n_w \\sum_{i=1}^a \\tau_i^2}{a\\,\\sigma_W^2},\n\\]\nwhere \\(n_w\\) is the number of whole-plot replicates, \\(\\tau_i\\) are the treatment contrasts, and \\(\\sigma_W^2\\) is the whole-plot variance. The power to detect a nonzero \\(\\tau_i\\) at significance level \\(\\alpha\\) becomes:\n\\[\n1 - \\beta \\;=\\; P\\!\\bigl(F_{a-1,(b-1)(a-1)} &gt; F_{\\alpha;a-1,(b-1)(a-1)} \\;\\bigm|\\; \\lambda\\bigr).\n\\]"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#a.3-optimal-design-theory",
    "href": "lectures/week-14_split-plot.html#a.3-optimal-design-theory",
    "title": "Split-Plot ANOVA",
    "section": "A.3 Optimal Design Theory",
    "text": "A.3 Optimal Design Theory\nFollowing Kempthorne (1977):\n\\[\n\\text{Minimize } C = c_w n_w + c_s n_s \\quad \\text{subject to }\nVar(\\hat{\\tau}^*_i - \\hat{\\tau}^*_{i'}) \\leq V_0.\n\\]\nIf \\(\\sigma_W^2\\) and \\(\\sigma_S^2\\) are known (or estimated from pilot data):\n\\[\nn_w^* = \\sqrt{\\frac{c_s}{c_w}} \\;\\frac{\\sigma_W}{\\sigma_S}\\;n_s^*.\n\\]\nThis balance ensures cost efficiency while meeting variance constraints."
  },
  {
    "objectID": "r-code-data-book.html",
    "href": "r-code-data-book.html",
    "title": "R Code and Data Files from the Textbook",
    "section": "",
    "text": "R Code and Data Files from the Textbook\nI Created the following table lists the R program files used in the textbook, along with the corresponding data files. Here is a link to the orginal textbook website for more information.\n\n\n\nChapter\nExperiment\nR Code File\nData File\nData Set Description\nTable/Details\nPage(s)\nSection\n\n\n\n\nChap. 3\nRandomization Procedure\nrandomize.r\n-\nRandomization demonstration data\n(in text)\npp. 59-60\nSect. 3.9.1\n\n\n\nSoap\nsoap.r\nsoap.txt\nSoap-making process data\nTables 3.10, 3.11\npp. 60, 63\nSects. 3.9.2, 3.9.3\n\n\nChap. 4\nBattery Experiment\nbattery.r\nbattery.txt\nBattery life experiment data\nTable 4.2\np. 97\nSect. 4.7\n\n\nChap. 5\nMung Bean Experiment\nmungbean.r\nmungbean.txt\nMung bean growth experiment data\nTable 5.11\np. 126\nSect. 5.9.1\n\n\n\n\nmungbean2.r\nmungbean.txt\nMung bean growth experiment (extended)\nTable 5.12\np. 128\nSect. 5.9.2\n\n\nChap. 6\nReaction Time\nreactiontime.r\nreaction.time.txt\nHuman reaction time data\nTable 6.15\np. 185\nSect. 6.9\n\n\nChap. 7\nDrill Advance\ndrilladvance.r\ndrill.advance.txt\nDrill efficiency data\nTable 7.12\np. 232\nSect. 7.7.1\n\n\nChap. 8\nBean Soaking\nbeansoaking.r\nbean.txt\nBean soaking time and weight data\nTable 8.10\np. 278\nSect. 8.10\n\n\nChap. 9\nBalloon Experiment\nballoon.r\nballoon.txt\nBalloon pressure and volume data\nTable 9.6\np. 300\nSect. 9.7\n\n\nChap. 10\nCotton Spinning\ncottonspinning.r\ncotton.spinning.txt\nCotton spinning machine performance data\nTables 10.14-17\npp. 332-336\nSect. 10.10\n\n\nChap. 12\nExercise Bicycle\nexercisebicycle.r\nexercise.bicycle.txt\nExercise bicycle performance data\nTable 12.12\np. 420\nSect. 12.9\n\n\nChap. 13\nCoil\ncoil.r\ncoil.txt\nElectrical coil properties data\nTable 13.21\np. 463\nSect. 13.12\n\n\nChap. 14\nDye Experiment\ndye.r\ndye2.txt\nDye color intensity experiment data\nTable 14.17\np. 490\nSect. 14.6\n\n\nChap. 15\nSludge\nsludge.r\nsludge.txt\nSludge sample chemical analysis data\nTables 15.45-46\npp. 544-545\nSect. 15.10.1\n\n\nChap. 16\nAcid Copper Pattern Plating\ncopper.r\ncopper.txt\nCopper plating pattern performance data\nTables 16.17-18\npp. 602-603\nSect. 16.8.1\n\n\nChap. 17\nClean Wool\ncleanwool.r\nclean.wool.txt\nWool cleaning efficiency data\nTable 17.15\np. 661\nSect. 17.11.1\n\n\nChap. 18\nVoltage\nvoltage.r\nvoltage.txt\nVoltage readings from electrical systems\nTable 18.8\np. 693\nSect. 18.6.2\n\n\nChap. 19\nOats\noats.r\noats.txt\nOats growth under varying conditions\nTables 19.22-23\npp. 746-747\nSect. 19.9.1",
    "crumbs": [
      "Home",
      "R Code and Datasets",
      "R Code and Datasets from the Book"
    ]
  },
  {
    "objectID": "syllabus/Lesson_plan.html",
    "href": "syllabus/Lesson_plan.html",
    "title": "Lesson Plan",
    "section": "",
    "text": "Purpose and scope of experimental design\nObservational studies vs. designed experiments\nKey principles: randomization, replication, and control\n\n\n\n\n\nDifferentiate between observational studies and experiments.\nExplain the three fundamental principles of experimental design.\nRecognize scenarios where designed experiments are beneficial.\n\n\n\n\n\nLecture: Introduction to experimentation (Dean et al., Ch. 1, Sec. 1.1–1.1.1; Ch. 2, Sec. 2.1–2.2).\nCase Study Discussion: The role of experimental design in manufacturing and biomedical research.\nHands-on Activity: Simple randomization exercises in R.\n\n\n\n\n\nHomework 1: Basics of Experimental Design (due Feb. 6)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-1-introduction-to-experimental-design",
    "href": "syllabus/Lesson_plan.html#week-1-introduction-to-experimental-design",
    "title": "Lesson Plan",
    "section": "",
    "text": "Purpose and scope of experimental design\nObservational studies vs. designed experiments\nKey principles: randomization, replication, and control\n\n\n\n\n\nDifferentiate between observational studies and experiments.\nExplain the three fundamental principles of experimental design.\nRecognize scenarios where designed experiments are beneficial.\n\n\n\n\n\nLecture: Introduction to experimentation (Dean et al., Ch. 1, Sec. 1.1–1.1.1; Ch. 2, Sec. 2.1–2.2).\nCase Study Discussion: The role of experimental design in manufacturing and biomedical research.\nHands-on Activity: Simple randomization exercises in R.\n\n\n\n\n\nHomework 1: Basics of Experimental Design (due Feb. 6)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-2-completely-randomized-designs-crd",
    "href": "syllabus/Lesson_plan.html#week-2-completely-randomized-designs-crd",
    "title": "Lesson Plan",
    "section": "Week 2: Completely Randomized Designs (CRD)",
    "text": "Week 2: Completely Randomized Designs (CRD)\n\nTopics Covered\n\nStructure and application of CRD\nOne-way ANOVA model and assumptions\nConducting and interpreting ANOVA in R\n\n\n\nLearning Objectives\n\nDesign and analyze experiments using CRD.\nConduct one-way ANOVA and interpret results.\nCheck ANOVA assumptions and assess validity.\n\n\n\nClass Activities\n\nLecture: CRD framework and implementation (Dean et al., Ch. 3, Sec. 3.1–3.5).\nIn-class Example: One-way ANOVA in R using real-world data.\nGroup Discussion: Interpreting ANOVA tables and visualizing results.\n\n\n\nHomework\n\nHomework 2: CRD Analysis (due Feb. 13)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-3-multiple-comparisons",
    "href": "syllabus/Lesson_plan.html#week-3-multiple-comparisons",
    "title": "Lesson Plan",
    "section": "Week 3: Multiple Comparisons",
    "text": "Week 3: Multiple Comparisons\n\nTopics Covered\n\nPost-hoc tests: Tukey, Bonferroni, Scheffé\nContrasts: linear, orthogonal, and non-orthogonal comparisons\n\n\n\nLearning Objectives\n\nImplement multiple comparison methods in R.\nInterpret post-hoc test results.\nApply linear contrasts for deeper treatment comparisons.\n\n\n\nClass Activities\n\nLecture: Multiple comparisons and contrasts (Dean et al., Ch. 4, Sec. 4.1–4.4).\nR Demonstration: Tukey’s HSD, Bonferroni correction, and Scheffé method.\nPractice Exercise: Identifying significant treatment differences.\n\n\n\nHomework\n\nHomework 3: Multiple Comparisons (due Feb. 20)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-4-checking-model-assumptions",
    "href": "syllabus/Lesson_plan.html#week-4-checking-model-assumptions",
    "title": "Lesson Plan",
    "section": "Week 4: Checking Model Assumptions",
    "text": "Week 4: Checking Model Assumptions\n\nTopics Covered\n\nResidual diagnostics\nDetecting non-normality and heteroscedasticity\nApplying transformations\n\n\n\nLearning Objectives\n\nConduct residual analysis to check ANOVA assumptions.\nImplement transformations when needed.\nIdentify and handle outliers in experimental data.\n\n\n\nClass Activities\n\nLecture: Model assumption checking (Dean et al., Ch. 5, Sec. 5.1–5.3).\nR Practical: Visualizing residuals and testing normality.\nGroup Activity: Analyzing transformed data.\n\n\n\nHomework\n\nHomework 4: Residual Analysis (due Feb. 27)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-5-two-way-factorial-designs",
    "href": "syllabus/Lesson_plan.html#week-5-two-way-factorial-designs",
    "title": "Lesson Plan",
    "section": "Week 5: Two-Way Factorial Designs",
    "text": "Week 5: Two-Way Factorial Designs\n\nTopics Covered\n\nTwo-factor factorial structure\nMain effects and interactions\nTwo-way ANOVA model and interpretation\n\n\n\nLearning Objectives\n\nImplement and analyze two-way factorial designs.\nInterpret interaction plots and ANOVA tables.\nRecognize the significance of main effects vs. interactions.\n\n\n\nClass Activities\n\nLecture: Factorial design theory (Dean et al., Ch. 6, Sec. 6.1–6.5).\nR Practical: Running and interpreting a two-way ANOVA.\nClass Discussion: Interaction effects and their implications.\n\n\n\nHomework\n\nHomework 5: Two-Factor ANOVA (due Mar. 6)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-6-higher-order-factorial-designs",
    "href": "syllabus/Lesson_plan.html#week-6-higher-order-factorial-designs",
    "title": "Lesson Plan",
    "section": "Week 6: Higher-Order Factorial Designs",
    "text": "Week 6: Higher-Order Factorial Designs\n\nTopics Covered\n\nThree-way factorial designs\nHigher-order interactions\nManaging unbalanced designs\n\n\n\nLearning Objectives\n\nAnalyze three-factor experiments.\nIdentify and interpret complex interactions.\nHandle missing or unbalanced data in factorial designs.\n\n\n\nClass Activities\n\nLecture: Higher-order factorial designs (Dean et al., Ch. 7, Sec. 7.1–7.6).\nR Demonstration: Analyzing a three-way factorial experiment.\nDiscussion: Real-world applications of high-order designs.\n\n\n\nHomework\n\nHomework 6: Higher-Order Factorial Analysis (due Mar. 6)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-7-midterm-review-and-exam",
    "href": "syllabus/Lesson_plan.html#week-7-midterm-review-and-exam",
    "title": "Lesson Plan",
    "section": "Week 7: Midterm Review and Exam",
    "text": "Week 7: Midterm Review and Exam\n\nMidterm Review Session: Key concepts from Weeks 1–6.\nMidterm Exam: Covers CRD, factorial designs, and multiple comparisons."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-8-blocking-randomized-complete-block-design-rcbd",
    "href": "syllabus/Lesson_plan.html#week-8-blocking-randomized-complete-block-design-rcbd",
    "title": "Lesson Plan",
    "section": "Week 8: Blocking & Randomized Complete Block Design (RCBD)",
    "text": "Week 8: Blocking & Randomized Complete Block Design (RCBD)\n\nTopics Covered\n\nConcept of blocking\nRCBD structure and analysis\nEfficiency of blocking designs\n\n\n\nLearning Objectives\n\nApply RCBD principles to experimental design.\nConduct ANOVA for blocked designs.\nAssess the efficiency of blocking.\n\n\n\nClass Activities\n\nLecture: RCBD and blocking efficiency (Dean et al., Ch. 10, Sec. 10.1–10.4).\nR Demonstration: Implementing RCBD.\nDiscussion: Comparing blocking to CRD.\n\n\n\nHomework\n\nHomework 7: RCBD Analysis (due Mar. 25)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-9-spring-break-no-classes",
    "href": "syllabus/Lesson_plan.html#week-9-spring-break-no-classes",
    "title": "Lesson Plan",
    "section": "Week 9: Spring Break – No Classes",
    "text": "Week 9: Spring Break – No Classes"
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-10-complete-block-designs",
    "href": "syllabus/Lesson_plan.html#week-10-complete-block-designs",
    "title": "Lesson Plan",
    "section": "Week 10: Complete Block Designs",
    "text": "Week 10: Complete Block Designs\n\nTopics Covered\n\nExtensions of RCBD\nFactorial treatments within blocks\nVariance components analysis\n\n\n\nLearning Objectives\n\nAnalyze complete block designs.\nEstimate variance components.\nCompare efficiency of different blocking approaches.\n\n\n\nClass Activities\n\nLecture: Advanced blocking designs (Dean et al., Ch. 10, Sec. 10.6–10.8).\nR Practical: Implementing factorial treatments in blocks.\nGroup Activity: Evaluating blocking efficiency.\n\n\n\nHomework\n\nHomework 8: Complete Block Designs (due Apr. 3)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-11-analysis-of-covariance-ancova",
    "href": "syllabus/Lesson_plan.html#week-11-analysis-of-covariance-ancova",
    "title": "Lesson Plan",
    "section": "Week 11: Analysis of Covariance (ANCOVA)",
    "text": "Week 11: Analysis of Covariance (ANCOVA)\n\nTopics Covered\n\nIncorporating covariates into ANOVA\nAdjusted treatment means\nANCOVA assumptions\n\n\n\nLearning Objectives\n\nImplement ANCOVA in R.\nInterpret adjusted means and significance.\nApply ANCOVA in experimental studies.\n\n\n\nClass Activities\n\nLecture: ANCOVA theory and application (Dean et al., Ch. 9, Sec. 9.1–9.4).\nR Practical: Performing ANCOVA and interpreting results.\nDiscussion: Comparing ANCOVA to standard ANOVA.\n\n\n\nHomework\n\nHomework 9: ANCOVA Analysis (due Apr. 10)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-1215-advanced-experimental-designs",
    "href": "syllabus/Lesson_plan.html#week-1215-advanced-experimental-designs",
    "title": "Lesson Plan",
    "section": "Week 12–15: Advanced Experimental Designs",
    "text": "Week 12–15: Advanced Experimental Designs\n\nRow–Column Designs (Latin Squares) (Week 12)\nRandom & Mixed Models (Week 13)\nNested Models (Week 14)\nSplit-Plot Designs (Week 15)\n\nEach week will include: - Lectures on theoretical foundations. - R demonstrations on implementing models. - Group discussions on real-world applications.\nHomework: - Week 12: Row–Column Analysis (due Apr. 17). - Week 13: Random and Mixed Models (due Apr. 24). - Week 14: Nested Models (due May 1). - Week 15: Split-Plot Analysis (due May 6)."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-16-catching-up-and-review",
    "href": "syllabus/Lesson_plan.html#week-16-catching-up-and-review",
    "title": "Lesson Plan",
    "section": "Week 16: Catching Up and Review",
    "text": "Week 16: Catching Up and Review\n\nReview key concepts from Weeks 1–15.\nAddress common misconceptions and questions.\nProvide additional practice problems."
  },
  {
    "objectID": "syllabus/Lesson_plan.html#week-17-final-exam",
    "href": "syllabus/Lesson_plan.html#week-17-final-exam",
    "title": "Lesson Plan",
    "section": "Week 17: Final Exam",
    "text": "Week 17: Final Exam\n\nDate: Tuesday, May 13, 2025.\nContent: Comprehensive (Weeks 1–15).\nFormat: Short-answer, computations, and application-based problems."
  },
  {
    "objectID": "syllabus/syllabus-final.html",
    "href": "syllabus/syllabus-final.html",
    "title": "📄 Course Syllabus",
    "section": "",
    "text": "Instructor: Dr. Davood Tofighi\nLocation: Science Math Learning Center 120\nTime: Tuesday & Thursday, 2:00 pm - 3:15 pm\nOffice Hours: Thursday, 3:30 pm - 4:30 pm\nCourse Website:\n\nUNM Canvas\nhttps://Data-Wise.github.io/doe/.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#course-information",
    "href": "syllabus/syllabus-final.html#course-information",
    "title": "📄 Course Syllabus",
    "section": "",
    "text": "Instructor: Dr. Davood Tofighi\nLocation: Science Math Learning Center 120\nTime: Tuesday & Thursday, 2:00 pm - 3:15 pm\nOffice Hours: Thursday, 3:30 pm - 4:30 pm\nCourse Website:\n\nUNM Canvas\nhttps://Data-Wise.github.io/doe/.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#course-description",
    "href": "syllabus/syllabus-final.html#course-description",
    "title": "📄 Course Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis course provides a rigorous introduction to experimental design and analysis of variance (ANOVA) methodologies, with an emphasis on their integration into linear modeling frameworks. Topics include fundamental principles of experimental design (randomization, replication, and blocking), handling unbalanced data, diagnostics for model assumptions, and advanced designs such as split-plot, nested, and mixed-effects models. Students will develop skills in designing experiments, selecting appropriate statistical approaches, analyzing data using R, and communicating findings effectively.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#learning-outcomes",
    "href": "syllabus/syllabus-final.html#learning-outcomes",
    "title": "📄 Course Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this course, students will be able to:\n\nDesign experiments using principles of randomization, replication, and blocking.\nImplement one-way, two-way, and higher-order factorial ANOVA techniques.\nAnalyze unbalanced data using modern statistical methods.\nConduct residual analysis, transformations, and diagnostics for model assumptions.\nPerform multiple comparisons and construct contrasts for treatment effect interpretation.\nIncorporate covariates into ANOVA frameworks through analysis of covariance (ANCOVA).\nApply advanced designs like row-column, nested, and split-plot experiments.\nUtilize statistical software (e.g., R) to perform experimental analyses and generate reproducible workflows.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#prerequisite",
    "href": "syllabus/syllabus-final.html#prerequisite",
    "title": "📄 Course Syllabus",
    "section": "Prerequisite",
    "text": "Prerequisite\nA prior course in statistics, such as STAT 440/550, is strongly recommended. Students without this background must meet with the instructor to discuss their preparation.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#textbook-and-software",
    "href": "syllabus/syllabus-final.html#textbook-and-software",
    "title": "📄 Course Syllabus",
    "section": "Textbook And Software",
    "text": "Textbook And Software\n\nPrimary Textbook: Dean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\n\nAvailable as a free eBook for UNM students through the UNM Library. Use your UNM credentials to access it. The eBook formats are in PDF and EPUB.\n\nSupplemental Textbooks\nLibre Textbook from Penn State University: Analysis of Variance and Design of Experiments\n\nThis book privides an accessible introduction to the principles of experimental design and ANOVA. If you find the primary textbook challenging, this resource may offer additional support.\n\nFirst version of Prof. Christensen’s book from his UNM website\n\nNote that this is an older (first) version of the book from Prod. Christensen’s website. The book is available in PDF format.\nAn execellent R companion, which is freely available, to the book is R Companion to Applied Regression. \n\nSoftware:\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd edition). O’Reilly.\nThis book is available online for free and provides a comprehensive guide to using R for data analysis and visualization.\nR (version 4.2 or higher), paired with RStudio, VS Code, or Positron, along with Quarto for reproducible workflows.\nKey R packages: lm, emmeans, pwr, multcomp, lme4, and others as introduced during the course.\nCran Task View: Experimental Design and Analysis\n\nThis resource provides an overview of R packages for experimental design and analysis, including links to package documentation and tutorials.\n\nPackage daewr accompanies the book Design and Analysis of Experiments with R by Lawson (2014) and does not only provide data sets from the book but also some standalone functionality that is not available elsewhere in R, e.g. definitive screening designs.\nPackage agricolae is a package for statistical analysis of experimental designs.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#course-resources",
    "href": "syllabus/syllabus-final.html#course-resources",
    "title": "📄 Course Syllabus",
    "section": "Course Resources",
    "text": "Course Resources\n1. Lecture Notes and Online Resources\n\nLecture notes and online tutorials will be made available in HTML/PDF format on the dedicated GitHub Pages website for this course. This platform ensures easy access to interactive and organized course materials.\nGitHub Pages Website Address: https://Data-Wise.github.io/doe/.\nThe GitHub Pages website will be regularly updated with new materials. However, please note that:\n\nSome supplemental readings may not be available on the website due to copyright restrictions.\nOnly supplemental readings in the public domain or with proper permissions will be provided on GitHub Pages.\n\n\nImportant Note about GitHub Pages and Browser Caching\n\nGitHub Pages occasionally caches content, which may result in outdated materials being displayed. To ensure you are viewing the latest updates:\nRefresh your browser: Press Ctrl + F5 (Windows) or Cmd + Shift + R (Mac) for a hard refresh.\nClear your browser cache: If refreshing doesn’t work, clear your browser’s cache in your browser’s settings and revisit the website. \n\n2. Canvas Course Platform\n\nAll other course materials, such as:\n\nHomework assignments (on both Canvas and GitHub Pages)\nAssignments Submission portals\nSupplemental readings not in the public domain\nAnnouncements\nDiscussion boards will be hosted on Canvas.\n\nStudents are responsible for checking Canvas regularly for important updates, announcements, and access to restricted course materials.\n\n3. Supplemental Readings\n\nSupplemental readings will be provided through Canvas whenever possible. However:\nReadings under copyright or without appropriate permissions will not be distributed directly.\nWhere applicable, references or external links will be shared for you to access materials through UNM’s library system or other authorized platforms.\n\nContact for Technical Issues\n\nIf you encounter persistent issues accessing materials on GitHub Pages or Canvas, please notify the instructor or TA immediately for assistance.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#communication-guidelines",
    "href": "syllabus/syllabus-final.html#communication-guidelines",
    "title": "📄 Course Syllabus",
    "section": "Communication Guidelines",
    "text": "Communication Guidelines\n\nAnnouncements: All important course announcements will be posted on Canvas. It is your responsibility to regularly check Canvas to stay updated. Missing important information due to not checking Canvas will not be considered an acceptable excuse.\nDiscussion/Questions: For course-related questions, post them on the Canvas discussion board. Do not email me or the TA directly for general questions about the course. Before posting, review the syllabus to ensure your question hasn’t already been addressed. Questions that are already answered in the syllabus will not receive a response.\nEmail: All email communication must be sent through Canvas. Direct emails to my university or personal email address will not receive a response. Please ensure that all personal or sensitive matters requiring email communication are sent via Canvas.\nPersonal Matters: For personal concerns, contact me only through Canvas email or visit during office hours.\nOffice Hours (by Appointment): Please feel free to schedule an appointment with me or the TA if you need additional help or clarification. Office hours are a great opportunity to discuss your questions and ensure your understanding of the material.\n\nReminder: Checking Canvas regularly is essential for staying informed and up-to-date with the course.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#attendance-policy",
    "href": "syllabus/syllabus-final.html#attendance-policy",
    "title": "📄 Course Syllabus",
    "section": "Attendance Policy",
    "text": "Attendance Policy\n\nAttendance Expectations\n\nMandatory Attendance: Students are expected to attend and actively participate in all class sessions to maximize learning and engagement with the course material.\nAttendance Tracking: Attendance will be recorded at the start of every class session. It is the student’s responsibility to ensure they are marked present.\nAccountability: Attendance is critical, as the course material is cumulative and often discussed interactively.\n\n\n\nAttendance Limits and Drop Policy\n\nAbsence Limits:\n\nStudents may accumulate up to 4 unexcused absences (equivalent to 15% of the total 28 class sessions). Students with 5 or more unexcused absences may be subject to an administrative drop from the course.\n\nAdministrative Drop:\n\nIf a student exceeds the allowed 4 unexcused absences, they may be administratively dropped from the course.\nStudents dropped before the withdrawal deadline will receive a grade of “W”.\nStudents dropped after the withdrawal deadline may receive a grade of “F”, depending on their overall course performance and UNM’s Grading Policies.\n\n\n\n\nExcused vs. Unexcused Absences\n\nExcused Absences: Absences may be excused if they meet the criteria outlined in the UNM Attendance Policy. Examples include:\n\nUniversity-sponsored activities (e.g., athletic or academic events).\nIllness or medical emergencies (with appropriate documentation).\nPersonal emergencies (with prior approval when possible).\n\nUnexcused Absences: Any absence that does not meet the criteria for excused absences will be considered unexcused.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#grading-and-assessments",
    "href": "syllabus/syllabus-final.html#grading-and-assessments",
    "title": "📄 Course Syllabus",
    "section": "Grading And Assessments",
    "text": "Grading And Assessments\nThe final grade in this course will be determined by the following components and their respective weights:\n\n\n\nComponent\nWeight\n\n\n\n\nHomework Assignments\n40%\n\n\nMidterm Exam\n20%\n\n\nFinal Exam\n30%\n\n\nQuizzes\n10%\n\n\nExtra Credit\nUp to 4%\n\n\n\n\nGrading Scale\nThe following cut-off values will be used to assign final letter grades:\n\n\n\nPercentage\nLetter Grade\n\n\n\n\n98-100\nA+\n\n\n93-97.99\nA\n\n\n90-92.99\nA-\n\n\n87-89.99\nB+\n\n\n83-86.99\nB\n\n\n80-82.99\nB-\n\n\n77-79.99\nC+\n\n\n73-76.99\nC\n\n\n70-72.99\nC-\n\n\n67-69.99\nD+\n\n\n63-66.99\nD\n\n\n60-62.99\nD-\n\n\nBelow 60\nF\n\n\n\n\n\nHomework (40%)\n\nHomework Submission Guidelines\n\nFormat and Template:\nAll homework assignments must be completed using the Quarto template provided on the course website. You are required to submit the rendered PDF version of the Quarto document. Failure to use the provided template or submit the correct format will result in point deductions.\nSubmission Platform:\nSubmit your completed assignments electronically through Canvas. Navigate to the “Assignments” folder, click on the appropriate homework assignment, and attach your rendered PDF file.\nPlagiarism Check:\nAll submitted assignments will be automatically checked for plagiarism by Canvas. Any instances of plagiarism or academic dishonesty will be handled in accordance with UNM’s Student Code of Conduct and the Support in Receiving Help and Doing What is Right section outlined in this syllabus. Please familiarize yourself with the academic integrity policies detailed in the syllabus and on the UNM website. Ignorance of the policy is not an excuse for violations.\nNo Late Submissions Accepted:\nAll homework assignments are due by the specified deadline on Canvas. Late submissions will not be accepted under any circumstances. This policy is strict and applies to all students without exceptions.\nExtra Credit Opportunity:\nIf you miss an assignment, you may compensate for it by earning extra credit as outlined in the syllabus. This provides an opportunity to recover points without requiring extensions or exceptions for missed deadlines.\nDo Not Email About Late Assignments:\nPlease do not email me or the TA regarding missed or late homework submissions. We cannot make exceptions or accept late assignments under any conditions. Instead, focus on utilizing the extra credit opportunities provided in the syllabus.\nContent Requirements:\n\nAll responses must be typed within the Quarto document.\nIf you include hand-written calculations or derivations, they must be scanned and appended to the Quarto document as an appendix before rendering the final PDF.\n\nIndependence:\nAll homework assignments must be completed independently. While collaboration in studying is encouraged, your final submission must reflect your own work.\nGrading and Feedback:\nAssignments will be graded and feedback will be provided through Canvas. Be sure to review comments and feedback promptly to address any misunderstandings before the next assignment.\n\nImportant Note: To avoid missing deadlines, plan your work carefully and submit your assignments early. If you are unfamiliar with Quarto or need assistance using the template, refer to the “Resources” section on Canvas for tutorials or contact me during office hours.\n\n\n\nExams (50%)\nMidterm Exam (20%)\n\nDate: Thursday, March 6, 2025 (during class time)\nLocation: Classroom (as specified in the syllabus)\nContent: Covers topics from Weeks 1-7.\n\nFinal Exam (30%)\n\nDate: Tuesday, May 13, 2025, from 10:00 a.m. to 12:00 p.m.\nLocation: Classroom (as specified in the syllabus)\nContent: Comprehensive, covering the entire course.\n\nExam Guidelines\n\nFormat: Exams are closed book but you may bring up to three double-sided cheat sheets (US Letter size) and a calculator. No other electronics or materials (e.g., books, lecture notes) are allowed.\nQuestion Types: Includes a combination of short-answer questions, computations, and application-based problems.\n\nNo Make-Up Exams Policy\n\nMake-up exams will only be granted in cases of proven emergency that meet the criteria set forth by the Department of Mathematics and Statistics and UNM policies (e.g., documented medical emergencies, official university-sponsored activities, or other unavoidable situations as defined by university guidelines).\nIf you anticipate a potential conflict or emergency, you must notify the instructor in advance of the exam date whenever possible. Failure to do so will result in forfeiture of the opportunity to take a make-up exam.\nFor non-emergency situations, personal conflicts, or reasons outside of those outlined in this document (e.g., vacation, workload, or personal oversight), make-up exams will not be permitted.\n\nProven Emergency Requirements:\n\nMedical emergencies: Provide a signed note from a licensed healthcare provider explaining the need for absence.\nUniversity-sponsored activities: Submit official documentation from the relevant department.\nOther emergencies: Submit verifiable proof (e.g., legal documentation, official notifications).\n\nIn all cases, documentation must be provided as soon as possible and must align with UNM’s student policies on academic integrity and attendance. For more information, consult the UNM Student Handbook.\nImportance of Timely Notification\nIf you foresee a conflict or are unable to attend an exam due to an emergency, notify the instructor as early as possible. While advance notice does not guarantee a make-up exam, failing to provide notice will disqualify you from consideration for a make-up opportunity.\nBy adhering to this policy, we ensure fairness and consistency for all students while maintaining compliance with university guidelines. If you have further questions or concerns, please contact the instructor before the exam dates.\n\n\nQuizzes (10%)\n\n\nQuiz Guidelines\n\nQuizzes will be posted on Canvas and must be completed before each class.\nQuizzes cover general questions and concepts about the materials for the upcoming class.\nIt is essential to read the materials before each week to prepare for the quizzes.\nYou do not need to know all the details, but a general understanding is required.\n\n\nHow to Read and Prepare for the Quiz\nTo effectively prepare for quizzes, follow this two-pass reading strategy:\n\nFirst Pass: Read through the materials quickly without pausing to understand the details.\n\nThis helps you familiarize yourself with the topics and structure of the content.\n\nSecond Pass: Focus on understanding the general ideas and concepts.\n\nThis will provide the foundation needed for the quiz. Details will be covered during the lecture.\n\n\n\n\n\nExtra Credit Opportunities (Up to 4%)\nStudents have the opportunity to earn up to 4% extra credit toward their final grade by completing the following tasks:\n1. End-of-Semester Course Evaluation (Up to 3%)\n\nStudents who complete the end-of-semester course evaluation will receive up to 3% extra credit.\nTo receive the extra credit, students must:\nSubmit the course evaluation through the university’s designated system (e.g., UNM’s course evaluation platform).\nPost a screenshot or copy of the receipt of submission (no personal information visible) to the “Course Evaluation” discussion board on Canvas.\nDeadline for posting the receipt will be announced during the final week of class.\n\n2. Background and Syllabus Quiz (1%)\n\nA “Background and Syllabus Quiz” will be available on the course website during the first week of class.\nThis quiz will assess familiarity with the syllabus and the course’s policies and expectations. Completing the quiz will earn 1% extra credit.\nNote: The availability of this quiz may vary by semester and will be announced at the start of the course.\n\nImportant Notes:\n\nThese extra credit opportunities are designed to encourage engagement with the course policies and participation in university processes.\nExtra credit is optional and cannot replace or make up for missed assignments, exams, or other coursework.\nThe maximum total extra credit available is 4%, which will be applied to your final grade at the end of the semester.\nStudents are encouraged to review the “Grading and Assessments” section of the syllabus to understand how extra credit impacts their overall grade.\n\nPlease take advantage of these opportunities to improve your final grade while reinforcing your understanding of the course and contributing to constructive course feedback.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#what-to-expect-and-how-to-study-and-be-successful-in-this-course",
    "href": "syllabus/syllabus-final.html#what-to-expect-and-how-to-study-and-be-successful-in-this-course",
    "title": "📄 Course Syllabus",
    "section": "What to Expect and How to Study and Be Successful in This Course",
    "text": "What to Expect and How to Study and Be Successful in This Course\nThis course is designed to be both rigorous and rewarding, equipping you with essential skills in experimental design and analysis of variance (ANOVA). Success in this course requires preparation, consistent effort, and active engagement with the material. Below are some key recommendations and expectations to help you succeed:\n1. Expect to Invest Significant Time\n\nMastery of the material requires dedicated study time each week. Plan to spend at least 8-10 hours per week outside of class on:\nReading assigned materials.\nCompleting homework assignments.\nPracticing statistical techniques in R.\nReviewing notes and working through practice problems.\n\n2. Prioritize Class Attendance\n\nAttendance is critical to your success in this course.\nClasses are designed to build upon each other, with cumulative content.\nDiscussions, group activities, and real-time problem-solving in class will reinforce your understanding.\nHistorical data has shown that students who miss class sessions struggle to keep up with the material.\nIf you must miss a class, review the lecture notes and course materials promptly, and reach out to classmates for additional support.\n\n3. Come Prepared for Every Class\n\nRead the assigned materials before class:\nFamiliarize yourself with the key topics to be covered.\nJot down questions or areas of confusion to bring to class discussions.\nEngage with supplemental materials like videos, tutorials, or examples provided on Canvas.\nStay organized by keeping track of due dates, reading assignments, and planned lectures using the course schedule.\n\n4. Stay Active During Class\n\nTake detailed notes during lectures and participate in discussions.\nActively engage in group activities. Collaborating with classmates helps deepen your understanding and clarifies difficult concepts.\nAsk questions during class or post them on Canvas if you need further clarification.\n\n5. Utilize Office Hours and Discussion Boards\n\nOffice hours are an excellent opportunity for one-on-one assistance with:\nUnderstanding difficult concepts.\nReviewing homework problems.\nDiscussing your specific challenges with course material.\nUse the Canvas Discussion Boards for peer-to-peer support and quick clarifications from the instructor or TA.\n\n6. Practice Regularly\n\nMastery of statistical methods requires consistent practice.\nWork through examples provided in class and additional exercises from the textbook.\nAttempt problems in R to solidify your understanding of statistical tools and software.\n\n7. Review and Revise\n\nAfter each class, set aside time to:\nReview your notes.\nSummarize key concepts in your own words.\nAttempt practice problems to apply what you’ve learned.\n\n8. Focus on Understanding, Not Memorization\n\nThis course emphasizes application and critical thinking.\nStrive to understand the “why” behind statistical techniques and experimental designs.\nApply concepts to real-world scenarios or examples from your own field of study.\n\n9. Manage Your Time Wisely\n\nStart assignments early to avoid last-minute stress.\nBreak down large tasks into smaller steps and set achievable goals for each study session.\nUse tools like planners or digital calendars to track deadlines and manage your workload effectively.\n\n10. Maintain a Growth Mindset\n\nStatistics can be challenging, but persistence is key.\nEmbrace mistakes as part of the learning process.\nReach out for help early if you’re struggling with a concept or assignment.\n\n11. Make Use of the Resources Provided\n\nThe course provides several resources to support your learning:\nA Quarto template for homework submissions.\nSupplemental readings, videos, and tutorials.\nAccess to the course textbook through UNM’s library system.\nR and VS Code or RStudio (or equivalent IDEs) for conducting statistical analyses.\n\n12. Avoid Falling Behind\n\nGiven the cumulative nature of this course, falling behind on one topic will make subsequent topics more difficult to grasp.\nStay on top of assignments and make reviewing previous material part of your weekly routine.\n\n13. Academic Honesty and Collaboration\n\nWhile collaboration is encouraged during study sessions or discussions, ensure all submitted work reflects your own understanding.\nFamiliarize yourself with the Support In Receiving Help and Doing What is Right section of the syllabus to avoid issues related to academic dishonesty.\n\nBy following these recommendations, staying engaged, and managing your time wisely, you’ll position yourself for success in this course. Remember, learning statistics is a journey that requires consistent effort and a positive attitude. I am here to support you every step of the way, so don’t hesitate to reach out for assistance.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#accommodation",
    "href": "syllabus/syllabus-final.html#accommodation",
    "title": "📄 Course Syllabus",
    "section": "Accommodation",
    "text": "Accommodation\nIn accordance with University Policy 2310 and the Americans with Disabilities Act (ADA), academic accommodations will be provided for students who notify the instructor of their need. It is the student’s responsibility to inform the instructor promptly so accommodations can be arranged. Instructors are not legally permitted to inquire about disabilities or the need for accommodations. For assistance in arranging accommodations, contact the Accessibility Resource Center (ARC):\n\nPhone: 277-3506\nEmail: arcsrvs@unm.edu\nWebsite: Accessibility Resource Center\n\nStudents who may require assistance in emergency evacuations should also contact the instructor to discuss the most appropriate procedures to follow.\nUNM is dedicated to fostering an inclusive, accessible, and supportive learning environment for all participants. If you encounter physical, academic, or mental health barriers, or concerns related to physical health, mental health, or COVID-19, you are encouraged to communicate these concerns to the instructor via Canvas email, phone, or during office hours. Additional support is available through the Accessibility Resource Center. Let’s work together to ensure you have equitable access to this course.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#title-ix",
    "href": "syllabus/syllabus-final.html#title-ix",
    "title": "📄 Course Syllabus",
    "section": "Title IX",
    "text": "Title IX\nAs part of UNM’s commitment to a safe and equitable campus environment, faculty, Teaching Assistants (TAs), and Graduate Assistants (GAs) are considered “responsible employees” by the Department of Education. This means that any report of gender discrimination—including sexual harassment, misconduct, or violence—shared with a faculty member, TA, or GA must be reported to the Title IX Coordinator at the Office of Equal Opportunity (OEO).\n\nOffice of Equal Opportunity.\nTitle IX policy details\n\nFor more information on reporting obligations, see the Title IX Guidelines Title IX Guidelines. If you prefer to discuss sensitive concerns confidentially, please consider reaching out to the UNM LoboRESPECT Advocacy Center or other confidential resources on campus.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#support-in-receiving-help-and-doing-what-is-right",
    "href": "syllabus/syllabus-final.html#support-in-receiving-help-and-doing-what-is-right",
    "title": "📄 Course Syllabus",
    "section": "Support In Receiving Help and Doing What is Right",
    "text": "Support In Receiving Help and Doing What is Right\nUNM provides a range of services and policies to help students succeed academically and thrive within the university community. If you are unsure where to begin, consult the UNM Student Services website or reach out to the instructor for guidance in identifying the right resource.\nKey Policies and Support Resources:\n\nStudent Grievances: Policies D175 and D176 in the Faculty Handbook detail procedures for addressing grievances.\nAcademic Dishonesty: Adhere to UNM’s policies outlined in the Faculty Handbook D100, which address plagiarism and academic misconduct.\nRespectful Campus Policy: Uphold UNM’s guidelines for fostering a respectful academic environment (Faculty Handbook C09).\n\nFind these resources in the Student Pathfinder or the Faculty Handbook.\nPlagiarism and Academic Dishonesty: Violations of academic integrity have serious consequences, including failing an assignment or course, and potential suspension or expulsion. If you have any questions about avoiding plagiarism, please ask the instructor or consult UNM’s policies. It is always better to ask for clarification than to risk committing an academic violation.\nIf you need assistance understanding the guidelines or accessing these resources, please do not hesitate to reach out.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#land-acknowledgement",
    "href": "syllabus/syllabus-final.html#land-acknowledgement",
    "title": "📄 Course Syllabus",
    "section": "Land Acknowledgement",
    "text": "Land Acknowledgement\nFounded in 1889, the University of New Mexico sits on the traditional homelands of the Pueblo of Sandia. The original peoples of New Mexico Pueblo, Navajo, and Apache since time immemorial, have deep connections to the land and have made significant contributions to the broader community statewide. We honor the land itself and those who remain stewards of this land throughout the generations and also acknowledge our committed relationship to Indigenous peoples. We gratefully recognize our history.",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus-final.html#course-schedule-assignment-deadlines-and-exam-dates",
    "href": "syllabus/syllabus-final.html#course-schedule-assignment-deadlines-and-exam-dates",
    "title": "📄 Course Syllabus",
    "section": "Course Schedule, Assignment Deadlines, and Exam Dates",
    "text": "Course Schedule, Assignment Deadlines, and Exam Dates\nThe course schedule, including assignment due dates and the midterm exam date, is subject to change based on the progress of the class, unforeseen circumstances, or instructor discretion. Students are responsible for regularly checking Canvas and announcements on the course GitHub page for the most up-to-date information regarding deadlines and schedule adjustments.\nHowever, the final exam date is not subject to change, as it is determined and scheduled by the university. Please plan accordingly based on the final exam schedule provided by UNM:\n\nFinal Exam Date: Tuesday, May 13, 2025, 10:00 a.m.-12:00 p.m.\nLocation: Classroom (as specified in the syllabus)\n\nStudents are strongly encouraged to stay proactive, monitor changes, and ensure they meet all revised deadlines and expectations as announce\n\nTentative Course Schedule\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\nReadings & Homework Assignments\n\n\n\n\n1\nJan. 21 & 23, 2025\nIntroduction to Experimental Design\nCh. 1 (Sections 1.1-1.1.1), Ch. 2 (Sections 2.1-2.2).\n\n\n2\nJan. 28 & 30, 2025\nCompletely Randomized Designs (CRD)\nCh. 3 (Sections 3.1-3.5); Homework 1 (due Feb. 6).\n\n\n3\nFeb. 4 & 6, 2025\nMultiple Comparisons\nCh. 4 (Sections 4.1-4.4); Homework 2 (due Feb. 13).\n\n\n4\nFeb. 11 & 13, 2025\nChecking Model Assumptions\nCh. 5 (Sections 5.1-5.3); Homework 3 (due Feb. 20).\n\n\n5\nFeb. 18 & 20, 2025\nTwo-Way Factorial Designs\nCh. 6 (Sections 6.1-6.5); Homework 4 (due Feb. 27).\n\n\n6\nFeb. 25 & 27, 2025\nHigher-Order Factorial Designs\nCh. 7 (Sections 7.1-7.6); Homework 5 (due Mar. 6).\n\n\n7\nMar. 4 & 6, 2025\nMidterm Exam\nReview all chapters and notes.\n\n\n8\nMar. 11 & 13, 2025\nBlocking & RCBD\nCh. 10 (Sections 10.1-10.4); Homework 6 (due Mar. 25).\n\n\n9\nMar. 16-20, 2025\nSpring Break\nNo classes.\n\n\n10\nMar. 25 & 27, 2025\nComplete Block Designs\nCh. 10 (Sections 10.6-10.8); Homework 7 (due Apr. 3).\n\n\n11\nApr. 1 & 3, 2025\nAnalysis of Covariance (ANCOVA)\nCh. 9 (Sections 9.1-9.4); Homework 8 (due Apr. 10).\n\n\n12\nApr. 8 & 10, 2025\nRow-Column (Latin square) Designs\nCh. 12 (Sections 12.1-12.4); Homework 9 (due Apr. 17).\n\n\n13\nApr. 15 & 17, 2025\nRandom & Mixed Models\nCh. 17 (Sections 17.1-17.8); Homework 10 (due Apr. 24).\n\n\n14\nApr. 22 & 24, 2025\nNested Models\nCh. 18 (Sections 18.1-18.4); Homework 11 (due May 1).\n\n\n15\nApr. 29 & May 1, 2025\nSplit-Plot Designs\nCh. 19 (Sections 19.1-19.5); Homework 12 (due May 6).\n\n\n16\nMay 6 & 8, 2025\nCatching Up and Review\nComprehensive review and Q&A.\n\n\n17\nMay 13\nFinal Exam\nComprehensive (Weeks 1-15).",
    "crumbs": [
      "Home",
      "Course Overview",
      "Syllabus"
    ]
  },
  {
    "objectID": "worksheets/latin-square.html",
    "href": "worksheets/latin-square.html",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "",
    "text": "Below is a self-contained classroom handout illustrating step-by-step how to set up and analyze a Latin Square design, inspired by the ideas in Box, Hunter, and Hunter (“Statistics for Experimenters”) regarding experiments that must block on two factors in addition to comparing several treatments."
  },
  {
    "objectID": "worksheets/latin-square.html#context-and-purpose",
    "href": "worksheets/latin-square.html#context-and-purpose",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "1. Context and Purpose",
    "text": "1. Context and Purpose\nWe want to compare four teaching methods (labeled A, B, C, D) for their effect on students’ test scores. However, the teacher suspects that two other factors also cause variability:\n\nTeacher differences. Four different teachers (labeled T1, T2, T3, T4) may have different baseline abilities or styles.\nClassroom differences. Four classrooms (labeled R1, R2, R3, R4) may differ in environment, resources, or size.\n\nWe would like to “block” on both Teachers and Classrooms so that the primary comparison (A vs B vs C vs D) is not distorted by large differences among teachers or classrooms. A Latin Square layout ensures each treatment is tested once per teacher and once per classroom."
  },
  {
    "objectID": "worksheets/latin-square.html#layout-of-a-44-latin-square",
    "href": "worksheets/latin-square.html#layout-of-a-44-latin-square",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "2. Layout of a 4×4 Latin Square",
    "text": "2. Layout of a 4×4 Latin Square\nA 4×4 Latin Square ensures:\n\nEach row represents one teacher (T1, T2, T3, T4).\nEach column represents one classroom (R1, R2, R3, R4).\nEach of the four treatments (A, B, C, D) appears exactly once in every row and once in every column.\n\nAn example random assignment is shown in the table below. (In practice, the row and column labels themselves—and the letter labels for the treatments—are also randomized.)\n\n\n\n\nR1\nR2\nR3\nR4\n\n\n\n\nT1\nA\nB\nC\nD\n\n\nT2\nB\nC\nD\nA\n\n\nT3\nC\nD\nA\nB\n\n\nT4\nD\nA\nB\nC\n\n\n\n\nRandomization Note: - We randomly label the rows (teachers T1–T4). - We randomly label the columns (classrooms R1–R4). - We randomly assign the letters (A, B, C, D) to the four teaching methods. - Then we fill the 4×4 grid so that each letter appears exactly once per row and once per column."
  },
  {
    "objectID": "worksheets/latin-square.html#example-data",
    "href": "worksheets/latin-square.html#example-data",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "3. Example Data",
    "text": "3. Example Data\nSuppose after implementing the above plan, the student test scores (averaged if multiple students per cell) turn out as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nR1\nR2\nR3\nR4\nRow Means (Teacher)\n\n\n\n\nT1 (A, B, C, D)\n82\n88\n85\n83\n84.5\n\n\nT2 (B, C, D, A)\n79\n87\n81\n80\n81.8\n\n\nT3 (C, D, A, B)\n84\n89\n82\n86\n85.3\n\n\nT4 (D, A, B, C)\n81\n78\n80\n83\n80.5\n\n\nColumn Means (Classroom)\n81.5\n85.5\n82.0\n83.0\n-\n\n\n\nFrom this table, each entry is “the average test score” for that Teacher–Classroom combination under the assigned teaching method. We can gather the raw data into a single 16–observation list if we wish, but it is easiest to keep it in this 4×4 grid.\nLet’s label these responses by \\(Y_{ij}\\), where \\(i\\) indicates the teacher (row) and \\(j\\) indicates the classroom (column). Our grand average \\(\\bar{Y}\\) is the mean of all 16 scores."
  },
  {
    "objectID": "worksheets/latin-square.html#model-assumptions",
    "href": "worksheets/latin-square.html#model-assumptions",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "4. Model Assumptions",
    "text": "4. Model Assumptions\nA Latin Square experiment typically uses an additive model:\n\\[\nY_{ij} \\;=\\; \\mu \\;+\\; \\theta_i \\;+\\; \\phi_j \\;+\\; \\tau_{k(i,j)} \\;+\\; \\varepsilon_{ij}\n\\]\n\n\\(\\mu\\) = overall mean\n\\(\\theta_i\\) = effect of row \\(i\\) (e.g., teacher \\(i\\))\n\\(\\phi_j\\) = effect of column \\(j\\) (e.g., classroom \\(j\\))\n\\(\\tau_{k(i,j)}\\) = effect of whichever treatment \\(k\\) is assigned in row \\(i\\), column \\(j\\)\n\\(\\varepsilon_{ij}\\) = residual (“noise”) term\n\nThe design ensures each treatment \\(\\tau_k\\) is replicated four times overall (once in each row and once in each column).\n\nKey assumption: There is no strong interaction among teacher, classroom, and the treatment. In other words, the teacher and classroom merely “add” or “subtract” some baseline amount from the outcome. If strong interactions are suspected, a Latin Square is not appropriate."
  },
  {
    "objectID": "worksheets/latin-square.html#the-analysis-of-variance-anova",
    "href": "worksheets/latin-square.html#the-analysis-of-variance-anova",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "5. The Analysis of Variance (ANOVA)",
    "text": "5. The Analysis of Variance (ANOVA)\nWe decompose total variation into three parts:\n\nDifferences among rows (teachers).\nDifferences among columns (classrooms).\nDifferences among treatments (the four methods).\nResidual “error” (everything left over).\n\n\n5.1 Step-by-Step Sums of Squares\n\nCompute Grand Average (\\(\\bar{Y}\\)):\n\n\\[\n\\bar{Y} \\;=\\; \\frac{\\text{sum of all 16 observations}}{16}.\n\\]\nFrom the table above, adding all 16 values:\n\\[\n82 + 88 + 85 + 83 \\;+\\; 79 + 87 + 81 + 80 \\;+\\; 84 + 89 + 82 + 86 \\;+\\; 81 + 78 + 80 + 83 \\;=\\; 1278.\n\\]\nHence,\n\\[\n\\bar{Y} = \\frac{1278}{16} = 79.875.\n\\]\n(Rounded as needed; we’ll carry decimals through for demonstration.)\n\nRow (Teacher) Means: We already have row means from the table, e.g. for T1,\n\n\\[\n\\bar{Y}_{\\text{row }1} \\;=\\; \\frac{82 + 88 + 85 + 83}{4} = 84.5.\n\\]\nSimilarly for T2, T3, T4.\n\nColumn (Classroom) Means: We also have them from the table, e.g. for R1,\n\n\\[\n\\bar{Y}_{\\text{col }1} = \\frac{82 + 79 + 84 + 81}{4} = 81.5.\n\\]\n\nTreatment Means: Gather the four observations corresponding to each treatment (A, B, C, D). For example, treatment A appears in (T1,R1), (T2,R4), (T3,R3), (T4,R2) in our table. Compute its mean from those 4 data points. Repeat for B, C, D.\nSums of Squares:\n\nTotal Sum of Squares (SST), relative to the grand mean:\n\n\n\\[\n\\text{SST} \\;=\\; \\sum_{i=1}^4 \\sum_{j=1}^4 \\left(Y_{ij} - \\bar{Y}\\right)^2.\n\\]\n\nRow (Teacher) Sum of Squares:\n\n\\[\n\\text{SS}_{\\text{Rows}} \\;=\\; 4 \\,\\sum_{i=1}^4 \\left(\\bar{Y}_{\\text{row } i} - \\bar{Y}\\right)^2.\n\\]\n (Multiply by 4 because each row mean is based on 4 observations.)\n\nColumn (Classroom) Sum of Squares:\n\n\\[\n\\text{SS}_{\\text{Columns}} \\;=\\; 4 \\,\\sum_{j=1}^4 \\left(\\bar{Y}_{\\text{col } j} - \\bar{Y}\\right)^2.\n\\]\n\nTreatment Sum of Squares:\n\n\\[\n\\text{SS}_{\\text{Treat}} \\;=\\; 4 \\,\\sum_{k=A,B,C,D} \\left(\\bar{Y}_{k} - \\bar{Y}\\right)^2.\n\\]\n (Again, each treatment mean is based on 4 observations in a 4×4 square.)\n\nResidual Sum of Squares:\n\n\\[\n\\text{SS}_{\\text{Error}} \\;=\\; \\text{SST} \\;-\\; \\text{SS}_{\\text{Rows}} \\;-\\; \\text{SS}_{\\text{Columns}} \\;-\\; \\text{SS}_{\\text{Treat}}.\n\\]\n\n\n5.2 Degrees of Freedom\n\nTotal: \\(16 - 1 = 15\\).\nRows: \\(4 - 1 = 3\\).\nColumns: \\(4 - 1 = 3\\).\nTreatments: \\(4 - 1 = 3\\).\nResidual: \\(15 - (3+3+3) = 6\\).\n\n\n\n5.3 Mean Squares and the F-Ratios\n\nMean Square for Rows:\n\n\\[\n\\text{MS}_{\\text{Rows}} = \\frac{\\text{SS}_{\\text{Rows}}}{3}.\n\\]\n\nMean Square for Columns:\n\n\\[\n\\text{MS}_{\\text{Columns}} = \\frac{\\text{SS}_{\\text{Columns}}}{3}.\n\\]\n\nMean Square for Treatments:\n\n\\[\n\\text{MS}_{\\text{Treat}} = \\frac{\\text{SS}_{\\text{Treat}}}{3}.\n\\]\n\nResidual Mean Square:\n\n\\[\n\\text{MS}_{\\text{Error}} = \\frac{\\text{SS}_{\\text{Error}}}{6}.\n\\]\nWe then test for significance:\n\nRows vs. Error: \\(F_{\\text{Rows}} = \\frac{\\text{MS}_{\\text{Rows}}}{\\text{MS}_{\\text{Error}}}\\).\nColumns vs. Error: \\(F_{\\text{Columns}} = \\frac{\\text{MS}_{\\text{Columns}}}{\\text{MS}_{\\text{Error}}}\\).\nTreatments vs. Error: \\(F_{\\text{Treat}} = \\frac{\\text{MS}_{\\text{Treat}}}{\\text{MS}_{\\text{Error}}}\\).\n\nEach ratio follows (approximately) an \\(F\\)-distribution with appropriate degrees of freedom (numerator df = 3, denominator df = 6 in each case), so we can check standard \\(F\\) tables or software to see if the ratio is large enough to claim statistical significance."
  },
  {
    "objectID": "worksheets/latin-square.html#illustrative-partial-anova-table",
    "href": "worksheets/latin-square.html#illustrative-partial-anova-table",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "6. Illustrative (Partial) ANOVA Table",
    "text": "6. Illustrative (Partial) ANOVA Table\nBelow is a partial example of how an ANOVA table might look. (Numerical values are for demonstration only.)\n\n\n\n\n\n\n\n\n\n\n\nSource\nSS\ndf\nMS\nF-Ratio\np-Value\n\n\n\n\nRows (T1–T4)\n72.5\n3\n24.17\n4.12\n0.05\n\n\nColumns (R1–R4)\n60.3\n3\n20.10\n3.43\n0.08\n\n\nTreatments (A,B,C,D)\n90.0\n3\n30.00\n5.12\n0.03\n\n\nResidual\n105.0\n6\n17.50\n–\n–\n\n\nTotal\n327.8\n15\n–\n–\n–\n\n\n\n\n\nWe see that Treatments might be significant at around the 3% level, meaning there is evidence that at least one method (A, B, C, or D) differs from the others.\nRows (teachers) may also be significant at around 5%.\nColumns (classrooms) are not as strongly significant but still borderline.\nThe conclusion might be that the teaching methods do affect student test scores, with teacher differences also playing a role."
  },
  {
    "objectID": "worksheets/latin-square.html#graphical-anova-optional",
    "href": "worksheets/latin-square.html#graphical-anova-optional",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "7. Graphical ANOVA (Optional)",
    "text": "7. Graphical ANOVA (Optional)\nTo complement the ANOVA table, we can create a Graphical ANOVA:\n\nPlot the row effects (teacher means minus grand mean) on a dot chart scaled so that the “noise” is visually comparable.\nPlot the column effects (classroom means minus grand mean).\nPlot the treatment effects (method means minus grand mean).\nPlot the residuals.\n\nIf the spread of points for “Treatments” is obviously bigger than the residual spread, that visually supports that treatments matter. Meanwhile, if row or column spreads are large, that tells us there really are teacher or classroom blocks that differ."
  },
  {
    "objectID": "worksheets/latin-square.html#practical-advice-and-interpretation",
    "href": "worksheets/latin-square.html#practical-advice-and-interpretation",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "8. Practical Advice and Interpretation",
    "text": "8. Practical Advice and Interpretation\n\nCheck Residuals: Make sure there are no severe outliers or strong patterns that hint at non-constant variance or a need for transformation (e.g. log-transform).\nCheck Interactions: A Latin Square works best when row and column factors do not strongly interact with the treatments. If you see suspicious patterns, consider alternative designs.\nReplications: Often, people replicate the entire Latin Square more than once if they can afford extra runs. That increases residual degrees of freedom.\nConclusions: If the “Treatments” \\(F\\)-ratio is large (small p-value), we conclude at least one of the methods differs significantly. Follow-up comparisons (like pairwise t-tests with error MS) can clarify which specific method is better."
  },
  {
    "objectID": "worksheets/latin-square.html#step-by-step-summary-for-students",
    "href": "worksheets/latin-square.html#step-by-step-summary-for-students",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "9. Step-by-Step Summary for Students",
    "text": "9. Step-by-Step Summary for Students\n\nPlan and Randomize\n\nAssign each “row” factor (Teacher T1–T4) at random.\nAssign each “column” factor (Classroom R1–R4) at random.\nRandomly assign treatments (A, B, C, D) so each appears once per row/column.\n\nCollect Data\n\nFill in the 4×4 data grid with the response measured in each cell.\n\nCompute Averages\n\nGrand Mean = sum of all 16 responses / 16.\nRow Means = average for each teacher.\nColumn Means = average for each classroom.\nTreatment Means = average across the 4 cells that use that treatment.\n\nANOVA Calculations\n\nTotal SS: sum of squared deviations from grand mean.\nRow SS: multiply 4 by the sum of squared deviations of row means from grand mean.\nColumn SS: multiply 4 by sum of squared deviations of column means from grand mean.\nTreatment SS: multiply 4 by sum of squared deviations of treatment means from grand mean.\nResidual SS: difference = Total SS–(Row + Column + Treatment SS).\n\nDegrees of Freedom\n\nTotal df = 16–1 = 15.\nEach of the row, column, and treatment sources has 3 df.\nResidual df = 15–(3+3+3) = 6.\n\nMean Squares and F-Tests\n\nMS = SS / df.\n\\(F = \\frac{\\text{MS}_{\\text{Factor}}}{\\text{MS}_{\\text{Error}}}\\).\nCompare each \\(F\\)-value to the \\(F\\)-distribution with (3,6) df to see if it is large enough to be significant.\n\nInterpret Results\n\nIdentify whether row or column blocking was significant.\nCheck if treatments differ significantly.\nPresent a final conclusion and any potential next steps (e.g., further studies, transformations, or different design if strong interactions are suspected)."
  },
  {
    "objectID": "worksheets/latin-square.html#final-remarks",
    "href": "worksheets/latin-square.html#final-remarks",
    "title": "Latin Square Design: Step-by-Step Worksheet",
    "section": "10. Final Remarks",
    "text": "10. Final Remarks\nA Latin Square design:\n\nBlocks out two known sources of variation (rows and columns).\nTests a single primary factor of interest (the treatment).\nIs valid under randomization (rows, columns, and treatments all randomized).\nIs straightforward to analyze with a standard ANOVA and standard software.\n\nThis example helps students see:\n\nHow to construct a Latin Square.\nWhy it is effective at removing large, unwanted variability in two directions (rows & columns).\nHow to perform step-by-step calculations of the sums of squares, degrees of freedom, and F-ratios.\n\n\n\nReferences and Further Reading\n\nBox, G.E.P., Hunter, J.S., and Hunter, W.G. Statistics for Experimenters: Design, Innovation, and Discovery (2nd ed.).\nChapters on “Comparing Several Treatments: Blocks and Latin Squares,” which illustrate the general analysis of variance principle for multi-way blocking designs."
  }
]